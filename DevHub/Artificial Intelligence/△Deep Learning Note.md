# 深度学习基础
## 数据操作
### N维数组
#### N维数组是机器学习和神经网络的主要数据结构
- 0-d(标量)----一个类别
- 1-d(向量)----一个特征向量
- 2-d(矩阵)----一个样本--特征矩阵
- 3-d(RGB图片--宽*高*通道)
- 4-d(RGB图片批量--批量大小*宽*高*通道)
- 5-d(视频批量--批量大小*宽*高*通道)
### 创建数组
#### 创建数组需要
- 形状 
例如3*4矩阵
- 每个元素的数据类型  
例如32位浮点数
- 每个元素的值  
例如全是0或者随机数
  - 正态分布
  - 随机分布
### 访问元素
一个元素
```
x[1,2]
```
一行元素
```
x[1,:]
```
一列元素
```
x[:,1]
```
连续子区域
```
x[1:3, 1:]
```
分散子区域
```
x[::3, ::2]
```
## 线性代数
### 标量
#### 简单操作

$$
c = a + b
$$


$$
c = a \cdot b
$$


$$
c = \sin a
$$

#### 长度

$$
|a| =
\begin{cases} 
a, & \text{if } a > 0 \\ 
-a, & \text{otherwise} 
\end{cases}
$$


$$
|a + b| \leq |a| + |b|
$$


$$
|a \cdot b| = |a| \cdot |b|
$$

### 向量
#### 简单操作

$$
c = a + b \quad \text{where} \quad c_i = a_i + b_i
$$


$$
c = \alpha \cdot b \quad \text{where} \quad c_i = \alpha \cdot b_i
$$


$$
c = \sin a \quad \text{where} \quad c_i = \sin a_i
$$

#### 长度

$$
\|a\|_2 = \left[ \sum a_i^2 \right]^{\frac{1}{2}}
$$


$$
\|a\| \geq 0 \quad \text{for all } a
$$


$$
\|a + b\| \leq \|a\| + \|b\|
$$


$$
\|a \cdot b\| = |a| \cdot \|b\|
$$


$$
c = a + b
$$


$$
c = \alpha \cdot b
$$

##### 点乘

$$
a^T b = \sum a_i b_i
$$

##### 正交

$$
a^T b = \sum a_i b_i = 0
$$


### 矩阵
#### 简单操作

$$
C = A + B \quad \text{where} \quad C_{ij} = A_{ij} + B_{ij}
$$


$$
C = \alpha \cdot B \quad \text{where} \quad C_{ij} = \alpha \cdot B_{ij}
$$


$$
C = \sin A \quad \text{where} \quad C_{ij} = \sin A_{ij}
$$

#### 乘法
##### 矩阵乘以向量

$$
c = A b \quad \text{where} \quad c_i = \sum_j A_{ij} b_j
$$

##### 矩阵乘以矩阵

$$
C = A B \quad \text{where} \quad C_{ik} = \sum_j A_{ij} B_{jk}
$$

#### 范数

$$
c = A \cdot b \quad \text{hence} \quad \|c\| \leq \|A\| \cdot \|b\|
$$

##### 取决于如何衡量b和c的长度
##### 常见范数
###### 矩阵范数：最小的满足上面公式的值
###### Frobenius范数

$$
\|A\|_{\text{Frob}} = \left[ \sum_{i,j} A_{ij}^2 \right]^{\frac{1}{2}}
$$

#### 特殊矩阵
##### 对称和反对称
1. **对称矩阵（Symmetric Matrix）**： 

$$
A_{ij} = A_{ji}, \quad \forall i, j
$$

2. **反对称（斜对称）矩阵（Skew-Symmetric Matrix）**： 

$$
A_{ij} = -A_{ji}, \quad \forall i, j
$$

##### 正定

$$
\|x\|_2^2 = x^T x \geq 0 \quad \Longrightarrow \quad x^T A x \geq 0
$$

##### 正交矩阵
- 所有行都相互正交
- 所有行都有单位长度

$$
\sum_j \Sigma U_{ij} U_{kj} = \delta_{ik}
$$

- 可以写成 

$$
U U^T = I
$$

##### 置换矩阵

$$
P_{ij} =
\begin{cases}
1, & \text{if } j = \pi(i) \\
0, & \text{otherwise}
\end{cases}
$$

置换矩阵是正交矩阵
#### 特征向量和特征值
- 不被矩阵改变竞向的向量
-

$$
Ax = \lambda x
$$

- 对称矩阵总是可以找到特征向量

## 微积分
导数是切线的斜率
### 标量导数
| 函数 \( y \) | 导数 \( \frac{dy}{dx} \) |
|-------------|------------------|
| \( y = a \) | \( 0 \) （a 不是 x 的函数） |
| \( y = x^n \) | \( n x^{n-1} \) |
| \( y = \exp(x) \) | \( \exp(x) \) |
| \( y = \log(x) \) | \( \frac{1}{x} \) |
| \( y = \sin(x) \) | \( \cos(x) \) |
| \( y = u + v \) | \( \frac{du}{dx} + \frac{dv}{dx} \) |
| \( y = uv \) | \( \frac{du}{dx} v + \frac{dv}{dx} u \) |
| \( y = f(u), u=g(x) \) | \( \frac{df}{du} \cdot \frac{du}{dx} \) |

### 亚导数
将导数拓展到不可微的函数
### 梯度
将导数拓展到向量
#### ∂y/∂X

$$
X = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix}
$$


$$
\frac{\partial y}{\partial X} =
\begin{bmatrix}
\frac{\partial y}{\partial x_1}, \frac{\partial y}{\partial x_2}, \dots, \frac{\partial y}{\partial x_n}
\end{bmatrix}
$$


##### 样例
| 函数 \( y \) | 导数 \( \frac{\partial y}{\partial X} \) |
|-------------|-------------------------|
| \( y = a \) | \( 0^T \) （0 是一个向量） |
| \( y = au \) | \( a \cdot \frac{\partial u}{\partial X} \) |
| \( y = \sum(X) \) | \( 1^T \) （1 是一个向量） |
| \( y = \|X\|_2 \) | \( 2 X^T \) |
| \( y = u + v \) | \( \frac{\partial u}{\partial X} + \frac{\partial v}{\partial X} \) |
| \( y = uv \) | \( \frac{\partial u}{\partial X} \cdot v + \frac{\partial v}{\partial X} \cdot u \) |
| \( y = \langle u, v \rangle \) | \( u^T \frac{\partial v}{\partial X} + v^T \frac{\partial u}{\partial X} \) |



#### ∂Y/∂x

$$
Y = \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_m \end{bmatrix}
$$


$$
\frac{\partial Y}{\partial x} =
\begin{bmatrix}
\frac{\partial y_1}{\partial x} \\
\frac{\partial y_2}{\partial x} \\
\vdots \\
\frac{\partial y_m}{\partial x}
\end{bmatrix}
$$

##### ∂y/∂X是行向量，∂Y/∂x是列向量
##### 这个被称为分子布局符号，反过来的版本叫分母布局符号

#### ∂Y/∂X

$$
X = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix}
$$


$$
Y = \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_m \end{bmatrix}
$$


$$
\frac{\partial Y}{\partial X} = 
\begin{bmatrix}
\frac{\partial y_1}{\partial x_1} & \frac{\partial y_1}{\partial x_2} & \dots & \frac{\partial y_1}{\partial x_n} \\
\frac{\partial y_2}{\partial x_1} & \frac{\partial y_2}{\partial x_2} & \dots & \frac{\partial y_2}{\partial x_n} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial y_m}{\partial x_1} & \frac{\partial y_m}{\partial x_2} & \dots & \frac{\partial y_m}{\partial x_n}
\end{bmatrix}
$$


##### 样例
| 函数 \( Y \)         | 导数 \( \frac{\partial Y}{\partial X} \)           |
|---------------------|-----------------------------------------------|
| \( Y = a \)         | \( 0 \) （0 是一个矩阵）                        |
| \( Y = X \)         | \( I \) （单位矩阵）                            |
| \( Y = AX \)        | \( A \)                                        |
| \( Y = X^T A \)     | \( A^T \)                                      |
| \( Y = aU \)        | \( a \frac{\partial U}{\partial X} \)          |
| \( Y = AU \)        | \( A \frac{\partial U}{\partial X} \)          |
| \( Y = U + V \)     | \( \frac{\partial U}{\partial X} + \frac{\partial V}{\partial X} \) |


### 拓展到矩阵

## 链式法则
### 标量链式法则

$$
y = f(u), \quad u = g(x)
$$


$$
\frac{\partial y}{\partial x} = \frac{\partial y}{\partial u} \cdot \frac{\partial u}{\partial x}
$$

### 向量链式法则

$$
\frac{\partial y}{\partial X} = \frac{\partial y}{\partial u} \cdot \frac{\partial u}{\partial X}
$$


$$
(1, n) \quad (1, ) \quad (1, n)
$$


$$
\frac{\partial y}{\partial X} = \frac{\partial y}{\partial U} \cdot \frac{\partial U}{\partial X}
$$


$$
(1, n) \quad (1, k) \quad (k, n)
$$


$$
\frac{\partial Y}{\partial X} = \frac{\partial Y}{\partial U} \cdot \frac{\partial U}{\partial X}
$$


$$
(m, n) \quad (m, k) \quad (k, n)
$$


## 自动求导
### 自动求导概念
- 自动求导计算一个函数在指定值上的导数
- 自动求导有别于符号求导和数值求导

### 计算图
- 将代码分解成操作子
- 将计算表示成一个无环图
- 显示构造
  - Tensorflow
  - Theano
  - MXNet
- 隐式构造
  - PyTorch
  - MXNet

### 自动求导的两种模式
#### 链式法则

$$
\frac{\partial y}{\partial x} = \frac{\partial y}{\partial u_n} \cdot \frac{\partial u_n}{\partial u_{n-1}} \cdot \dots \cdot \frac{\partial u_2}{\partial u_1} \cdot \frac{\partial u_1}{\partial x}
$$

#### 正向积累

$$
\frac{\partial y}{\partial x} = \frac{\partial y}{\partial u_n} \left( \frac{\partial u_n}{\partial u_{n-1}} \left( \dots \left( \frac{\partial u_2}{\partial u_1} \cdot \frac{\partial u_1}{\partial x} \right) \right) \right)
$$

#### 反向积累，又称反向传递

$$
\frac{\partial y}{\partial x} = \left( \left( \frac{\partial y}{\partial u_n} \cdot \frac{\partial u_n}{\partial u_{n-1}} \right) \dots \frac{\partial u_2}{\partial u_1} \right) \cdot \frac{\partial u_1}{\partial x}
$$

- 构造计算图 
- 前向:执行图,存储中间结果 
- 反向:从相反竞向执行图 
  - 去除不需要的枝
### 复杂度
#### 计算复杂度
O(n), n 是操作子个数 

通常正向和反向的代价类似 
#### 内存复杂度
O(n),因为需要存储正向的所有中间结果 
#### 跟正向累积对比: 
- O(n) 计算复杂度用来计算一个变量的梯度 
- O(1) 内存复杂度


# 线性神经网络
## 线性回归
### 定义
给定 n 维输入

$$
X = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix}
$$

线性模型有一个 n 维权重和一个标量偏差 

$$
W = \begin{bmatrix} w_1 \\ w_2 \\ \vdots \\ w_n \end{bmatrix}, \quad b
$$

输出是输入的加权和 

$$
y = w_1 x_1 + w_2 x_2 + \dots + w_n x_n + b
$$


$$
y = \langle W, X \rangle + b
$$



- 线性回归是对n维输入的加权,外加偏差
- 使用平方损失来衡量预测值和真实值的差异
- 线性回归有显式解
- 线性模型可以看做是单层神经网络

### 衡量预估质量
- 比较真实值和预估值,例如房屋售价和估价 
- 假设y是真实值, y_hat是估计值,我们可以比较  

$$
l(y, \hat{y}) = \frac{1}{2} (y - \hat{y})^2
$$

这个叫做平方损失

### 训练数据
- 收集一些数据点来决定参数值(权重和偏差),例如过去6个月卖的房子 
- 这被称之为训练数据 
- 通常越多越好 
- 假设有n个样本,记  


$$
X = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix}
$$


$$
y = \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{bmatrix}
$$


### 参数学习
训练损失

$$
l(X,y,w,b) = \frac{1}{2n} \sum_{i=1}^{n} \left( y_i - \langle x_i, w \rangle - b \right)^2 = \frac{1}{2n} \| y - Xw - b \|^2
$$


最小化损失来学习参数

$$
w^*, b^* = \arg \min l(X, y, w, b)
$$


### 显示解
将偏差加入权重

$$
X \leftarrow \begin{bmatrix} X \\ 1 \end{bmatrix}, \quad w \leftarrow \begin{bmatrix} w \\ b \end{bmatrix}
$$


$$
l(X, y, w) = \frac{1}{2n} \|y - Xw\|^2
$$


$$
\frac{\partial}{\partial w} l(X, y, w) = \frac{1}{n} (y - Xw)^T X
$$

损失是凸函数,所以最优解满足

$$
\frac{\partial}{\partial w} l(X, y, w) = 0
$$


## 基础优化方法
### 梯度下降
梯度下降通过不断沿着反梯度方向更新参数求解
#### 步骤
- 挑选一个初始值w_0
- 重复迭代参数 t=1,2,3 


$$
W_t = W_{t-1} - \eta \frac{\partial}{\partial W_{t-1}}
$$


- 沿梯度方向将增加损失函数值 
#### 学习率
步长的超参数
- 选择学习率，不能太小也不能太大
  - 学习率太小，收敛太慢
  - 学习率太大，会在局部最低点附近反复横跳

### 小批量随机梯度下降
- 小批量随机梯度下降是深度学习默认的求解算法
- 在整个训练集上算梯度太贵 
  - 一个深度神经网络模型可能需要数分钟至数小时

我们可以随机采样 b 个样本i_1, i_2,..., i_b来近似损失 

$$
\frac{1}{b} \sum l(X_i, y_i, W)
$$

b 是批量大小,另一个重要的超参数

#### 选择批量大小
一个重要的超参数
- 不能太小  
每次计算量太小,不适合并行来最大利用计算资源
- 不能太大  
内存消耗增加 浪费计算,例如如果所有样本都是相同的

## Softmax 回归
### 概念
- Softmax 回归是一个多类分类模型 
- 使用 Softmax 操作子得到每个类的预测置信度 
- 使用交叉熵来衡量预测和标号的区别
### 回归 vs 分类
- 回归估计一个连续值 
- 分类预测一个离散类别
  - MNIST: 手写数字识别(10类)
  - ImageNet: 自然物体分类(1000类)

### 回归
#### 回归特点
- 单连续数值输出 
- 自然区间 
- 跟真实值的区别作为损失

### 分类
#### 分类特点
- 通常多个输出 
- 输出 i 是预测为第 i 类的置信度

### 均方损失
#### 对类别进行一位有效编码

$$
y = \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{bmatrix}
$$


$$
y_i = \begin{cases} 
1 & \text{if } i = y \\
0 & \text{otherwise}
\end{cases}
$$

#### 使用均方损失训练 
#### 最大值最为预测

$$
\hat{y} = \arg\max O_i
$$


### 无校验比例
#### 对类别进行一位有效编码 
#### 最大值最为预测

$$
\hat{y} = \arg\max O_i
$$

#### 需要更置信的识别正确类(大余量)

$$
O_y - O_i \geq \Delta(y, i)
$$


### 校验比例
#### 输出匹配概率(非负,和为 1)

$$
\hat{y} = \text{softmax}(O)
$$


$$
\hat{y}_i = \frac{\exp(O_i)}{\sum_k \exp(O_k)}
$$

#### 概率y和y_hat的区别作为损失

### 交叉熵损失
#### 交叉熵常用来衡量两个概率的区别

$$
H(p, q) = - \sum p_i \log(q_i)
$$

#### 将它作为损失

$$
l(y, \hat{y}) = - \sum y_i \log(\hat{y}_i) = - \log(\hat{y}_y)
$$

#### 其梯度是真实概率和预测概率的区别

$$
\frac{\partial}{\partial O_i} l(y, \hat{y}) = \text{softmax}(O)_i - y_i
$$


## 损失函数
### L2 Loss

$$
l(y, y') = \frac{1}{2}(y - y')^2
$$

### L1 Loss

$$
l(y, y') = |y - y'|
$$

### Huber's Robust Loss

$$
l(y, y') = 
\begin{cases}
|y - y'| - \frac{1}{2} & \text{if } |y - y'| > 1 \\
\frac{1}{2}(y - y')^2 & \text{otherwise}
\end{cases}
$$




# 多层感知机
## 感知机
### 概念
- 感知机是一个二分类模型,是最早的AI模型之一 
- 它的求解算法等价于使用批量大小为1的梯度下降 
- 它不能拟合 XOR 函数,导致的第一次 AI 寒冬

### 算法定义
给定输入x，权重w，和偏移b，感知机输出

$$
o = \sigma(\langle w, x \rangle + b)
$$



$$
\sigma(x) = 
\begin{cases}
1 & \text{if } x > 0 \\
0 & \text{otherwise}
\end{cases}
$$


### 二分类： -1或1
- Vs.回归输出实数
- Vs. Softmax回归输出概率

### 收敛定理
- 数据在半径r内 
- 余量ρ分类两类


$$
y(\mathbf{x}^T w + b) \geq \rho
$$



$$
\text{subject to } \|w\|^2 + b^2 \leq 1
$$


- 感知机保证在


$$
\frac{r^2 + 1}{\rho^2}
$$


步后收敛

### XOR 问题 (Minsky & Papert, 1969)
感知机不能拟合 XOR 函数,它只能产生线性分割面

## 多层感知机
### 概念
- 多层感知机使用隐藏层和激活函数来得到非线性模型 
- 常用激活函数是Sigmoid,Tanh,ReLU 
- 使用 Softmax 来处理多类分类 
- 超参数为隐藏层数,和各个隐藏层大小

### Sigmoid 激活函数
将输入投影到 (0, 1), 是一个软的

$$
\sigma(x) =
\begin{cases}
1 & \text{if } x > 0 \\
0 & \text{otherwise}
\end{cases}
$$

#### 公式

$$
\sigma(x) = \frac{1}{1 + \exp(-x)}
$$


### Tanh 激活函数
将输入投影到 (-1, 1)
#### 公式

$$
\tanh(x) = \frac{1 - \exp(-2x)}{1 + \exp(-2x)}
$$


### ReLU 激活函数
ReLU: rectified linear unit
#### 公式

$$
\text{ReLU}(x) = \max(x, 0)
$$


### 多类分类

$$
y_1, y_2, \dots, y_k = \text{softmax}(o_1, o_2, \dots, o_k)
$$


## 模型选择
### 训练误差和泛化误差
- 训练误差
模型在训练数据上的误差 
- 泛化误差
模型在新数据上的误差

### 验证数据集和测试数据集
#### 验证数据集
用来评估模型好坏的数据集 
- 例如拿出 50% 的训练数据 
- 不要跟训练数据混在一起(常犯错误)
#### 测试数据集
只用一次的数据集。例如 
- 未来的考试 
- 我出价的房子的实际成交价 
- 用在 Kaggle 私有排行榜中的数据集

### K-则交叉验证
在没有足够指数据时使用(这是常态)
#### 算法
- 将训练数据分割成 K 块 
- For i = 1, …, K 
- 使用第 i 块作为验证数据集,其余的作为训练数据集 
- 报告 K 个验证集误差的平均 
- 常用:K = 5 或 10


非大数据集上通常使用 k-折交叉验证

### 训练数据集和验证数据集
- 训练数据集----训练模型参数
- 验证数据集----选择模型超参数


## 过拟合(Overfitting)和欠拟合(Underfitting)
### 模型容量
- 拟合各种函数的能力 
- 低容量的模型难以拟合训练数据 
- 高容量的模型可以记住所有的训练数据
- 模型容量需要匹配数据择杂度,否则可能导致欠拟合和过拟合
  - 数据复杂，模型容量低，容易出现欠拟合
  - 数据简单，模型容量高，容易出现过拟合
### 估计模型容量
难以在不同的种类算法之间比较 
- 例如数模型和神经网络 

给定一个模型种类,将有两个主要因素 
- 参数的个数 
- 参数值的选择范围

### VC 维
- 统计学习理论的一个核心思想
- 统计机器学习提供数学工具来衡量模型择杂度
- 对于一个分类模型,VC等于一个最大的数据集的大小,不管如何给定标号,都存在一个模型来对它进行完美分类

### 线性分类器的 VC 维
- 2 维输入的感知机,VC 维 = 3 
  - 能够分类任何三个点,但不是4个 (xor)
- 支持 N 维输入的感知机的 VC 维是 N + 1
- 一些多层感知机的 VC 维 O(N log2 N)

### VC 维的用处
- 提供为什么一个模型好的理论依据 
  - 它可以衡量训练误差和泛化误差之间的间隔 
- 但深度学习中很少使用 
  - 衡量不是很准确 
  - 计算深度学习模型的 VC 维很困难
- 实际中一般靠观察训练误差和验证误差

### 数据复杂度
多个重要因素 
- 样本个数 
- 每个样本的元素个数 
- 时间、空间结构 
- 多样性


## 权重衰退 weight Decay
### 概念
权重衰退通过 L2 正则项使得模型参数不会过大,从而控制模型复杂度 

正则项权重是控制模型择杂度的超参数

### 使用均方范数作为硬性限制
通过限制参数值的选择范围来控制模型容量

$$
\min l(w,b) \quad \text{subject to} \quad ||w||^2 \leq \theta
$$

- 通常不限制偏移 b (限不限制都差不指) 
- 小的θ意味着更强的正则项

### 使用均方范数作为柔性限制
对每个θ,都可以找到λ使得之前的目标函数等价于

$$
\min l(w,b) + \frac{\lambda}{2} ||w||^2
$$

可以通过拉格朗日乘子来证明

超参数λ控制了正则项的重要程度

$$
\lambda = 0: \text{无作用}
$$


$$
\lambda \to \infty, \quad w^* \to 0
$$


### 参数更新法则
#### 计算梯度

$$
\frac{\partial}{\partial w} \left( l(w, b) + \frac{\lambda}{2} ||w||^2 \right) = \frac{\partial l(w, b)}{\partial w} + \lambda w
$$

#### 时间 t 更新参数

$$
w_{t+1} = (1 - \eta \lambda) w_t - \eta \frac{\partial l(w_t, b_t)}{\partial w_t}
$$

通常 ηλ<1 ,在深度学习中通常叫做权重衰退


## 丢弃法 Dropout
### 概念
- 丢弃法将一些输出项随机置0来控制模型复杂度 
- 常作用在多层感知机的隐藏层输出上 
- 丢弃概率是控制模型复杂度的超参数


### 动机
- 一个好的模型需要对输入数据的扰动鲁棒 
- 使用有噪音的数据等价于 Tikhonov 正则 
- 丢弃法:在层之间加入噪音

### 无偏差的加入噪音
- 对x加入噪音得到x',我们希望 


$$
E[x'] = x
$$

- 丢弃法对每个元素进行如下扰动


$$
x'_i = 
\begin{cases} 
0 & \text{with probability } p \\
\frac{x_i}{1-p} & \text{otherwise}
\end{cases}
$$


### 使用丢弃法
通常将丢弃法作用在隐藏全连接层的输出上

$$
h = \sigma(W_1x + b_1)
$$


$$
h' = \text{dropout}(h)
$$


$$
o = W_2h' + b_2
$$


$$
y = \text{softmax}(o)
$$


### 推理中的丢弃法
- 正则项只在训练中使用，他们影响模型参数的更新 
- 在推理过程中,丢弃法直接返回输入


$$
h' = \text{dropout}(h)
$$

这样也能变证确定性的输出


## 数值稳定性
### 神经网络的梯度
考虑如下有 d 层的神经网络

$$
h_t = f_t(h_{t-1})
$$


$$
y = l \circ f_d \circ \dots \circ f_1(x)
$$

计算损失l关于参数W_t的梯度

$$
\frac{\partial l}{\partial W_t} = \frac{\partial l}{\partial h_d} \frac{\partial h_d}{\partial h_{d-1}} \dots \frac{\partial h_{t+1}}{\partial h_t} \frac{\partial h_t}{\partial W_t}
$$

d-t 次矩阵乘法

### 数值稳定性的常见两个问题
#### 梯度爆炸
##### 值超出值域(infinity) 
对于16位浮点数尤为严重(数值区间6e-5 - 6e4)
##### 对学习率敏感 
- 如果学习率太大 -> 大参数值 -> 更大的梯度 
- 如果学习率太小 -> 训练无进展 
- 可能需要在训练过程不断调整学习率
#### 梯度消失
##### 梯度值变成0
对 16 位浮点数尤为严重
##### 训练没有进展
不管如何选择学习率 
##### 对于底部层尤为严重 
- 仅仅顶部层训练的较好 
- 无法让神经网络更深

**当数值过大或者过小时会导致数值问题**
**常发生在深度模型中,因为其会对 n 个数累乘**


## 模型初始化和激活函数
合理的权重初始值和激活函数的选取可以提升数值稳定性
### 让训练更加稳定
#### 目标：让梯度值在合理的范围内 
例如[1e-6, 1e3]
#### 将乘法变加法
ResNet, LSTM
#### 归一化 
- 梯度归一化
- 梯度裁剪
#### 合理的权重初始和激活函数


### 让每层的方差是一个常数
- 将每层的输出和梯度都看做随机变量 
- 让它们的均值和方差都保持一致

### 权重初始化
在合理值区间里随机初始参数

训练开始的时候更容易有数值不稳定 
- 远离最优解的地方损失函数表面可能很复杂
- 最优解附近表面会比较平 

使用N(0, 0.01)来初始可能对小网络没问题,但不能保证深度神经网络

### Xavier 初始化
难以需要满足

$$
n_{t-1} \gamma_t = 1
$$

和

$$
n_t \gamma_t = 1
$$

Xavier 使得

$$
\frac{\gamma_t (n_{t-1} + n_t)}{2} = 1 \quad \Rightarrow \quad \gamma_t = 2(n_{t-1} + n_t)
$$

- 正态分布


$$
N\left(0, \sqrt{\frac{2}{n_{t-1} + n_t}}\right)
$$


- 均匀分布


$$
u\left(-\sqrt{\frac{6}{n_{t-1} + n_t}}, \sqrt{\frac{6}{n_{t-1} + n_t}}\right)
$$


分布u[-a, a]和方差是 a^2/3

适配权重形状变换,特别是 n_t


# 深度学习硬件
## CPU和GPU
### 提升CPU利用率
在计算a+b之前,需要准备数据 

主内存 -> L3 -> L2 -> L1 -> 寄存器 
- L1 访问延时:0.5 ns 
- L2 访问延时:7 ns (14 x L1) 
- 主内存访问延时:100ns (200 x L1) 
提升空间和时间的内存本地性 
- 时间
重用数据使得变持它们在缓存里 
- 空间
按序读写数据使得可以预读取

高端 CPU 有几十个核 
- EC2 P3.16xlarge: 2 Intel Xeon CPUs, 32 物理核 

并行来利用所有核 
- 超线程不一定提升性能，因为它们共享寄存器

### 样例分析
如果一个矩阵是按列存储,访问一行会比访问一列要快 
- CPU一次读取 64 字节(缓存线) 
- CPU会“聪明的”提前读取下一个(缓存线)

代码A比代码B慢(python)
代码A：
```
for i in range(len(a)):
	c[i] = a[i] + b[i]
```
代码B：
```
c = a + b
```
代码A调用 n 次函数,每次调用有开销 

代码B很容易被并行(例如下面C++实现)
```
#pragma omp for
for (i=0; i<a.size(); i++) {
	c[i] = a[i] + b[i]
}
```
### 提升GPU利用率
#### 并行 
使用数千光线程 
#### 内存本地性 
缓存更小,架构更加简单 
#### 少用控制语句 
- 支持有限 
- 同步开销很大

**不要频繁在CPU和GPU之前传数据：带宽限制，同步开销**

### 更多的 CPUs 和 GPUs
- CPU
  - AMD
  - ARM 
- GPU
  - NVIDIA
  - AMD
  - Intel
  - ARM
  - Qualcomm
  - …

### CPU/GPU高性能计算编程
- CPU
  - C++ 或者任何高性能语言 
  - 编译器成熟 
- GPU 
  - Nvidia 上用 CUDA 
    - 编译器和驱动成熟 
  - 其他用 OpenCL 
    - 质量取决于硬件厂商

### CPU/GPU比较
- CPU  
可以处理通用计算。性能优化考虑数据读写效率和多线程 
- GPU  
使用更多的小核和更好的内存带宽,适合能大规模并行的计算任务

## 专有硬件
### DSP----数字信号处理
- 为数字信号处理算法设计
  - 点积
  - 卷积
  - FFT 
- 低功耗、高性能 
  - 比移动GPU快5x,功耗更低 
- VLIW: Very long instruction word 
  - 一条指令计算上百次乘累加 
- 编程和调试困难 
- 编译器质量良莠不齐

### 可编程阵列 (FPGA)
- 有大量可以编程逻辑单元和可配置的连接 
- 可以配置成计算复杂函数 
  - 编程语言:VHDL,Verilog 
- 通常比通用硬件更高效 
- 工具链质量良莠不齐 
- 一次“编译”需要数小时

### AI ASIC
- 深度学习的热门领域 
  - 大公司都在造自己的芯片 (Intel, Qualcomm, Google, Amazon, Facebook, …) 
- Google TPU 是标志性芯片 
  - 能够媲美Nvidia GPU性能 
  - 在Google大量部署 
  - 核心是 systolic array

### Systolic Array
- 计算单元(PE)阵列 
- 特别适合做矩阵乘法 
- 设计和制造相对简单
- 对于一般的矩阵内法，通过切开和填充来匹配SA大小 
- 批量输入来降低延时 
- 通常有其他硬件单元来处理别的 NN 操作子，例如激活层

## 多GPU并行
### 单机多卡并行
- 一台机器可以安装多个GPU(1-16) 
- 在训练和预测时,我们将一个小批量计算切分到多个GPU上来达到加速目的 
### 常用切分方案有 
- 数据并行 
- 模型并行 
- 通道并行(数据+模型并行)

### 数据并行 vs 模型并行
#### 数据并行
- 将小批量分成n块，每个GPU拿到完整参数计算一块数据的梯度
- 通常性能更好
#### 模型并行
- 将模型分成n块，每个GPU拿到一块模型计算它的前向和反向结果 
- 通常用于模型大到单GPU放不下

#### 数据并行步骤
- 读一个数据块
- 拿回参数
- 计算梯度
- 发出梯度
- 更新梯度


当一个模型能用单卡计算时,通常使用数据并行拓展到多卡上 

模型并行则用在超大模型上


## 分布式计算
### 分布式计算架构
- 数据放在分布式文件系统上
- 通过网络读取数据
- 多个worker
- 通过网络发送
- 多个参数服务器

### GPU机器架构
- 交换机
- CPU
- PCle交换
- GPU

### 计算一个小批量
- 每个计算服务器读取小批量中的一块
- 进一步将数据切分到每个GPU上
- 每个worker从参数服务器那里获取模型
- 复制参数到每个GPU上
- 每个GPU计算梯度
- 将所有GPU上的梯度求和
- 梯度传回服务器
- 每个服务器对梯度求和,并更新参数

### 同步 SGD
- 这里每个worker都是同步计算一个批量,称为同步SGD 
- 假设有n个GPU,每个GPU每次处理b个样本,那么同步SGD等价于在单 GPU 运行批量大小为nb的SGD 
- 在理想情况下, n个GPU可以得到相对单个GPU的n倍加速

### 性能
- t_1 = 在单GPU上计算b个样本梯度时间 
- 假设有m个参数,一个 worker 每次发送和接收m个参数、梯度 
  - t2 = 发送和接收所用时间 
- 每个批量的计算时间为 max(t_1, t_2)
  - 选取足够大的 b 使得 t_1 > t_2
  - 增加b或n导致更大的批量大小，导致需要更多计算来得到给定的模型精度

### 性能的权衡
- 系统性能----每epoch时间
- 训练有效性--需要多个epoch

### 实践时的建议
- 使用一个大数据集 
- 需要好的GPU-GPU和机器-机器带宽 
- 高效的数据读取和预处理 
- 模型需要有好的计算(FLOP)通讯(model size)比 
  - Inception > ResNet > AlexNet 
- 使用足够大的批量大小来得到好的系统性能 
- 使用高效的优化算法对对应大批量大小

### 分布式计算注意
- 分布式同步数据并行是多GPU数据并行在多机器上的拓展 
- 网络通讯通常是瓶颈 
- 需要注意使用特别大的批量大小时收敛效率 
- 更复杂的分布式有异步、模型并行




# 卷积神经网络
## 从全连接层到卷积层
对全连接层使用平移不变性和局部性得到卷积层
### 重新考察全连接层
- 将输入和输出变形为矩阵(宽度,高度)
- 将权重变形为4-D张量(h, w)到(h', w')


$$
h_{i,j} = \sum_{k,l} (w_{i,j,k,l} x_{k,l}) = \sum_{a,b} (v_{i,j,a,b} x_{i+a,j+b})
$$


- V 是 W 的重新索引


$$
v_{i,j,a,b} = w_{i,j,i+a,j+b}
$$


### 原则 #1 - 平移不变性
x 的平移导致 h 的平移

$$
h_{i,j} = \sum_{a,b} v_{i,j,a,b} x_{i+a,j+b}
$$

v不应该依赖于(i,j) 

解决方案

$$
v_{i,j,a,b} = v_{a,b}
$$


$$
h_{i,j} = \sum_{a,b} v_{a,b} x_{i+a,j+b}
$$

这就是2维交叉相关,即2维卷积

### 原则 #2 - 局部性

$$
h_{i,j} = \sum_{a,b} v_{a,b} x_{i+a,j+b}
$$

#### 当评估h_i,j时,我们不应该用远离x_i,j的参数
#### 解决方案

$$
\text{When } |a|, |b| > \Delta, \text{ set } v_{a,b} = 0
$$


$$
h_{i,j} = \sum_{a,b} v_{a,b} x_{i+a,j+b}
$$

 
## 图像卷积
### 二维交叉相关

### 二维卷积层
输入

$$
X: n_h \times n_w
$$

核 

$$
W: k_h \times k_w
$$

偏差 

$$
b \in \mathbb{R}
$$

输出

$$
Y: (n_h - k_h + 1) \times (n_w - k_w + 1)
$$


$$
Y = X \star W + b
$$

W 和 b 是可学习的参数

### 交叉相关 vs 卷积
二维交叉相关

$$
y_{i,j} = \sum \sum w_{a,b} x_{i+a,j+b}
$$

二维卷积

$$
y_{i,j} = \sum \sum w_{-a,-b} x_{i+a,j+b}
$$

由于对称性,在实际使用中没有区别

### 一维和三维交叉相关
一维

$$
y_i = \sum w_a x_{i+a}
$$

应用
- 文本
- 语言
- 时序序列


三维

$$
y_{i,j,k} = \sum \sum \sum w_{a,b,c} x_{i+a,j+b,k+c}
$$

应用
- 视频
- 医学图像
- 气象地图

### 图像卷积特点
- 卷积层将输入和核矩阵进行交叉相关,加上偏移后得到输出 
- 核矩阵和偏移是可学习的参数 
- 核矩阵的大小是超参数


## 填充和步幅
### 概念
- 填充和步幅是卷积层的超参数 
- 填充在输入周围添加额外的行/列,来控制输出形状的减少量 
- 步幅是每次滑动核窗口时的行/列的步长,可以成倍的减少输出形状

### 填充
- 给定(32 × 32)输入图像 
- 应用 5 × 5 大小的 卷积核
  - 第1层得到输出大小 28 x 28 
  - 第7层得到输出大小4 x 4 
- 更大的卷积核可以更快地减小输出大小 
  - 形状从n_h × n_w减少到


$$
(n_h - k_h + 1) \times (n_w - k_w + 1)
$$


- 在输入周围添加额外的行 / 列
- 填充p_h行和p_w列,输出形状为


$$
(n_h - k_h + p_h + 1) \times (n_w - k_w + p_w + 1)
$$


- 通常取


$$
p_h = k_h - 1, \quad p_w = k_w - 1
$$


  - 当k_h为奇数:在上下两侧填充p_h/2
  - 当k_h为偶数:在上侧填充「p_h/2|，在下侧填充 |ph/2」


$$
\text{Top padding} = \left\lceil \frac{p_h}{2} \right\rceil, \quad \text{Bottom padding} = \left\lfloor \frac{p_h}{2} \right\rfloor
$$



### 步幅
填充减小的输出大小与层数线性相关 
- 给定输入大小 224 × 224,在使用 5 × 5 卷积核的情况下,需要 44 层将输出降低到 4 × 4 
- 需要大量计算才能得到较小输出
- 步幅是指行/列的滑动步长 
  - 例:高度3 宽度2 的步幅

给定高度s_h和宽度s_w的步幅,输出形状是

$$
\left\lfloor \frac{n_h - k_h + p_h + s_h}{s_h} \right\rfloor \times \left\lfloor \frac{n_w - k_w + p_w + s_w}{s_w} \right\rfloor
$$


如果

$$
p_h = k_h - 1, \quad p_w = k_w - 1
$$


$$
\left\lfloor \frac{n_h + s_h - 1}{s_h} \right\rfloor \times \left\lfloor \frac{n_w + s_w - 1}{s_w} \right\rfloor
$$


如果输入高度和宽度可以被步幅整除

$$
\frac{n_h}{s_h} \times \frac{n_w}{s_w}
$$



## 多个输入和输出通道
### 概念
- 输出通道数是卷积层的超参数 
- 每个输入通道有独立的二维卷积核,所有通道结果相加得到一个输出通道结果 
- 每个输出通道有独立的三维卷积核


### 多个输入通道
- 彩色图像可能有 RGB 三个通道
- 转换为灰度会丢失信息
- 每个通道都有一个卷积核,结果是所有通道卷积结果的和

输入

$$
X: c_i \times n_h \times n_w
$$

核

$$
W: c_i \times k_h \times k_w
$$

输出

$$
Y: m_h \times m_w
$$


$$
Y = \sum X_{i,:,:} \star W_{i,: ,:}
$$


### 多个输出通道
无论有多少输入通道,到目前为止我们只用到单输出通道

我们可以有多个三维卷积核,每个核生成一个输出通道 

输入

$$
X: c_i \times n_h \times n_w
$$

核

$$
W: c_o \times c_i \times k_h \times k_w
$$

输出

$$
Y: c_o \times m_h \times m_w
$$


$$
Y = X \star W_{i,:,:} \quad \text{for} \quad i = 1, \dots, C_o
$$


### 多个输入和输出通道
- 每个输出通道可以识别特定模式
- 输入通道核识别并组合输入中的模式

### 1 × 1 卷积层

$$
k_h = k_w = 1
$$

是一个受欢迎的选择。它不识别空间模式,只是融合通道

相当于输入形状为

$$
n_h \times n_w \times c_i
$$

权重为

$$
c_o \times c_i
$$

的全连接层


## 池化层
### 池化层
卷积对位置敏感

需要一定程度的平移不变性

照明,物体位置,比例,外观等等因图像而异

### 池化层特征
- 池化层返回窗口中最大或平均值 
- 缓解卷积层对位置的敏感性 
- 同样有窗口大小、填充、和步幅作为超参数
- 没有可学习的参数

### 二维最大池化
返回滑动窗口中的最大值

### 填充,步幅和多个通道
- 池化层与卷积层类似,都具有填充和步幅 
- 在每个输入通道应用池化层以获得相应的输出通道
- 输出通道数 = 输入通道数

### 平均池化层
- 最大池化层  
每个窗口中最强的模式信号
- 平均池化层  
将最大池化层中的“最大”操作替换为“平均”


## 批量归一化
要解决的问题
- 损失出现在最后,后面的层训练较快 
- 数据在最底部 
  - 底部的层训练较慢 
  - 底部层一变化,所有都得跟着变 
  - 最后的那些层需要重新学习多次 
  - 导致收敛变慢 

*我们可以在学习底部层的时候避免变化顶部层吗？*

### 批量归一化算法
固定小批量里面的均值和方差

$$
\mu_B = \frac{1}{|B|} \sum x_i
$$


$$
(\sigma_B)^2 = \frac{1}{|B|} \sum (x_i - \mu_B)^2 + \epsilon
$$

然后再做额外的调整

$$
x_{i+1} = \gamma \frac{x_i - \mu_B}{\sigma_B} + \beta
$$

可学习的参数
- 方差γ
- 均值β

### 批量归一化层
- 作用在 
  - 全连接层和卷积层输出上,激活函数前 
  - 全连接层和卷积层输入上
- 对全连接层,作用在特征维 
- 对于卷积层,作用在通道维

### 批量归一化本质上是在做什么
最初论文是想用它来减少内部协变量转移 

后续有论文指出它可能就是通过在每个小批量里加入噪音来控制模型复杂度

$$
x_{i+1} = \gamma \frac{x_i - \hat{\mu}_B}{\hat{\sigma}_B} + \beta
$$


- μ_hat_B是随机偏移
- σ_hat_B是随机缩放
**因此批量归一化没必要跟丢弃法混合使用**

### 批量归一化特点
- 批量归一化固定小批量中的均值和方差，然后学习出适合的偏移和缩放 
- 可以加速收敛速度,但一般不改变模型精度


## 现代卷积神经网络
### LeNet
#### MNIST数据集
- 手写数字的黑白图片
- 样本数 60K
- 50,000 个训练数据
- 10,000 个测试数据
- 图像大小 28 × 28 
- 10类
#### LeNet架构
       Dense(10)
          ↑
       Dense(84)
          ↑
       Dense(120)
          ↑
    2×2 AvgPool, stride 2
          ↑
    5×5 Conv(6), pad 2
          ↑
    2×2 AvgPool, stride 2
          ↑
    5×5 Conv(6), pad 2
          ↑
      image(32×32)


LeNet是早期成功的神经网络 

- 先使用卷积层来学习图片空间信息 
- 然后使用全连接层来转换到类别空间

### AlexNet
#### 90年代的机器学习特点
- 特征提取 
- 选择核函数来计算相似性 
- 凸优化问题 
- 漂亮的定理
#### 几何学
- 抽取特征 
- 描述几何(例如多相机) 
- (非)凸优化 
- 漂亮定理 

如果假设满足了,效果很好
#### 特征工程
- 特征工程是关键 
- 特征描述子:SIFT,SURF 
- 视觉词袋(聚类) 
- 最后用 SVM

#### ImageNet (2010)数据集
- 自然物体的彩色图片
- 样本数 1.2M
- 图像大小 469 × 387 
- 1000类

#### AlexNet特点
- AlexNet 赢了 2012 年 ImageNet 竞赛,标志着新的一轮神经网络热潮的开始 
- 更深更大的 LeNet,10x 参数个数,260x 计算复杂度 
- 主要改进 
  - 丢弃法 
  - ReLu 
  - MaxPooling 
- 计算机视觉方法论的改变


#### AlexNet 架构
       Dense(1000)
          ↑
       Dense(4096)
          ↑
       Dense(4096)
          ↑
    3×3 MaxPool, stride 2
          ↑
    3×3 Conv(384), pad 1
          ↑
    3×3 Conv(384), pad 1
          ↑
    3×3 Conv(384), pad 1
          ↑
    3×3 MaxPool, stride 2
          ↑
    5×5 Conv(256), pad 2
          ↑
    3×3 MaxPool, stride 2
          ↑
    11×11 Conv(96), stride 4
          ↑
      image(3×224×224)


#### AlexNet细节
- 更大的池化窗口，使用最大池化层
- 更大的核窗口和步长，因为图片更大了
- 激活函数从 sigmoid 变到了 ReLu (减缓梯度消失) 
- 隐藏全连接层后加入了丢弃层 
- 数据增强

### 使用块的网络VGG
#### 为解决的问题
- AlexNet 比 LeNet 更深更大来得到更好的精度 
- 能不能更深和更大? 
- 选项 
  - 更多的全连接层(太贵) 
  - 更多的卷积层 
  - 将卷积层组合成块

#### VGG块
##### 深 vs. 宽? 
- 5×5 卷积 
- 3×3 卷积 
- 深但窄效果更好 
##### VGG 块 
- 3×3 卷积(填充 1) (n 层, m 通道) 
- 2×2 最大池化层 (步幅 2)

#### VGG 架构
- 多个VGG块后接全连接层 
- 不同次数的重复块得到不同的架构 VGG-16, VGG-19,...

#### LeNet和AlexNet和VGG
##### LeNet (1995) 
- 2 卷积 + 池化层 
- 2 全连接层 
##### AlexNet 
- 更大更深 
- ReLu, Dropout, 数据增强 
##### VGG 
- 更大更深的 AlexNet(重复的 VGG 块)

#### VGG特点
- VGG使用可重复使用的卷积块来构建深度卷积神经网络 
- 不同的卷积块个数和超参数可以得到不同复杂度的变种

### 网络中的网络(NiN)
#### 全连接层的问题
##### 卷积层需要较少的参数

$$
c_i \times c_o \times k^2
$$

##### 但卷积层后的第一个全连接层的参数
###### LeNet
16×5×5×120 = 48k 
###### AlexNet
256×5×5×4096 = 26M 
###### VGG
512×7×7×4096 = 102M

#### NiN 块
- 一个卷积层后跟两个全连接层 
  - 步幅 1,无填充,输出形状跟卷积层输出一样 
  - 起到全连接层的作用

#### NiN架构
- 无全连接层 
- 交替使用NiN块和步幅为2的最大池化层 
  - 逐步减小高宽和增大通道数 
- 最后使用全局平均池化层得到输出 
  - 其输入通道数是类别数

#### NiN特点
- NiN块使用卷积层加两个1×1卷积层
  - 后者对每个像素增加了非线性性 
- NiN使用全局平均池化层来替代VGG和AlexNet中的全连接层 
  - 不容易过拟合,更少的参数个数

### 含并行连结的网络(GoogLeNet)
#### Inception块
Inception块用4条有不同超参数的卷积层和池化层的路来抽取不同的信息，然后在输出通道维合并
- 使用不同窗口大小的卷积层
- 使用池化层
- 输出跟输入等同高宽

跟单3×3或5×5卷积层比,Inception块有更少的参数个数和计算复杂度

它的一个主要优点是模型参数小,计算复杂度低

#### GoogLeNet架构
GoogleNet使用了9个Inception块,是第一个达到上百层的网络
##### Stage 1 & 2
更小的宽口，更多的通道
##### Stage 3
- 通道分配不同
- 输出通道增加
##### Stage 4 & 5
- 增加通道数
- 1024维特征输出

#### Inception 有各种后续变种
##### Inception-BN (v2)
使用 batch normalization
##### Inception-V3
修改了Inception块 
- 替换 5×5 为多个 3×3 卷积层 
- 替换 5×5 为 1×7 和 7×1 卷积层 
- 替换 3×3 为 1×3 和 3×1 卷积层 
- 更深 
##### Inception-V4
使用残差连接


### 残差网络(ResNet)
#### 残差块
- 串联一个层改变函数类,我们希望能扩大函数类 
- 残差块加入快速通道来得到 f(x) = x + g(x) 的结构

#### ResNet 块
- 高宽减半ResNet块(步幅 2) 
- 后接指个高宽不变ResNet块

#### ResNet 架构
- 类似 VGG 和 GoogleNet 的总体架构 
- 但替换成了ResNet 块

#### 残差网络特点
- 残差块使得很深的网络更加容易训练 
  - 甚至可以训练一千层的网络 
- 残差网络对随后的深层神经网络设计出问了深远影响,无论是卷积类网络还是全连接类网络



# 循环神经网络

# 注意力机制

# 优化算法



# 计算机视觉
## 数据增强(Data Augmentation)
### 数据增强概念
增加一个已有数据集,使得有更多的多样性 
- 在语言里面加入各种不同的背景噪音 
- 改变图片的颜色和形状

数据增广通过变形数据来获取多样性，从而使得模型泛化性能更好

### 使用增强数据训练
- 原始数据
- 在线生成增强数据
- 将增强后的数据传入模型训练

### 数据增强方法
- 翻转
  - 左右翻转
  - 上下翻转

*注意：上下翻转不总是可行*
- 切割
从图片中切割一块,然后变形到固定形状 
  - 随机高宽比 (e.g. [3/4, 4/3]) 
  - 随机大小 (e.g. [8%, 100%]) 
  - 随机位置
- 颜色
改变色调,饱和度,明切度(e.g. [0.5, 1.5])
- 几十种其他的办法


## 微调Fine Tune
### 微调
- 标注一个数据集很贵
- 微调通过使用在大数据上得到的预训练好的模型来初始化模型权重，从而完成提升精度 
- 预训练模型质量很重要 
- 微调通常速度更快、精度更高

### 网络架构
- 一个神经网络一般可以分成两块 
  - 特征抽取将原始像素变成容易线性分割的特征 
  - 线性分类器来做分类
- 微调中，Output Layer不能直接使用，因为标号可能变了
- 特征抽取层可能仍然可以对我数据集做特征抽取

### 微调中的权重初始化
- Layer 1到 Layer L-1
直接copy权重

- Output Layer
Random Initialization

### 训练
是一个目标数据集上的正常训练任务，但使用更强的正则化 
- 使用更小的学习率 
- 使用更少的数据迭代

源数据集远复杂于目标数据,通常微调效果更好

### 重用分类器权重
- 源数据集可能也有目标数据中的部分标号 
- 可以使用预训练好模型分类器中对应标号对应的向量来做初始化

### 固定一些层
- 神经网络通常学习有层次的特征表示 
  - 低层次的特征更加通用 
  - 高层次的特征则更跟数据集相关 
- 可以固定底部一些层的参数，不参与更新 
  - 更强的正则

## 目标检测
### 基本概念
- 物体检测识别图片里的多个物体的类别和位置 
- 位置通常用边缘框表示
### 边缘框
一个边缘框可以通过4个数字定义
- (左上x, 左上y, 右下x, 右下y) 
- (左上x, 左上y, 宽, 高)

### 目标检测数据集
#### COCO (cocodataset.org) 
- 80物体
- 330K图片
- 1.5M 物体
##### 每行表示一个物体 
- 图片文件名, 物体类别, 边缘框 

### 锚框
#### 概念
- 一类目标检测算法基于锚框来预测 
- 首先生成大量锚框,并赋予标号,每个锚框作为一个样本进行训练 
- 在预测时,使用NMS来去掉冗余的预测

一类目标检测算法是基于锚框 
- 提出多个被称为锚框的区域(边缘框) 
- 预测每个锚框里是否含有关注的物体 
- 如果是，预测从这个锚框到真实边缘框的偏移

#### IoU - 交并比
IoU 用来计算两个框之间的相似度 
- 0 表示无重叠,1 表示重合 

这是 Jacquard 指数的一个特殊情况 
- 给定两个集合 A 和 B 


$$
J(A, B) = \frac{|A \cap B|}{|A \cup B|}
$$


#### 赋予锚框标号
每个锚框是一个训练样本 
- 将每个锚框，要么标注成背景，要么关联上一个真实边缘框 
- 我们可能会生成大量的锚框 
  - 这导致大量的负类样本

#### 使用非极大值抑制(NMS)输出
每个锚框预测一个边缘框 

NMS可以合并相似的预测 
- 选中是非背景类的最大预测值 
- 去掉所有其它和它IoU值大于 θ的预测 
- 重复上述过程直到所有预测要么被选中，要么被去掉


### 区域卷积神经网络R-CNN
#### 相关概念
- R-CNN 是最早、也是最有名的一类基于锚框和CNN的目标检测算法 
- Fast/Faster R-CNN持续提升性能 
- Faster R-CNN 和 Mask R-CNN是在最求高精度场景下的常用算法
#### R-CNN概念
- 使用启发式搜索算法来选择锚框 
- 使用预训练模型来对每个锚框抽取特征 
- 训练一个SVM来对类别分类 
- 训练一个线性回归模型来预测边缘框偏移

#### 兴趣区域(RoI)池化层
给定一个锚框,均匀分割成n×m块,输出每块里的最大值

不管锚框多大,总是输出nm个值

#### Fast RCNN
- 使用CNN对图片抽取特征 
- 使用RoI池化层对每个锚框固定长度特征

#### Faster R-CNN
使用一个区域提议网络来替代启发式搜索来获得更好的锚框

#### Mask R-CNN
如果有像素级别的标号,使用FCN来利用这些信息


### 单发多框检测(SSD)
#### SSD相关概念
- SSD通过单神经网络来检测模型 
- 以每个像素为中心的产生多个锚框 
- 在多个段的输出上进行多尺度的检测
#### 生成锚框
- 对每个像素,生成多个以它为中心的锚框 
- 给定n个大小s_1, ..., s_n和m个高宽比,那么生成 n+m-1个锚框,其大小和高宽比分别为:  


$$
(s_1, r_1), (s_2, r_1), \dots, (s_n, r_1), (s_1, r_2), \dots, (s_1, r_m)
$$


#### SSD 模型
一个基础网络来抽取特征,然后多个卷积层块来减半高宽 

在每段都生成锚框 
- 底部段来拟合小物体,顶部段来拟合大物体 

对每个锚框预测类别和边缘框


### YOLO(你只看一次)
- SSD中锚框大量重叠,因此浪费了很多计算 
- YOLO 将图片均匀分成S×S个锚框 
- 每个锚框预测B个边缘框 
- 后续版本(V2,V3,V4…)有持续改进

## 语义分割
语义分割将图片中的每个像素分类到对应的类别
### 语义分割的应用
- 背景虚化
- 路面分割

### 语义分割和实例分割

### 转置卷积
卷积不会增大输入的高宽,通常要么不变、要么减半 

转置卷积则可以用来增大输入高宽  

$$
Y[i : i + h, j : j + w] \mathrel{+}= X[i, j] \cdot K
$$


转置卷积是一种变化了输入和核的卷积,来得到上采样的目的
#### “转置”含义
对于卷积  

$$
Y = X \star W
$$


- 可以对W构造一个V,使得卷积等价于矩阵乘法


$$
Y' = V X'
$$


  - 这里Y′, X′ 是Y, X对应的向量版本 
- 转置卷积则等价于


$$
Y' = V^\top X'
$$


- 如果卷积将输入从(h,w)变成了 (h′,w′)
  - 同样超参数的转置卷积则从(h′,w′) 变成 (h,w)

#### 转置卷积是一种卷积 
- 它将输入和核进行了重新排列 
- 同卷积一般是做下采样不同，它通常用作上采样 
- 如果卷积将输入从(h,w)变成了 (h′,w′),同样超参数下它将(h′,w′) 变成 (h,w)

#### 重新排列输入和核
当填充为0,步幅为1时 
- 将输入填充k-1(k是核窗口) 
- 将核矩阵上下、左右翻转 
- 然后做正常卷积(填充0、步幅1)

当填充为p,步幅为1时 
- 将输入填充k-p-1(k是核窗口) 
- 将核矩阵上下、左右翻转 
- 然后做正常卷积(填充0、步幅1)

当填充为p,步幅为s时 
- 在行和列之间插入s-1行或列 
- 将输入填充k-p-1(k是核窗口) 
- 将核矩阵上下、左右翻转 
- 然后做正常卷积(填充0、步幅1)

#### 形状换算
输入高(宽)为n,核k,填充p,步幅s 

转置卷积 

$$
n' = s n + k - 2p - s
$$


卷积 

$$
n' = \left\lfloor \frac{n - k - 2p + s}{s} \right\rfloor \Rightarrow n \geq s n' + k - 2p - s
$$

如果让高宽成倍增加,那么

$$
k = 2p + s
$$


#### 同反卷积的关系
数学上的反卷积(deconvolution)是指卷积的逆运算
   
$$
Y = \text{conv}(X, K) \quad \Rightarrow \quad X = \text{deconv}(Y, K)
$$

反卷积很少用在深度学习中 
- 我们说的反卷积神经网络指用了转置卷积的神经网络，不等同于数学上的反卷积操作

### 全连接卷积神经网络(FCN)
- FCN是用深度神经网络来做语义分割的奠基性工作
- 它用转置卷积层来替换CNN最后的全连接层,从而可以实现每个像素的预测


## 样式迁移
将样式图片中的样式迁移到内容图片上,得到合成图片
### 基于CNN的样式迁移


# 自然语言处理


















































































































































