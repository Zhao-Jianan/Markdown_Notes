# 机器学习概述
## 人工智能概述
### 机器学习与人工智能、深度学习
机器学习和人工智能，深度学习的关系
- 机器学习是人工智能的一个实现途径
- 深度学习是机器学习的一个方法发展而来

达特茅斯会议-人工智能的起点
- 1956年8月，在美国汉诺斯小镇达特茅斯学院，
- 约翰·麦卡锡（John McCarthy）
- 马文·闵斯基（Marvin Minsky，人工智能与认知学专家）
- 克劳德·香农（Claude Shannon，信息论的创始人）
- 艾伦·纽厄尔（Allen Newell，计算机科学家）
- 赫伯特·西蒙（Herbert Simon，诺贝尔经济学奖得主）

用机器来模仿人类学习以及其他方面的智能
- 1956年是人工智能元年
### 机器学习、深度学习应用场景
机器学习的应用场景非常多，可以说渗透到了各个行业领域当中。医疗、航空、教育、物流、电商等等领域的各种场景
- 用在挖掘、预测领域  
应用场景：店铺销量预测、量化投资、广告推荐、企业客户分类、SQL语句安全检测分类…
- 用在图像领域  
应用场景：街道交通标志检测、人脸识别等等
- 用在自然语言处理领域  
应用场景：文本分类、情感分析、自动聊天、文本检测等等


## 什么是机器学习
### 定义
机器学习是从数据中自动分析获得模型，并利用模型对未知数据进行预测
### 解释
- 我们人从大量的日常经验中归纳规律，当面临新的问题的时候，就可以利用以往总结的规律去分析现实状况，采取最佳策略
- 从数据（大量的猫和狗的图片）中自动分析获得模型（辨别猫和狗的规律），从而使机器拥有识别猫和狗的能力
- 从数据（房屋的各种信息）中自动分析获得模型（判断房屋价格的规律），从而使机器拥有预测房屋价格的能力
### 数据集构成
结构：特征值+目标值
- 对于每一行数据可以称之为样本
- 有些数据集可以没有目标值

## 机器学习算法分类
### 监督学习(supervised learning)（预测）
#### 定义
输入数据是由输入特征值和目标值所组成。函数的输出可以是一个连续的值(称为回归），或是输出是有限个离散值（称作分类）
#### 分类
分类问题目标值为离散值
- k-近邻算法
- 贝叶斯分类
- 决策树与随机森林
- 逻辑回归
- 神经网络
#### 回归
回归问题目标值为连续值
- 线性回归
- 岭回归
### 无监督学习(unsupervised learning)
#### 定义
输入数据是由输入特征值所组成
##### 聚类
k-means

## 机器学习开发流程
- 原始数据
- 数据特征工程
  - 训练数据和测试数据
- 算法进行学习
  - 模型选择
  - 模型评估
    - 测试数据
  - 判断模型是否合格
- 模型应用

## 机器学习步骤
- 获取数据集
- 数据处理
- 特征⼯程
- 机器学习
- 模型评估


# 特征工程
## 数据集
### 可用数据集
#### Kaggle
网址  
https://www.kaggle.com/datasets


特点
-  大数据竞赛平台
-  80万科学家
-  真实数据
-  数据量巨大

#### UCI数据集
网址  
http://archive.ics.uci.edu/ml/


特点
-  收录了360个数据集
-  覆盖科学、生活、经济等领域
-  数据量几十万

#### scikit-learn
网址  
http://scikit-learn.org/stable/datasets/index.html#datasets


特点
-  数据量较小
-  方便学习

### 数据集的划分
机器学习一般的数据集会划分为两个部分
- 训练数据  
用于训练，构建模型
- 测试数据  
在模型检验时使用，用于评估模型是否有效

划分比例
- 训练集  
70% 80% 75%
- 测试集  
30% 20% 30%

数据集划分API
```
sklearn.model_selection.train_test_split(arrays, *options)
```
-  ```x```
数据集的特征值
-  ```y```
数据集的标签值
-  ```test_size```
测试集的大小，一般为float
-  ```random_state```
随机数种子,不同的种子会造成不同的随机采样结果。相同的种子采样结果相同。
-  ```return```
测试集特征训练集特征值值，训练标签，测试标签(默认随机取)

例如：
```
# 对鸢尾花数据集进行分割
# 训练集的特征值x_train 测试集的特征值x_test
# 训练集的目标值y_train 测试集的目标值y_test
x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=22)

print("x_train:\n", x_train.shape)
```

## Scikit-learn
### 介绍
- Python语言的机器学习工具
- Scikit-learn包括许多知名的机器学习算法的实现
- Scikit-learn文档完善，容易上手，丰富的API
### Scikit-learn包含的内容
Scikit-learn接口
- 分类、聚类、回归
- 特征工程
- 模型选择、调优
### sklearn数据集
#### scikit-learn数据集API
##### sklearn.datasets
-  加载获取流行数据集
-  ```datasets.load_*()```
获取小规模数据集，数据包含在datasets里
-  ```datasets.fetch_*(data_home=None)```
获取大规模数据集，需要从网络上下载，函数的第一个参数是data_home，表示数据集下载的目录,默认是 ~/scikit_learn_data/

#### sklearn小数据集
##### ```sklearn.datasets.load_iris()```
加载并返回鸢尾花数据集

##### ```sklearn.datasets.load_boston()```
加载并返回波士顿房价数据集

#### sklearn大数据集
##### ```sklearn.datasets.fetch_20newsgroups(data_home=None,subset=‘train’)```
-  ```subset```
'train'或者'test'，'all'，可选，选择要加载的数据集。
-  训练集的“训练”，测试集的“测试”，两者的“全部”

### sklearn数据集的使用
#### sklearn数据集返回值
load和fetch返回的数据类型 ```datasets.base.Bunch``` (字典格式)
-  data
特征数据数组，是 ```[n_samples * n_features]``` 的二维 ```numpy.ndarray``` 数组
-  target
标签数组，是 ```n_samples``` 的一维 ```numpy.ndarray``` 数组
-  DESCR
数据描述
-  feature_names
特征名,新闻数据，手写数字、回归数据集没有
-  target_names
标签名

例如：
```
from sklearn.datasets import load_iris
# 获取鸢尾花数据集
iris = load_iris()
print("鸢尾花数据集的返回值：\n", iris)
# 返回值是一个继承自字典的Bench
print("鸢尾花的特征值:\n", iris["data"])
print("鸢尾花的目标值：\n", iris.target)
print("鸢尾花特征的名字：\n", iris.feature_names)
print("鸢尾花目标值的名字：\n", iris.target_names)
print("鸢尾花的描述：\n", iris.DESCR)
```

## 特征工程介绍
### 需要特征工程(Feature Engineering)的原因
Andrew Ng(吴恩达)老师说“Coming up with features is difficult, time-consuming, requires expert knowledge. “Applied machine learning” is basically feature engineering. ”

数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已

### 特征工程的定义
特征工程是使用专业背景知识和技巧处理数据，使得特征能在机器学习算法上发挥更好的作用的过程
#### 意义
会直接影响机器学习的效果

### 特征工程的位置与数据处理的比较
pandas(Prepare Data) ----> Scikit-learn(Feature Engineering) --> Scikit-learn(Learn from Data)

#### pandas
一个数据读取非常方便以及基本的处理格式的工具
#### sklearn
对于特征的处理提供了强大的接口

### 特征工程包含内容
- 特征提取（Feature extraction）
- 特征预处理（Feature preprocessing）
- 特征降维（Feature dimensionality reduction）

## 特征提取
### 定义
将任意数据（如文本或图像）转换为可用于机器学习的数字特征  

*特征值化是为了计算机更好的去理解数据*

- 字典特征提取(特征离散化)
- 文本特征提取
- 图像特征提取
#### 特征提取API
```
sklearn.feature_extraction
```

### 字典特征提取
#### 作用
对字典数据进行特征值化
#### API
```
sklearn.feature_extraction.DictVectorizer(sparse=True,…)
```
```DictVectorizer.fit_transform(X)```
X:字典或者包含字典的迭代器返回值：返回sparse矩阵

```DictVectorizer.inverse_transform(X)```
X:array数组或者sparse矩阵 返回值:转换之前数据格式

```DictVectorizer.get_feature_names() ```
返回类别名称

#### 应用
对以下数据进行特征提取
[{'city': '北京','temperature':100}
{'city': '上海','temperature':60}
{'city': '深圳','temperature':30}]

##### 流程
-  实例化类 ```DictVectorizer```
-  调用 ```fit_transform``` 方法输入数据并转换（注意返回格式）
```
from sklearn.feature_extraction import DictVectorizer

def dict_demo():
    """
    对字典类型的数据进行特征抽取
    :return: None
    """
    data = [{'city': '北京','temperature':100}, {'city': '上海','temperature':60}, {'city': '深圳','temperature':30}]
    # 1、实例化一个转换器类
    transfer = DictVectorizer(sparse=False)
    # 2、调用fit_transform
    data = transfer.fit_transform(data)
    print("返回的结果:\n", data)
    # 打印特征名字
    print("特征名字：\n", transfer.get_feature_names())

    return None
```

这个处理数据的技巧叫做”one-hot“编码

**对于特征当中存在类别信息的都要做one-hot编码处理**

### 文本特征提取
#### 作用
对文本数据进行特征值化
#### API
```
sklearn.feature_extraction.text.CountVectorizer(stop_words=[])
```
返回词频矩阵

```CountVectorizer.fit_transform(X)```
- X:文本或者包含文本字符串的可迭代对象
- 返回值：返回sparse矩阵

```CountVectorizer.inverse_transform(X)```
- X:array数组或者sparse矩阵 返回值:转换之前数据格

```CountVectorizer.get_feature_names()```
- 返回值:单词列表
```
sklearn.feature_extraction.text.TfidfVectorizer
```

#### 应用
对以下数据进行特征提取
["life is short,i like python",
"life is too long,i dislike python"]
##### 流程
-  实例化类CountVectorizer
-  调用fit_transform方法输入数据并转换 （注意返回格式，利用toarray()进行sparse矩阵转换array数组）
```
from sklearn.feature_extraction.text import CountVectorizer

def text_count_demo():
    """
    对文本进行特征抽取，countvetorizer
    :return: None
    """
    data = ["life is short,i like like python", "life is too long,i dislike python"]
    # 1、实例化一个转换器类
    # transfer = CountVectorizer(sparse=False)
    transfer = CountVectorizer()
    # 2、调用fit_transform
    data = transfer.fit_transform(data)
    print("文本特征抽取的结果：\n", data.toarray())
    print("返回特征名字：\n", transfer.get_feature_names())

    return None
```
#### jieba分词处理
##### API
```
jieba.cut()
```
-  返回词语组成的生成器
*需要安装jieba库*

#### Tf-idf文本特征提取
##### TF-IDF的主要思想
如果某个词或短语在一篇文章中出现的概率高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类
##### TF-IDF作用
用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度
##### 公式
-  词频（term frequency，tf）
指的是某一个给定的词语在该文件中出现的频率
-  逆向文档频率（inverse document frequency，idf）

是一个词语普遍重要性的度量。某一特定词语的idf，可以由总文件数目除以包含该词语之文件的数目，再将得到的商取以10为底的对数得到  
$$
\text{tfidf}_{i,j} = \text{tf}_{i,j} \cdot \text{idf}_i
$$

最终得出结果可以理解为重要程度

##### Tf-idf的重要性
分类机器学习算法进行文章分类中前期数据处理方式


## 特征预处理
### 特征预处理定义
#### scikit-learn的解释
provides several common utility functions and transformer classes to change raw feature vectors into a representation that is more suitable for the downstream estimators

通过一些转换函数将特征数据转换成更加适合算法模型的特征数据过程


#### 包含内容
- 数值型数据的无量纲化
  -  归一化
  -  标准化
#### 特征预处理API
```
sklearn.preprocessing
```

#### 进行归一化/标准化的原因
特征的单位或者大小相差较大，或者某特征的方差相比其他的特征要大出几个数量级，容易影响（支配）目标结果，使得一些算法无法学习到其它的特征

需要用一些方法进行无量纲化，使不同规格的数据转换到同一规格

### 归一化(Normalized)
#### 定义
通过对原始数据进行变换把数据映射到(默认为[0,1])之间
#### 公式
$$
X' = \frac{x - \min}{\max - \min}
$$
$$
X'' = X' \cdot (\text{mx} - \text{mi}) + \text{mi}
$$

作用于每一列，max为一列的最大值，min为一列的最小值,那么$X''$为最终结果，$\text{mx}，\text{mi}$ 分别为指定区间值默认 $\text{mx}$ 为1, $\text{mi}$ 为0

#### API
```
sklearn.preprocessing.MinMaxScaler (feature_range=(0,1)… )
```
```
MinMaxScalar.fit_transform(X)
```
-  X
numpy array格式的数据[n_samples,n_features]

##### 返回值
转换后的形状相同的array

#### 归一化总结
- 注意最大值最小值是变化的
- 另外，最大值与最小值非常容易受异常点影响
- 所以这种方法鲁棒性较差，只适合传统精确小数据场景


### 标准化(Standardization)
#### 定义
通过对原始数据进行变换把数据变换到均值为0,标准差为1范围内
#### 公式
$$
X' = \frac{x - \text{mean}}{\sigma}
$$

作用于每一列，mean为平均值，σ 为标准差

#### 异常点影响
- 对于归一化来说：如果出现异常点，影响了最大值和最小值，那么结果显然会发生改变
- 对于标准化来说：如果出现异常点，由于具有一定数据量，少量的异常点对于平均值的影响并不大，从而方差改变较小

#### API
```
sklearn.preprocessing.StandardScaler( )
```
处理之后每列来说所有数据都聚集在均值0附近标准差为1
```
StandardScaler.fit_transform(X)
```
-  X
numpy array格式的数据[n_samples,n_features]

- 返回值：转换后的形状相同的array

#### 标准化总结
在已有样本足够多的情况下比较稳定，适合现代嘈杂大数据场景


## 特征降维
### 降维
降维是指在某些限定条件下，降低随机变量(特征)个数，得到一组“不相关”主变量的过程
- 降低随机变量的个数
- 相关特征(correlated feature)
  - 相对湿度与降雨量之间的相关
  - 等等

*正是因为在进行训练的时候，我们都是使用特征进行学习。如果特征本身存在问题或者特征之间相关性较强，对于算法学习预测会影响较大*

### 降维的两种方式
- 特征选择
- 主成分分析（可以理解一种特征提取的方式）

### 特征选择
#### 定义
数据中包含冗余或无关变量（或称特征、属性、指标等），旨在从原有特征中找出主要特征
#### 方法
##### Filter(过滤式)
主要探究特征本身特点、特征与特征和目标值之间关联
-  方差选择法
低方差特征过滤
-  相关系数
##### Embedded (嵌入式)
算法自动选择特征（特征与目标值之间的关联）
-  决策树
信息熵、信息增益
-  正则化
L1、L2
-  深度学习
卷积等

#### 模块
```
sklearn.feature_selection
```
#### 过滤式
##### 低方差特征过滤
-  定义
删除低方差的一些特征
  - 特征方差小  
  某个特征大多样本的值比较相近
  - 特征方差大  
  某个特征很多样本的值都有差别
##### API
```
sklearn.feature_selection.VarianceThreshold(threshold = 0.0)
```
删除所有低方差特征
```Variance.fit_transform(X)```
- X  
numpy array格式的数据[n_samples,n_features]
- 返回值  
训练集差异低于threshold的特征将被删除。默认值是保留所有非零方差特征，即删除所有样本中具有相同值的特征

##### 相关系数
皮尔逊相关系数(Pearson Correlation Coefficient)
反映变量之间相关关系密切程度的统计指标
-  公式
-  特点

相关系数的值介于–1与+1之间，即–1≤ r ≤+1。其性质如下：
- 当r>0时，表示两变量正相关，r<0时，两变量为负相关
- 当|r|=1时，表示两变量为完全相关，当r=0时，表示两变量间无相关关系
- 当0<|r|<1时，表示两变量存在一定程度的相关。且|r|越接近1，两变量间线性关系越密切；|r|越接近于0，表示两变量的线性相关越弱
- 一般可按三级划分：|r|<0.4为低度相关；0.4≤|r|<0.7为显著性相关；0.7≤|r|<1为高度线性相关

API
```
from scipy.stats import pearsonr
x : (N,) array_like
y : (N,) array_like Returns:(Pearson’s correlation coefficient, p-value)
```

## 主成分分析(PCA)
### 定义
高维数据转化为低维数据的过程，在此过程中可能会舍弃原有数据、创造新的变量
### 作用
是数据维数压缩，尽可能降低原数据的维数（复杂度），损失少量信息
### 应用
回归分析或者聚类分析当中

### API
```
sklearn.decomposition.PCA(n_components=None)
```
#### 将数据分解为较低维数空间
```
n_components
```
- 小数
表示保留百分之多少的信息
- 整数
减少到多少特征
```
PCA.fit_transform(X)
```
- X
numpy array格式的数据[n_samples,n_features]
- 返回值
转换后指定维度的array


# 分类算法
## 数据集的介绍与划分
### 数据集划分
#### 概念
机器学习一般的数据集会划分为两个部分
-  训练数据：用于训练，构建模型
-  测试数据：在模型检验时使用，用于评估模型是否有效

划分比例
-  训练集：70% 80% 75%
-  测试集：30% 20% 30%

#### API
```
sklearn.model_selection.train_test_split(arrays, *options)
```
##### x
数据集的特征值
##### y
数据集的标签值
##### test_size
测试集的大小，一般为float
##### random_state
随机数种子,不同的种子会造成不同的随机采样结果。相同的种子采样结果相同
##### return
返回 测试集特征训练集特征值值，训练标签，测试标签(默认随机取)，即x_train, x_test, y_train, y_test


## sklearn转换器和估计器
### 转换器(Transformer)
#### 特征工程的步骤
##### 第1步
实例化 (实例化的是一个转换器类(Transformer))
##### 第2步
调用fit_transform(对于文档建立分类词频矩阵，不能同时调用)

#### 特征工程的接口称之为转换器，其中转换器调用有这么几种形式
##### fit_transform
##### fit
计算 每一列的平均值、标准差
##### transform
(x - mean) / std 进行最终的转换


### 估计器(estimator)
sklearn机器学习算法的实现
在sklearn中，估计器(estimator)是一个重要的角色，是一类实现了算法的API
#### 估计器类别
##### 用于分类的估计器
```
sklearn.neighbors
```
k-近邻算法
```
sklearn.naive_bayes
```
贝叶斯
```
sklearn.linear_model.LogisticRegression
```
逻辑回归
```
sklearn.tree
```
决策树与随机森林
##### 用于回归的估计器
```
sklearn.linear_model.LinearRegression
```
线性回归
```
sklearn.linear_model.Ridge
```
岭回归
##### 用于无监督学习的估计器
```
sklearn.cluster.KMeans
```
聚类

#### 估计器工作流程
##### 实例化一个estimator
```
estimator.fit(x_train, y_train)
```
调用完毕，模型生成
##### 模型评估
-  直接比对真实值和预测值
```
y_predict = estimator.predict(x_test)
y_test == y_predict
```
-  计算准确率
```
accuracy = estimator.score(x_test, y_test)
```

## K-近邻算法
k近邻算法（k-Nearest Neighbors, 简称k-NN）是一种简单且常用的监督学习算法，主要用于分类和回归任务
### 定义
如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别

**KNN算法最早是由Cover和Hart提出的一种分类算法**

### 距离公式
#### 欧式距离（Euclidean Distance）
##### 定义
欧氏距离是最常用的距离度量方法，用于计算两点之间的直线距离。它可以用于任何维度的空间
##### 公式

$$
d(x, y) = \sqrt{\sum (x_i - y_i)^2}
$$

-  其中，x 和 y 分别是两个点的坐标，n是维度数

##### 特点
-  计算点与点之间的直线距离
-  对于高维数据，欧氏距离可能会受到“维度灾难”的影响，导致距离计算不准确

#### 曼哈顿距离（Manhattan Distance）
##### 定义
曼哈顿距离是两点之间在各个维度上绝对差值的和。它也被称为“城市街区距离”或“L1距离”，因为它像是在网格状的街道中走的距离
##### 公式
$$
d(x, y) = \sum |x_i - y_i|
$$

-  其中，x 和 y 分别是两个点的坐标，n是维度数
##### 特点
-  适用于高维空间和稀疏数据
-  由于计算的是轴对齐的绝对距离，可能比欧氏距离对噪声更具鲁棒性
#### 明可夫斯基距离
明可夫斯基距离（Minkowski Distance）
##### 定义
明可夫斯基距离是欧氏距离和曼哈顿距离的广义形式，通过引入一个参数p，它可以表示不同的距离度量方法
##### 公式
$$
d(x, y) = \left( \sum |x_i - y_i|^p \right)^{\frac{1}{p}}
$$

-  其中，x 和 y 分别是两个点的坐标，n是维度数，p是参数
##### 特点
-  当 p=1 时，明可夫斯基距离等同于曼哈顿距离
-  当 p=2 时，明可夫斯基距离等同于欧氏距离。
-  p 越大，距离度量越接近最大值度量（极大距离）
#### 比较
- 欧氏距离适用于计算直线距离，适合于均匀的连续数据
- 曼哈顿距离适用于高维空间和稀疏数据，具有鲁棒性
- 明可夫斯基距离通过调节参数 p可以灵活适应不同的场景

### K-近邻算法API
```
sklearn.neighbors.KNeighborsClassifier(n_neighbors=5,algorithm='auto')
```

#### ```n_neighbors```
int,可选（默认= 5），k_neighbors查询默认使用的邻居数
#### ```algorithm```
```{‘auto’，‘ball_tree’，‘kd_tree’，‘brute’}```，可选用于计算最近邻居的算法：```'ball_tree'``` 将会使用 ```BallTree```，```‘kd_tree’``` 将使用 ```KDTree``` 。```‘auto’``` 将尝试根据传递给fit方法的值来决定最合适的算法。 (不同实现方式影响效率)


### KNN中K值大小选择对模型的影响
#### K值过小
- 容易受到异常点的影响
- 容易过拟合
#### k值过大
- 受到样本均衡的问题
- 容易欠拟合

### K-近邻算法优缺点
#### 优点
- 简单有效
- 重新训练的代价低
- 适合类域交叉样本
  - KNN方法主要靠周围有限的邻近的样本,而不是靠判别类域的方法来确定所属类别的，因此对于类域的交叉或重叠较多的待分样本集来说，KNN方法较其他方法更为适合
- 适合大样本自动分类
  - 该算法比较适用于样本容量比较大的类域的自动分类，而那些样本容量较小的类域采用这种算法⽐较容易产生误分
#### 缺点
- 惰性学习
KNN算法是懒散学习方法（lazy learning,基本上不学习），一些积极学习的算法要快很多
- 类别评分不是规格化
不像一些通过概率评分的分类
- 输出可解释性不强
例如决策树的输出可解释性就较强
- 对不均衡的样本不擅长
当样本不平衡时，如一个类的样本容量很大，而其他类样本容量很小时，有可能导致当输入一个新样本时，该样本的K个邻居中大容量类的样本占多数。该算法只计算“最近的”邻居样本，某一类的样本数量很大，那么或者这类样本并不接近目标样本，或者这类样本很靠近目标样本。无论怎样，数量并不能影响运行结果。可以采用权值的方法（和该样本距离小的邻居权值大）来改进
- 计算量较大
目前常用的解决方法是事先对已知样本点进行剪辑，事先去除对分类作用不大的样本

#### 使用场景
小数据场景，几千～几万样本

## kd树
### K-近邻算法性能问题
- 实现k近邻算法时，主要考虑的问题是如何对训练数据进行快速k近邻搜索，这在特征空间的维数大及训练数据容量大时尤其必要
- k近邻法最简单的实现是线性扫描（穷举搜索），即要计算输入实例与每一个训练实例的距离。计算并存储好以后，再查找K近邻。当训练集很大时，计算非常耗时
- 为了提高kNN搜索的效率，可以考虑使用特殊的结构存储训练数据，以减小计算距离的次数
- 根据KNN每次需要预测一个点时，我们都需要计算训练数据集里每个点到这个点的距离，然后选出距离最近的k个点进行投票。当数据集很大时，这个计算成本非常高，针对N个样本，D个特征的数据集，其算法复杂度为 $O(DN^2)$


### kd树概念
为了避免每次都重新计算一遍距离，算法会把距离信息保存在一棵树里，这样在计算之前从树里查询距离信息，尽量避免重新计算。其基本原理是，如果A和B距离很远，B和C距离很近，那么A和C的距离也很远。有了这个信息，就可以在合适的时候跳过距离远的点

这样优化后的算法复杂度可降低到O(DNlog(N))

1989年，另外一种称为Ball Tree的算法，在kd Tree的基础上对性能进一步进行了优化

### kd树原理
#### 树的建立
黄色的点作为根节点，上面的点归左子树，下面的点归右子树，接下来再不断地划分，分割的那条线叫做分割超平面(splitting hyperplane)，在一维中是一个点，二维中是线，三维的是面
#### 最近邻域搜索（Nearest-Neighbor Lookup）
- kd树(K-dimension tree)是一种对k维空间中的实例点进行存储以便对其进行快速检索的树形数据结构
- kd树是一种二叉树，表示对k维空间的一个划分，构造kd树相当于不断地用垂直于坐标轴的超平面将K维空间切分，构成一系列的K维超矩形区域
- kd树的每个结点对应于一个k维超矩形区域。利用kd树可以省去对大部分数据点的搜索，从而减少搜索的计算量

### 构造方法
- 构造根结点，使根结点对应于K维空间中包含所有实例点的超矩形区域
- 通过递归的方法，不断地对k维空间进行切分，生成子结点。在超矩形区域上选择一个坐标轴和在此坐标轴上的一个切分点，确定一个超平面，这个超平面通过选定的切分点并垂直于选定的坐标轴，将当前超矩形区域切分为左右两个子区域（子结点）；这时，实例被分到两个子区域
- 上述过程直到子区域内没有实例时终止（终止时的结点为叶结点）。在此过程中，将实例保存在相应的结点上
- 通常，循环的选择坐标轴对空间切分，选择训练实例点在坐标轴上的中位数为切分点，这样得到的kd树是平衡的（平衡二叉树：它是一棵空树，或其左子树和右子树的深度之差的绝对值不超过1，且它的左子树和右子树都是平衡二叉树）

KD树中每个节点是一个向量，和二叉树按照数的大小划分不同的是，KD树每层需要选定向量中的某一维，然后根据这一维按左小右大的方式划分数据。在构建KD树时，关键需要解决2个问题
- 选择向量的哪一维进行划分
- 如何划分数据
  - 第一个问题简单的解决方法可以是随机选择某一维或按顺序选择，但是更好的方法应该是在数据比较分散的那一维进行划分（分散的程度可以根据方差来衡量）
  - 第二个问题中，好的划分方法可以使构建的树比较平衡，可以每次选择中位数来进行划分


### kd树的构建过程
#### 第1步
构造根节点
#### 第2步
通过递归的方法，不断地对k维空间进行切分，生成子节点
#### 第3步
重复第二步骤，直到子区域中没有示例时终止
#### 需要关注细节
- 选择向量的哪一维进行划分
- 如何划分数据
### kd树的搜索过程
#### 第1步
二叉树搜索比较待查询节点和分裂节点的分裂维的值，（小于等于就进入左子树分支，大于就进入右子树分支直到叶子结点）
#### 第2步
顺着“搜索路径”找到最近邻的近似点
#### 第3步
回溯搜索路径，并判断搜索路径上的结点的其他子结点空间中是否可能有距离查询点更近的数据点，如果有可能，则需要跳到其他子结点空间中去搜索
#### 第4步
重复这个过程直到搜索路径为空


## 模型选择与调优
### 交叉验证(cross validation)
#### 交叉验证的目的
为了让被评估的模型更加准确可信
#### 交叉验证的概念
将拿到的训练数据，分为训练和验证集

例如：将数据分成X份，其中一份作为验证集。然后经过X次(组)的测试，每次都更换不同的验证集。即得到X组模型的结果，取平均值作为最终结果。又称X折交叉验证

#### 为了让从训练得到模型结果更加准确，要做以下处理
- 训练集：训练集+验证集
- 测试集：测试集

### 网格搜索(Grid Search)
通常情况下，有很多参数是需要手动指定的（如k-近邻算法中的K值），这种叫超参数。但是手动过程繁杂，所以需要对模型预设几种超参数组合。每组超参数都采用交叉验证来进行评估。最后选出最优参数组合建立模型

### 交叉验证，网格搜索（模型选择与调优）API
```
sklearn.model_selection.GridSearchCV(estimator, param_grid=None,cv=None)
```
- 对估计器的指定参数值进行详尽搜索
- estimator：估计器对象
- param_grid：估计器参数(dict){“n_neighbors”:[1,3,5]}
- cv：指定几折交叉验证
- fit：输入训练数据
- score：准确率
- 结果分析
  -  bestscore
在交叉验证中验证的最好结果
  -  bestestimator
最好的参数模型
  -  cvresults
每次交叉验证后的验证集准确率结果和训练集准确率结果


## 朴素贝叶斯算法
### 概率基础
#### 概率(Probability)定义
概率定义为一件事情发生的可能性
-  扔出一个硬币，结果头像朝上
-  某天是晴天
P(X): 取值在[0, 1]
#### 条件概率与联合概率
##### 联合概率
包含多个条件，且所有条件同时成立的概率
-  记作
$$
P(A, B)
$$

-  特性
$$
P(A, B) = P(A) P(B)
$$


##### 条件概率
事件A在另外一个事件B已经发生条件下的发生概率
-  记作
$$
P(A \mid B)
$$

-  特性
$$
P(A_1, A_2 \mid B) = P(A_1 \mid B) P(A_2 \mid B)
$$

*注意：此条件概率的成立，是由于A1,A2相互独立的结果*

**朴素贝叶斯，之所以朴素，就在于假定了特征与特征相互独立**

### 贝叶斯公式
#### 公式
$$
P(C \mid W) = \frac{P(W \mid C) P(C)}{P(W)}
$$


#### 贝叶斯估计
处理估计条件概率P(X∣Y)时出现概率为0的情况
##### 公式
$$
P(F_1 \mid C) = \frac{N_i + \alpha}{N + \alpha m}
$$
-  α为指定的系数，一般为1，拉普拉斯平滑系数，为1时称为拉普拉斯平滑；为0时，是普通的极大似然估计
-  m为训练文档中统计出的特征词个数
##### 目的
防止计算出的分类概率为0
#### API
```
sklearn.naive_bayes.MultinomialNB(alpha = 1.0)
```
- 朴素贝叶斯分类
- alpha
拉普拉斯平滑系数


### 朴素贝叶斯算法的原理
- 朴素贝叶斯法是基于贝叶斯定理与特征条件独立假设的分类方法
- 对于给定的待分类项x，通过学习到的模型计算后验概率分布， 即：在此项出现的条件下各个目标类别出现的概率，将后验概率最大的类作为x所属的类别


### 朴素贝叶斯朴素在哪里
在计算条件概率分布P(X=x∣Y=c_k)时，NB引入了一个很强的条件独立假设，即，当Y确定时，X的各个特征分量取值之间相互独立

### 为什么引入条件独立性假设
为了避免贝叶斯定理求解时面临的组合爆炸、样本稀疏问题

#### 假设条件概率分布为：
$$
P(X = x \mid Y = c_k) = P(X_1 = x_1, \dots, X_n = x_n \mid Y = c_k)
$$

- 其中xj可能的取值有 $S_j$ 个，$j=1,2,...n$，Y可能取值有K个，那么参数个数为 $K \prod_{j=1}^{n} S_j$ 个，这导致条件概率分布的参数数量为指数级别

### 朴素贝叶斯算法优缺点
#### 优点
- 朴素贝叶斯模型发源于古典数学理论，有稳定的分类效率
- 对缺失数据不太敏感，算法也比较简单，常用于文本分类
- 分类准确度高，速度快
#### 缺点
- 由于使用了样本属性独立性的假设，所以如果特征属性有关联时其效果不好
- 需要计算先验概率，而先验概率很多时候取决于假设，假设的模型可以有很多种，因此在某些时候会由于假设的先验模型的原因导致预测效果不佳

为什么属性独立性假设在实际情况中很难成立，但朴素贝叶斯仍能取得较好的效果?
- 在使用分类器之前，首先做的第一步（也是最重要的一步）往往是特征选择，这个过程的目的就是为了排除特征之间的共线性、选择相对较为独立的特征
- 对于分类任务来说，只要各类别的条件概率排序正确，无需精准概率值就可以得出正确分类； 
- 如果属性间依赖对所有类别影响相同，或依赖关系的影响能相互抵消，则属性条件独立性假设在降低计算复杂度的同时不会对性能产生负面影响

### 朴素贝叶斯与逻辑回归的区别
#### 基本区别
##### 区别1
-  朴素贝叶斯是生成模型，根据已有样本进行贝叶斯估计学习出先验概率P(Y)和条件概率P(X|Y)，进而求出联合分布概率P(XY)，最后利用贝叶斯定理求解P(Y|X)
-  逻辑回归是判别模型，根据极大化对数似然函数直接求出条件概率P(Y|X)

从概率框架的角度来理解机器学习，主要有两种策略： 
- 第一种：给定 x， 可通过直接建模 P(c|x) 来预测 c，这样得到的是"判别式模型" (discriminative models) 
- 第二种：也可先对联合概率分布 P(x,c) 建模，然后再由此获得 $P(c|x)$，这样得到的是"生成式模型" (generative models)
- 逻辑回归、决策树、多层感知器神经网络、支持向量机等都可归入判别式模型的范畴 
- 对生成式模型来说，必然需要考虑 $P(c|x)=P(x,c)/P(x)$

##### 区别2
-  朴素贝叶斯是基于很强的条件独立假设（在已知分类Y的条件下，各个特征变量取值是相互独立的）
-  逻辑回归则对此没有要求

##### 区别3
-  朴素贝叶斯适用于数据集少的情景
-  逻辑回归适用于大规模数据集

#### 根本区别
朴素贝叶斯是生成式模型，逻辑回归是判别式模型，二者的区别就是生成式模型与判别式模型的区别 
##### 朴素贝叶斯
Navie Bayes通过已知样本求得先验概率P(Y), 及条件概率 $P(X|Y)$, 对于给定的实例，计算联合概率，进而求出后验概率。也就是说，它尝试去找到底这个数据是怎么生成的（产生的），然后再进行分类。哪个类别最有可能产生这个信号，就属于那个类别

-  优点
样本容量增加时，收敛更快；隐变量存在时也可适用 
-  缺点
时间长；需要样本多；浪费计算资源 

##### 逻辑回归
Logistic回归不关心样本中类别的比例及类别下出现特征的概率，它直接给出预测模型的式子。设每个特征都有一个权重，训练样本数据更新权重w，得出最终表达式 
- 优点
  - 直接预测往往准确率更高 
  - 简化问题 
  - 可以反应数据的分布情况，类别的差异特征 
  - 适用于较多类别的识别
- 缺点 
  - 收敛慢 
  - 不适用于有隐变量的情况



## 决策树
### 概念
#### 决策树来源
决策树思想的来源非常朴素，程序设计中的条件分支结构就是if-then结构，最早的决策树就是利用这类结构分割数据的一种分类学习方法
#### 决策树定义
- 是一种树形结构，本质是一颗由多个判断节点组成的树
- 其中每个内部节点表示一个属性上的判断
- 每个分支代表一个判断结果的输出
- 最后每个叶节点代表一种分类结果

### 决策树分类原理
#### 熵
##### 熵定义
-  物理学上，熵 Entropy 是“混乱”程度的量度
-  系统越有序，熵值越低；系统越混乱或者分散，熵值越高

#### 信息熵
1948年香农提出了信息熵（Entropy）的概念
#### 信息理论
##### 从信息的完整性上进行的描述
当系统的有序状态⼀致时，数据越集中的地方熵值越小，数据越分散的地方熵值越大
##### 从信息的有序性上进⾏的描述
当数据量⼀致时，系统越有序，熵值越低；系统越混乱或者分散，熵值越高

"信息熵" (information entropy)是度量样本集合纯度最常用的⼀种指标

假定当前样本集合 D 中第 k 类样本所占的比例为 $p_k(k = 1, 2,. . . , |y|)，
p_k =C_k/D$ , $D$ 为样本的所有数量，$C_k$ 为第k类样本的数量。则D的信息熵定义为(（log是以2为底，lg是以10为底）:

$$
\text{Ent}(D) = -\sum \frac{C_k}{D} \log \frac{C_k}{D} = -\sum p_k \log p_k
$$

其中：$\text{Ent}(D)$ 的值越小，则 $D$ 的纯度越高


信息和消除不确定性是相联系的
- 当我们得到的额外信息越多的话，那么我们猜测的代价越小（猜测的不确定性减小）


### 决策树的划分依据一------信息增益
#### 定义
以某特征划分数据集前后的熵的差值 。熵可以表示样本集合的不确定性，熵越大，样本的不确定性就越大。因此可以使用划分前后集合熵的差值来衡量使用当前特征对于样本集合D划分效果的好坏

信息增益 = entroy(前) - entroy(后)

#### 公式
特征A对训练数据集D的信息增益g(D,A),定义为集合D的信息熵H(D)与特征A给定条件下D的信息条件熵H(D|A)之差

假定离散属性a有 V 个可能的取值

若使用a来对样本集 D 进行划分 ，则会产生 V 个分支结点，其中第v个分支结点包含了 D 中所有在属性a上取值为av的样本，记为Dv， 我们可根据信息熵公式计算出Dv的信息熵，再考虑到不同的分支结点所包含的样本数不同，即样本数越多的分支结点的影响越大，于是可计算出用属性a对样本集 D 进行划分所获得的"信息增益" (information gain)

其中：
特征a对训练数据集D的信息增益Gain(D,a),定义为集合D的信息熵Ent(D)与给定特征a条件下D的信息条件熵Ent(D|a)之 差， 即公式为：

$$
\text{Gain}(D, a) = \text{Ent}(D) - \text{Ent}(D \mid a) = \text{Ent}(D) - \sum \frac{D_v}{D} \text{Ent}(D_v)
$$

##### 公式的详细解释
-  信息熵的计算
$$
\text{Ent}(D) = -\sum \frac{C_k}{D} \log \frac{C_k}{D}
$$


-  条件熵的计算
$$
\text{Ent}(D, a) = \sum \frac{D_v}{D} \log D_v = -\sum \frac{D_v}{D} \sum \frac{C_{kv}}{D_v} \log \frac{C_{kv}}{D_v}
$$


##### 其中
-  $D_v$
表示a属性中第v个分支节点包含的样本数
-  $C_{kv}$
表示a属性中第v个分支节点包含的样本数中 ，第k个类别下包含的样本数

一般而言，信息增益越大，则意味着使用属性 a 来进行划分所获得的"纯度提升"越大。因此 ，我们可用信息增益来进行决策树的划分属性选择，著名的 ID3 决策树学习算法 [Quinlan， 1986] 就是以信息增益为准则来选择划分属性(其中，ID3 名字中的 ID 是 Iterative Dichotomiser (迭代二分器)的简称)

### 决策树的划分依据二----信息增益率(C4.5)
#### 定义
在上面的介绍中 ，我们有意忽略了"编号"这一列.若把"编号"也作为一个候选划分属性 ，则根据信息增益公式可计算出它 的信息增益为 0.9182 ，远大于其他候选划分属性

计算每个属性的信息熵过程中,我们发现,该属性的值为0, 也就是其信息增益为0.9182. 但是很明显这么分类,最后 出现的结果不具有泛化效果.无法对新样本进行有效预测

实际上，信息增益准则对可取值数目较多的属性有所偏好，为减少这种偏好可能带来的不利影响，著名的 C4.5 决策树 算法 [Quinlan ，1993J 不直接使用信息增益，而是使用 "增益率" (gain ratio) 来选择最优划分属性
##### 增益率
增益率是用前面的信息增益Gain(D, a)和属性a对应的"固有值"(intrinsic value) [Quinlan , 1993J的比值来共同定义的

#### 公式
$$
\text{Gain\_ratio}(D, a) = \frac{\text{Gain}(D, a)}{\text{IV}(a)}
$$
其中
$$
\text{IV}(a) = -\sum \frac{D_v}{D} \log \frac{D_v}{D}
$$

属性 a 的可能取值数目越多(即 V 越大) ，则 IV(a) 的值通常会越大

#### 为什么使用C4.5要好
##### 用信息增益率来选择属性
克服了用信息增益来选择属性时偏向选择值多的属性的不足
##### 采用了一种后剪枝方法
避免树的高度无节制的增长 ，避免过度拟合数据
##### 对于缺失值的处理
-  在某些情况下 ，可供使用的数据可能缺少某些属性的值 。假如 $〈x ，c(x)〉$ 是样本集S中的一个训练实例 ，但是其属性A 的值A(x)未知
-  处理缺少属性值的一种策略是赋给它结点n所对应的训练实例中该属性的最常见值
-  另外一种更复杂的策略是为A的每个可能值赋予一个概率
-  例如 ，给定一个布尔属性A ，如果结点n包含6个已知A=1和4个A=0的实例 ，那么A(x)=1的概率是0.6 ，而A(x)=0的概率 是0.4 。于是， 实例x的60%被分配到A=1的分支 ，40%被分配到另一个分支。C4.5就是使用这种方法处理缺少的属性值

### 决策树的划分依据三----基尼值和基尼指数(CART)
#### 概念
CART 决策树 [Breiman et al., 1984] 使用 "基尼指数" (Gini index)来选择划分属性

CART 是Classification and Regression Tree的简称 ，这是一种著名的决策树学习算法,分类和回归任务都可用

基尼值Gini（D） ：从数据集D中随机抽取两个样本 ，其类别标记不一致的概率。故，Gini（D）值越小 ，数据集D的纯度越高

数据集 D 的纯度可用基尼值来度量
$$
\text{Gini}(D) = \sum \sum p_k p_k = 1 - \sum p_k^2
$$

-  $P_k=C_k/D$ ,$D$ 为样本的所有数量 ，$C_k$ 为第k类样本的数量

基尼指数Gini_index（D）：一般选择使划分后基尼系数最小的属性作为最优化分属性

$$
\text{Gini\_index}(D, a) = \sum \left(\frac{D_v}{D} \cdot \text{Gini}(D_v)\right)
$$

#### CART的算法流程
while(当前节点 "不纯 ")：
1. 遍历每个变量的每一种分割方式 ，找到最好的分割点
2. 分割成两个节点N1和N2

end while
每个节点足够“纯”为止

### 常见决策树的启发函数比较
#### ID3算法
-提出时间  1975
- 分支方式
信息增益
- 备注
ID3只能对离散属性的数据集构成决策树
- 存在的缺点
  - ID3算法在选择根节点和各内部节点中的分支属性时，采用信息增益作为评价标准。信息增益的缺点是倾向于选择取值较多的属性，在有些情况下这类属性可能不会提供太多有价值的信息
  - ID3算法只能对描述属性为离散型属性的数据集构造决策树

#### C4.5算法
- 提出时间
1993
- 分支方式
信息增益率
- 备注
优化后解决了ID3分支过程中总喜欢偏向选择值较多的属性
- 做出的改进(比C4.5好的地方)
  - 用信息增益率来选择属性
  - 可以处理连续数值型属性
  - 采用了一种后剪枝方法
  - 对于缺失值的处理
##### 优点
产生的分类规则易于理解，准确率较高
##### 存在的缺点
- 在构造树的过程中，需要对数据集进行多次的顺序扫描和排序，因而导致算法的低效。
- 此外，C4.5只适合于能够驻留于内存的数据集，当训练集大得无法在内存容纳时程序无法运行

#### CART算法
##### 提出时间
1984
##### 分支方式
Gini系数
##### 备注
可以进行分类和回归 ，可以处理离散属性 ，也可以处理连续属性
##### 优点
CART算法相比C4.5算法的分类方法，采用了简化的二叉树模型，同时特征选择采用了近似的基尼系数来简化计算

*C4.5不一定是二叉树，但CART一定是二叉树*

### 多变量决策树(multi-variate decision tree)
无论是ID3, C4.5还是CART,在做特征选择的时候都是选择最优的一个特征来做分类决策 ，但是大多数 分类决策 不应该是由某一个特征决定的，而是应该由一组特征决定的。这样决策得到的决策树更加准确。这个决策树叫做多变量决策树(multi-variate decision tree)

在选择最优特征的时候，多变量决策树不是选择某一个最优特征，而是选择最优的一个特征线性组合来做决策。这个算法的代表是OC1

如果样本发生一点点的改动，就会导致树结构的剧烈改变。这个可以通过集成学习里面的随机森林之类的方法解决


### 决策树变量的两种类型
#### 数字型（Numeric）
变量类型是整数或浮点数，如前面例子中的“年收入 ”。用 “>=” ，“>” ,“<”或“<=”作为分割条件（排序后，利用已有的分割情况，可以优化分割算法的时间复杂度） 。
#### 名称型（Nominal）
类似编程语言中的枚举类型，变量只能从有限的选项中选取，比如前面例子中的“婚姻情况”，只能是“单身”，“已婚”或“离婚”，使用“=”来分割

### 如何评估分割点的好坏？
- 如果一个分割点可以将当前的所有节点分为两类 ，使得每一类都很“纯” ，也就是同一类的记录较多，那么就是一个好分割点
- 比如上面的例子 ，“拥有房产” ，可以将记录分成了两类 ，“是” 的节点全部都可以偿还债务 ，非常“纯” ；“否” 的节点 ，可以 偿还贷款和无法偿还贷款的人都有 ，不是很“纯” ，但是两个节点加起来的纯度之和与原始节点的纯度之差最大 ，所以按 照这种方法分割
- 构建决策树采用贪心算法， 只考虑当前纯度差最大的情况作为分割点


### CART剪枝
#### 需要剪枝的情况
随着树的增长，在训练样集上的精度是单调上升的，然而在独立的测试样例上测出的精度先上升后下降
#### 出现这种情况的原因
- 原因1：噪声、样本冲突，即错误的样本数据
- 原因2：特征即属性不能完全作为分类标准
- 原因3：巧合的规律性，数据量不够大

**剪枝 (pruning)是决策树学习算法对付"过拟合"的主要手段**

在决策树学习中 ，为了尽可能正确分类训练样本 ，结点划分过程将不断重复，有时会造成决策树分支过多，这时就可能 因训练样本学得"太好" 了，以致于把训练集自身的一些特点当作所有数据都具有的一般性质而导致过拟合。因此，可通过主动去掉一些分支来降低过拟合的风险

#### 常用的减枝方法
决策树剪枝的基本策略有"预剪枝" (pre-pruning)和"后剪枝"(post- pruning) 
##### 预剪枝
预剪枝是指在决策树生成过程中，对每个结点在划分前先进行估计，若当前结点的划分不能带来决策树泛化性能提升，则停止划分并将当前结点标记为叶结点
##### 后剪枝
后剪枝则是先从训练集生成一棵完整的决策树，然后自底向上地对非叶结点进行考察，若将该结点对应的子树替换为叶结点能带来决策树泛化性能提升，则将该子树替换为叶结点

#### 两种剪枝方法对比
- 后剪枝决策树通常比预剪枝决策树保留了更多的分支
- 一般情形下，后剪枝决策树的欠拟合风险很小，泛化性能往往优于预剪枝决策树
- 但后剪枝过程是在生成完全决策树之后进行的。并且要自底向上地对树中的所有非叶结点进行逐一考察，因此其训练时间开销比未剪枝决策树和预剪枝决策树都要大得多


### 决策树算法API
```
class sklearn.tree.DecisionTreeClassifier(criterion='gini', max_depth=None, random_state=None)
```
#### criterion
- 特征选择标准
"gini"或者"entropy" ，前者代表基尼系数 ，后者代表信息增益 。一默认"gini"， 即CART算法

#### ```min_samples_split```
- 内部节点再划分所需最小样本数
- 这个值限制了子树继续划分的条件，如果某节点的样本数少于 ```min_samples_split``` ，则不会继续再尝试选择最优特征来进行划分。默认是2.如果样本量不大，不需要管这个值 。如果样本量数量级非常大，则推荐增大这个值。大概10万样本 ，建立决策树时，选择了 ```min_samples_split=10```。可以作为参考
#### ```min_samples_leaf```
叶子节点最少样本数
- 这个值限制了叶子节点最少的样本数 ，如果某叶子节点数目小于样本数 ，则会和兄弟节点一起被剪枝
- 默认是1,可以输入最少的样本数的整数 ，或者最少样本数占样本总数的百分比。如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。之前的10万样本项目使用 ```min_samples_leaf``` 的值为5，仅供参考
#### max_depth
决策树最大深度
- 默认可以不输入，如果不输入的话，决策树在建立子树的时候不会限制子树的深度。一般来说，数据少或者特征少的时候可以不管这个值。如果模型样本量多，特征也多的情况下，推荐限制这个最大深度，具体的取值取决于数据的分布。常用的可以取值10-100之间
#### random_state
随机数种子

### 决策树可视化
#### 保存树的结构到dot文件
```sklearn.tree.export_graphviz()``` 该函数能够导出DOT格式
```
tree.export_graphviz(estimator,out_file='tree.dot’ ,feature_names=[‘’ ,’’])
```
#### 网站显示结构
http://webgraphviz.com/


### 决策树优缺点
#### 优点
简单的理解和解释 ，树木可视化
#### 缺点
决策树学习者可以创建不能很好地推广数据的过于复杂的树,容易发生过拟合
#### 改进
- 减枝cart算法
- 随机森林（集成学习的一种）

*企业重要决策，由于决策树很好的分析能力 ，在决策过程应用较多，可以选择特征*


## 集成学习方法之随机森林
### 集成学习方法
集成学习通过建立几个模型组合的来解决单一预测问题。它的工作原理是生成多个分类器/模型，各自独立地学习和作出预测。这些预测最后结合成组合预测，因此优于任何一个单分类的做出预测
#### Bagging集成原理
- 第1步，采样不同数据集
- 第2步，训练分类器
- 第3步，平权投票 ，获取最终结果

### 随机森林定义
在机器学习中，随机森林是一个包含多个决策树的分类器，并且其输出的类别是由个别树输出的类别的众数而定

**随机森林 = Bagging + 决策树**


### 随机森林原理
学习算法根据下列算法而建造每棵树
- 用N来表示训练用例（样本）的个数，M表示特征数目
  - 一次随机选出一个样本，重复N次， （有可能出现重复的样本）
  - 随机去选出m个特征, m <<M，建立决策树
- 采取bootstrap抽样

### 为什么采用BootStrap抽样
#### 为什么要随机抽样训练集？　　
如果不进行随机抽样，每棵树的训练集都一样，那么最终训练出的树分类结果也是完全一样的
#### 为什么要有放回地抽样？
如果不是有放回的抽样，那么每棵树的训练样本都是不同的，都是没有交集的，这样每棵树都是“有偏的”，都是绝对“片面的”（当然这样说可能不对），也就是说每棵树训练出来都是有很大的差异的；而随机森林最后分类取决于多棵树（弱分类器）的投票表决

### API
```
class sklearn.ensemble.RandomForestClassifier(n_estimators=10, criterion=’gini’, max_depth=None, bootstrap=True, random_state=None, min_samples_split=2)
```
#### 随机森林分类器

#### ```n_estimators```
```integer，optional（default = 10）```森林里的树木数量120,200,300,500,800,1200
#### criteria
string，可选（default =“gini”）分割特征的测量方法
#### max_depth
integer或None，可选（默认=无）树的最大深度 5,8,15,25,30
#### ```max_features="auto”```
每个决策树的最大特征数量
##### ```If "auto", then max_features=sqrt(n_features)```
##### ```If "sqrt", then max_features=sqrt(n_features) (same as "auto")```
##### ```If "log2", then max_features=log2(n_features)```
##### ```If None, then max_features=n_features```
#### bootstrap
```boolean，optional（default = True）``` 是否在构建树时使用放回抽样
#### ```min_samples_split```
节点划分最少样本数
#### ```min_samples_leaf```
叶子节点的最小样本数
#### 超参数
```n_estimator, max_depth, min_samples_split,min_samples_leaf```



### Bagging集成优点
- Bagging + 决策树/线性回归/逻辑回归/深度学习 … = bagging集成学习方法
- 经过上面方式组成的集成学习方法
  - 均可在原有算法上提高约2%左右的泛化正确率
  - 简单, 方便, 通用

### 随机森林优点
- 具有极好的准确率
- 能够有效地运行在大数据集上，处理具有高维特征的输入样本，而且不需要降维
- 能够评估各个特征在分类问题上的重要性


# 回归算法
## 线性回归
### 线性回归应用场景
- 房价预测
- 销售额度预测
- 金融：贷款额度预测、利用线性回归以及系数分析因子
### 定义和公式
#### 定义
线性回归(Linear regression)是利用回归方程(函数)对一个或多个自变量(特征值)和因变量(目标值)之间关系进行建模的一种分析方式
#### 特点
只有一个自变量的情况称为单变量回归，大于一个自变量情况的叫做多元回归
#### 通用公式
$$
h(w) = w_1x_1 + w_2x_2 + w_3x_3 + \dots + b = w^T x + b
$$

- 其中，w，x可以理解为矩阵：$W=(b,w_1,w_2), X=(1,x_1,x_2)$

### 线性回归的特征与目标的关系分析
线性回归当中的关系有两种
- 线性关系
如果在单特征与目标值的关系呈直线关系，或者两个特征与目标值呈现平面的关系，即为线性关系
- 非线性关系

### 线性回归的损失函数
#### 损失函数公式
总损失定义为
$$
J(\theta) = \sum (h_w(x_i) - y_i)^2
$$

- y_i为第i个训练样本的真实值
- h(x_i)为第i个训练样本特征值组合预测函数
- 又称最小二乘法
### 优化算法
目的是找到最小损失对应的W值  
线性回归经常使用的两种优化算法
#### 正规方程
$$
W = (X^T X)^{-1} X^T y
$$

- X为特征值矩阵，y为目标值矩阵。直接求到最好的结果
- 缺点：当特征过多过复杂时，求解速度太慢并且得不到结果
#### 梯度下降(Gradient Descent)
$$
w_i = w_i - \alpha \frac{\partial \text{cost}}{\partial w_i}
$$
- α为学习速率，需要手动指定（超参数），α旁边的整体表示方向
- 沿着这个函数下降的方向找，最后就能找到山谷的最低点，然后更新W值
- 在面对训练数据规模十分庞大的任务 ，能够找到较好的结果

### 线性回归API
sklearn提供=两种实现的API， 可以根据选择使用
```
sklearn.linear_model.LinearRegression(fit_intercept=True)
```
- 通过正规方程优化
- fit_intercept：是否计算偏置
- LinearRegression.coef_：回归系数
- LinearRegression.intercept_：偏置
```
sklearn.linear_model.SGDRegressor(loss="squared_loss", fit_intercept=True, learning_rate ='invscaling', eta0=0.01)
```
- SGDRegressor类实现了随机梯度下降学习，它支持不同的loss函数和正则化惩罚项来拟合线性回归模型
- loss:损失类型
- loss=”squared_loss”: 普通最小二乘法
- fit_intercept：是否计算偏置
- learning_rate : string, optional
  - 学习率填充
  - 'constant': eta = eta0
  - 'optimal': eta = 1.0 / (alpha * (t + t0)) [default]
  - 'invscaling': eta = eta0 / pow(t, power_t)
    - power_t=0.25:存在父类当中
  - 对于一个常数值的学习率来说，可以使用learning_rate=’constant’ ，并使用eta0来指定学习率
- SGDRegressor.coef_：回归系数
- SGDRegressor.intercept_：偏置

### 回归性能评估
均方误差(Mean Squared Error,MSE)评价机制
#### 公式
$$
\text{MSE} = \frac{1}{m} \sum_{i=1}^{m} (y_i - \hat{y}_i)^2
$$

#### API
```
sklearn.metrics.mean_squared_error(y_true, y_pred)
```
-  均方误差回归损失
-  y_true:真实值
-  y_pred:预测值
-  return:浮点数结果

### 正规方程和梯度下降对比
#### 梯度下降
- 需要选择学习率
- 需要迭代求解
- 特征数量较大可以使用
#### 正规方程
- 不需要选择学习率
- 一次运算得出
- 需要计算方程，时间复杂度高O(n3)

### 方法选择
#### 小规模数据
- LinearRegression(不能解决拟合问题)
- 岭回归
#### 大规模数据
- SGDRegressor

### 优化方法GD、SGD、SAG
#### GD--梯度下降(Gradient Descent)
原始的梯度下降法需要计算所有样本的值才能够得出梯度，计算量大，所以后面才有会一系列的改进
#### SGD--随机梯度下降(Stochastic gradient descent)
随机梯度下降(Stochastic gradient descent)是一个优化方法。它在一次迭代时只考虑一个训练样本
##### SGD的优点是
-  高效
-  容易实现
##### SGD的缺点是
-  SGD需要许多超参数：比如正则项参数、迭代数。
-  SGD对于特征标准化是敏感的
#### SAG--随机平均梯度法(Stochasitc Average Gradient)
- 由于随机平均梯度法(Stochasitc Average Gradient)收敛的速度太慢，有人提出SAG等基于梯度下降的算法
- Scikit-learn：SGDRegressor、岭回归、逻辑回归等当中都会有SAG优化

## 欠拟合与过拟合
### 定义
#### 过拟合
一个假设在训练数据上能够获得比其他假设更好的拟合， 但是在测试数据集上却不能很好地拟合数据，此时认为这个假设出现了过拟合的现象

**即模型过于复杂**

#### 欠拟合
一个假设在训练数据上不能获得更好的拟合，并且在测试数据集上也不能很好地拟合数据，此时认为这个假设出现了欠拟合的现象

**即模型过于简单**

### 原因以及解决办法
#### 欠拟合原因以及解决办法
##### 原因
学习到数据的特征过少
##### 解决办法
增加数据的特征数量
#### 过拟合原因以及解决办法
##### 原因
原始特征过多，存在一些嘈杂特征， 模型过于复杂是因为模型尝试去兼顾各个测试数据点
##### 解决办法
正则化

*针对回归，选择正则化。但是对于其他机器学习算法如分类算法来说也会出现这样的问题，除了一些算法本身作用之外（决策树、神经网络），我们更多的也是去自己做特征选择，包括删除、合并一些特征*

### 正则化
#### 定义
在学习的时候，数据提供的特征有些影响模型复杂度或者这个特征的数据点异常较多，所以算法在学习的时候尽量减少这个特征的影响（甚至删除某个特征的影响），这就是正则化

调整时候，算法并不知道某个特征影响，而是去调整参数得出优化的结果

#### 正则化类别
##### L2正则化
-  作用：可以使得其中一些W的都很小，都接近于0，削弱某个特征的影响
-  优点：越小的参数说明模型越简单，越简单的模型则越不容易产生过拟合现象
-  即Ridge回归（岭回归）
##### L1正则化
-  作用：可以使得其中一些W的值直接为0，删除这个特征的影响
-  LASSO回归

#### 正则化原理
- 线性回归的损失函数用最小二乘法，等价于当预测值与真实值的误差满足正态分布时的极大似然估计
- 岭回归的损失函数，是最小二乘法+L2范数，等价于当预测值与真实值的误差满足正态分布，且权重值也满足正态分布（先验分布）时的最大后验估计
- LASSO的损失函数，是最小二乘法+L1范数，等价于等价于当预测值与真实值的误差满足正态分布，且且权重值满足拉普拉斯分布（先验分布）时的最大后验估计


## 岭回归
### 定义
岭回归，其实也是一种线性回归。只不过在算法建立回归方程时候，加上正则化的限制，从而达到解决过拟合的效果

岭回归即带有L2正则化的线性回归

### API
```
sklearn.linear_model.Ridge(alpha=1.0, fit_intercept=True,solver="auto", normalize=False)
```
##### 具有L2正则化的线性回归
##### alpha
正则化力度，也叫 λ
- λ取值：0~1 1~10
##### solver
会根据数据自动选择优化方法
- sag
如果数据集、特征都比较大，选择该随机梯度下降优化
##### normalize
数据是否进行标准化
- normalize=False
可以在fit之前调用 ```preprocessing.StandardScaler``` 标准化数据
##### Ridge.coef_
回归权重
##### Ridge.intercept_
回归偏置

Ridge方法相当于 ```SGDRegressor(penalty='l2', loss="squared_loss")``` ,只不过SGDRegressor实现了一个普通的随机梯度下降学习，推荐使用Ridge(实现了SAG)

```
sklearn.linear_model.RidgeCV(_BaseRidgeCV, RegressorMixin)
```
- 具有l2正则化的线性回归，可以进行交叉验证
- coef_:回归系数

### 正则化程度的变化，对结果的影响
- 正则化力度越大，权重系数会越小
- 正则化力度越小，权重系数会越大



## 逻辑回归
逻辑回归是解决二分类问题的利器
### 逻辑回归的原理
#### 输入
$$
h(w) = w_1 x_1 + w_2 x_2 + w_3 x_3 + \dots + b
$$

逻辑回归的输入就是一个线性回归的结果
#### 激活函数
##### sigmoid函数
$$
g(\theta^T x) = \frac{1}{1 + e^{-\theta^T x}}
$$

回归的结果输入到sigmoid函数当中

输出结果：[0, 1]区间中的一个概率值，默认为0.5为阈值

逻辑回归最终的分类是通过属于某个类别的概率值来判断是否属于某个类别，并且这个类别默认标记为1(正例),另外的一个类别会标记为0(反例)。（方便损失计算）

#### 输出结果解释
假设有两个类别A，B，并且假设我们的概率值为属于A(1)这个类别的概率值。现在有一个样本的输入到逻辑回归输出结果0.6，那么这个概率值超过0.5，意味着我们训练或者预测的结果就是A(1)类别。那么反之，如果得出结果为0.3那么，训练或者预测结果就为B(0)类别

### 损失以及优化
#### 损失
逻辑回归的损失，称之为对数似然损失，公式如下
##### 分开类别
$$
\text{cost}(h_\theta(x), y) =
\begin{cases} 
-\log(h_\theta(x)), & \text{if } y = 1 \\
-\log(1 - h_\theta(x)), & \text{if } y = 0
\end{cases}
$$


##### 综合完整损失函数
$$
J(\theta) = \sum_{i=1}^{m} \left[ -y_i \log h_\theta(x_i) - (1 - y_i) \log (1 - h_\theta(x_i)) \right]
$$

这个式子，与信息熵类似

#### 优化
使用梯度下降优化算法，去减少损失函数的值。这样去更新逻辑回归前面对应算法的权重参数，提升原本属于1类别的概率，降低原本是0类别的概率

### 逻辑回归API
```
sklearn.linear_model.LogisticRegression(solver='liblinear', penalty=‘l2’, C = 1.0)
```
- solver
优化求解方式（默认开源的liblinear库实现，内部使用了坐标轴下降法来迭代优化损失函数）
  - sag
根据数据集自动选择，随机平均梯度下降
- penalty
正则化的种类
- C
正则化力度

*默认将类别数量少的当做正例*

LogisticRegression方法相当于 ```SGDClassifier(loss="log", penalty=" ")```,SGDClassifier实现了一个普通的随机梯度下降学习，也支持平均随机梯度下降法（ASGD），可以通过设置average=True。而使用LogisticRegression(实现了SAG)


### 分类的评估方法
#### 精确率与召回率
##### 混淆矩阵
在分类任务下，预测结果(Predicted Condition)与正确标记(True Condition)之间存在四种不同的组合，构成混淆矩阵(适用于多分类)
-  预测结果--正例  真实结果--正例  ----> 真正例TP(True Positive)
-  预测结果--正例  真实结果--假例  ----> 伪正例FP(False Positive)
-  预测结果--假例  真实结果--正例  ----> 伪反例FN(False Negative)
-  预测结果--假例  真实结果--假例  ----> 真反例TN(True Negative)

##### 精确率(Precision)与召回率(Recall)
-  精确率
预测结果为正例样本中真实为正例的比例，即TP/(TP+FP)
-  召回率
真实为正例的样本中预测结果为正例的比例（查的全，对正样本的区分能力），即TP/(TP+FN)

##### F1-score
F1-score可以反映模型的稳健性
###### 公式
$$
F_1 = \frac{2TP}{2TP + FN + FP} = \frac{2 \times \text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
$$

##### 分类评估报告API
```
sklearn.metrics.classification_report(y_true, y_pred, labels=[], target_names=None )
```
-  y_true：真实目标值
-  y_pred：估计器预测目标值
-  labels:指定类别对应的数字
-  target_names：目标类别名称
-  return：每个类别精确率与召回率

#### ROC曲线与AUC指标
样本不均衡下的评估问题
##### TPR与FPR
$$
TPR = \frac{TP}{TP + FN}
$$
所有真实类别为1的样本中，预测类别为1的比例

$$
FPR = \frac{FP}{FP + TN}
$$
所有真实类别为0的样本中，预测类别为1的比例

##### ROC曲线
ROC曲线的横轴就是FPRate，纵轴就是TPRate，当二者相等时，表示的意义则是：对于不论真实类别是1还是0的样本，分类器预测为1的概率是相等的，此时AUC为0.5

##### AUC指标
-  AUC的概率意义是随机取一对正负样本，正样本得分大于负样本的概率
-  AUC的最小值为0.5，最大值为1，取值越高越好
-  AUC=1，完美分类器，采用这个预测模型时，不管设定什么阈值都能得出完美预测。绝大多数预测的场合，不存在完美分类器
-  0.5<AUC<1，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值

*最终AUC的范围在[0.5, 1]之间，并且越接近1越好*

##### AUC计算API
```
from sklearn.metrics import roc_auc_score

sklearn.metrics.roc_auc_score(y_true, y_score)
```
- 计算ROC曲线面积，即AUC值
- y_true:每个样本的真实类别，必须为0(反例),1(正例)标记
- y_score:每个样本预测的概率值

##### AUC特点
-  AUC只能用来评价二分类
-  AUC非常适合评价样本不平衡中的分类器性能



## 模型保存和加载
### 保存和加载API
#### 导入joblib包
```
import joblib
```
#### 保存模型
```
joblib.dump(rf, 'test.pkl')
```
#### 加载模型
```
estimator = joblib.load('test.pkl')
```

## 无监督学习 K-means算法
### 无监督学习
无监督学习是从无标签的数据开始学习的
#### 无监督学习包含算法
##### 聚类
-  K-means(K均值聚类)
##### 降维
-  PCA

### K-means算法
#### K-means聚类步骤
- 随机设置K个特征空间内的点作为初始的聚类中心
- 对于其他每个点计算到K个中心的距离，未知的点选择最近的一个聚类中心点作为标记类别
- 接着对着标记的聚类中心之后，重新计算出每个聚类的新中心点（平均值）
- 如果计算得出的新中心点与原中心点一样，那么结束，否则重新进行第二步过程

### K-means API
```
sklearn.cluster.KMeans(n_clusters=8,init=‘k-means++’)
```
- k-means聚类
- n_clusters
开始的聚类中心数量
- init
初始化方法，默认为'k-means ++’
- labels_
默认标记的类型，可以和真实值比较（不是值比较）

### Kmeans性能评估指标
#### 轮廓系数
$$
SC_i = \frac{b_i - a_i}{\max(b_i, a_i)}
$$

对于每个点i为已聚类数据中的样本 ，b_i为i到其它族群的所有样本的距离最小值，a_i为i到本身簇的距离平均值。最终计算出所有的样本点的轮廓系数平均值

#### 轮廓系数值分析
分析过程（以一个蓝1点为例）
- 计算出蓝1离本身族群所有点的距离的平均值a_i
- 蓝1到其它两个族群的距离计算出平均值红平均，绿平均，取最小的那个距离作为b_i
- 根据公式：极端值考虑：如果b_i >>a_i: 那么公式结果趋近于1；如果a_i>>>b_i: 那么公式结果趋近于-1

如果b_i>>a_i:趋近于1效果越好， b_i<<a_i:趋近于-1，效果不好。轮廓系数的值是介于 [-1,1] ，越趋近于1代表内聚度和分离度都相对较优

#### 轮廓系数API
```
sklearn.metrics.silhouette_score(X, labels)
```
- 计算所有样本的平均轮廓系数
- X
特征值
- labels
被聚类标记的目标值

### K-means总结
#### 特点分析
采用迭代式算法，直观易懂并且非常实用
#### 缺点
容易收敛到局部最优解(多次聚类)
#### 聚类一般做在分类之前

























































































