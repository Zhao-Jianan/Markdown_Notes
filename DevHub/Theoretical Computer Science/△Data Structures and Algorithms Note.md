# 数据结构
数据结构是一门研究非数值计算的程序设计问题中的操作对象，以及它们之间的关系和操作等相关问题的学科

数据结构研究数据元素之间的关系，包括数组、链表、树、图
## 基本概念
### 数据
数据是描述客观事物的符号，是计算机中可以操作的对象，是能被计算机识别，并输入给计算机处理的符号集合。数据不仅仅包括整型、实型等数值类型，还包括字符及声音、图像、视频等非数值类型
#### 数据的特点
- 可以输入到计算机
- 可以被计算机程序处理
#### 数据元素
组成数据的基本单位，例如数组中的一个元素
#### 数据项
一个数据元素由若干数据项组成。数据项是数据不可分割的最小单位
#### 数据对象
性质相同的数据元素的集合，是数据的子集。比如数组、链表。在不产生混淆的情况下，通常将数据对象简称为数据

由大到小，包含关系
数据-->数据对象-->数据元素-->数据项
#### 数据结构
是相互之间存在一种或多种特定关系的数据元素的集合


### 数据的逻辑关系
#### 数据的逻辑结构
##### 集合结构
数据元素间除了同属一个集合外，无其他关系
##### 线性结构
一个对一个，如线性表、栈、队列
##### 树形结构
一个对多个，如树
##### 图状结构
多个对多个，如图
#### 数据的物理结构
物理结构也叫存储结构，是数据结构在计算机存储器内的表示或映像，它依赖于计算机
存储结构可分为四大类
##### 顺序存储结构
把数据元素存放在地址连续的存储单元里，其数据间的逻辑关系和物理关系是一致的
借助元素在存储器中的相对位置来表示数据元素间的逻辑关系
##### 链式存储结构
把数据元素存放在任意的存储单元里，这组存储单元可以是连续的，也可以是不连续的
借助指示元素存储地址的指针表示数据元素间的逻辑关系
##### 索引存储结构
##### 散列存储结构

数据的逻辑结构与存储结构密切相关
算法设计-->逻辑结构
算法实现-->存储结构

### 抽象数据类型
#### 数据类型
是指一组性质相同的值的集合及定义在此集合上的一些操作的总称
#### 数据类型分类
##### 原子类型
是不可以再分解的基本类型，包括整型、实型、字符型等
##### 结构类型
由若干个类型组合而成，是可以再分解的。例如，整型数组是由若干整型数据组成的
#### 抽象数据类型（Abstract Data Type,ADT）
- 指一个数学模型及定义在该模型上的一组操作
- 抽象的意义在于数据类型的数学抽象特性
- 抽象数据类型体现了程序设计中问题分解、抽象和信息隐藏的特性

### 数据的运算
数据的运算，又叫数据的操作或数据的处理，是在数据的逻辑结构上定义的操作，它在数据的存储结构上实现
常用的数据运算有5种
- 插入
- 删除
- 修改
- 查找
- 排序


# 算法
## 算法概述
### 算法概念
算法是解决特定问题求解步骤的描述，在计算机中表现为指令的有限序列，并且每条指令表示一个或多个操作
算法是独立存在的一种解决问题的方法和思想
对于算法而言，语言并不重要，重要的是编程思想
### 算法与数据结构的区别
- 数据结构只是静态的描述了数据元素之间的关系
- 高效的程序需要在数据结构的基础上设计和选择算法
- 程序=数据结构+算法

总结：
- 算法是为了解决实际问题而设计的
- 数据结构是算法需要处理的问题载体
- 数据结构与算法相辅相成
### 算法特性
##### 输入
算法具有0个或多个输入
##### 输出
算法至少有1个或多个输出
##### 有穷性
算法在有限的步骤之后会自动结束而不会无限循环，且每一步在可接受的时间内完成
##### 确定性
算法的每一步骤都具有确定的含义，不会出现二义性
算法在一定条件下，只有一条执行路径，相同的输入只能有唯一的输出结果。算法的每个步骤被精确定义而无歧义
##### 可行性
算法的每一步都必须是可行的，也就是说，每一步都能通过执行有限次数完成

### 算法设计的要求
#### 正确性
算法的正确性是指算法至少应该具有输入、输出和加工处理无歧义性、能正确反映问题的需求、能够得到问题的正确答案
##### 正确性的四个层次
1. 算法程序没有语法错误
2. 算法程序对于合法的输入数据能够产生满足要求的输出结果
3. 算法程序对于非法的输入数据能够得出满足规格说明的结果
4. 算法程序对于精心选择的，甚至刁难的测试数据都有满足要求的输出结果

算法的正确性在大部分情况下都不能用程序来证明，而是用数学方法证明的
一般情况下，把层次3作为一个算法是否正确的标准
#### 可读性
算法设计的另一目的是为了便于阅读、理解和交流
#### 健壮性
当输入数据不合法时,算法也能做出相关处理,而不是产生异常或莫名其妙的结果
#### 时间效率高和存储量低
设计算法应尽量满足时间效率高和存储量低的需求

### 算法效率的度量
#### 事后统计方法
比较不同算法对同一组输入数据的运行处理时间
这种方法主要是通过设计好的测试程序和数据，利用计算机计时器对不同算法编制的程序的运行时间进行比较，从而确定算法效率的高低
一般不用事后统计方法
##### 缺陷
1. 为了获得不同算法的运行时间必须编写相应程序
2. 运行时间严重依赖硬件以及运行时的环境因素
3. 算法的测试数据的选取相当困难

事后统计法虽然直观，但是实施困难且缺陷多
#### 事前分析估算
依据统计的方法对算法效率进行估算

影响算法效率的主要因素
- 算法采用的策略和方法
- 问题的输入规模
- 编译器所产生的代码
- 计算机执行速度
抛开与计算机硬件、软件有关的因素，一个程序的运行时间，依赖于算法的好坏和问题的输入规模。所谓的问题输入规模是指输入量的多少


### 算法的时间复杂度
#### 定义
在进行算法分析时，语句总的执行次数T(n)是关于问题规模n的函数，进而分析T(n）随n的变化情况并确定T（n)的数量级

算法的时间复杂度，也就是算法的时间量度，记作:T(n)= O(f(n))。它表示随问题规模n的增大，算法执行时间的增长率和f(n)的增长率相同，称作算法的渐近时间复杂度，简称为时间复杂度。其中f(n）是问题规模n的某个函数

#### 大O表示法
算法效率严重依赖于操作（Operation）数量
在判断时，首先关注操作数量的最高次项
操作数量的估算可以作为时间复杂度的估算

$O(1) < O(\log n) < O(n) < O(n \log n) < O(n^2) < O(n^3) < O(2^n) < O(n!) < O(n^n)$


**注意：**
1. 判断一个算法的效率时，往往只需要关注操作数量的最高次项，其他次要项和常数项可以忽略
2. 在没有特殊说明时，我们所分析的算法的时间复杂度都是指最坏时间复杂度

**O(1)叫常数阶，O(n)叫线性阶，O(n^2)叫平方阶**

*推导大O阶方法*
- *第1步：用常数1取代运行时间中的所有加法常数*
- *第2步：在修改后的运行次数函数中，只保留最高阶项*
- *第3步：如果最高阶项存在且不是1，则去除与这个项相乘的常数。得到的结果就是大O阶*

#### 常数阶
顺序结构的时间复杂度是O(1)
单纯的分支结构（不包含在循环结构中），其时间复杂度是O(1)
#### 线性阶
分析算法的复杂度，关键就是分析循环结构的运行情况
#### 对数阶
例如：
```
int count=1;
while(count<n)
{
  count=count*2;
}
```
时间复杂度为O(logn)

**时间复杂度中的log都是以2为底**

#### 平方阶
循环嵌套的时间复杂度为O(n^2)

#### 最坏情况与平均情况
最坏情况运行时间是一种保证，那就是运行时间将不会再坏了。在应用中，这是一种最重要的需求，通常，除非特别指定，我们提到的运行时间都是最坏情况的运行时间

平均运行时间是所有情况中最有意义的，因为它是期望的运行时间

一般在没有特殊说明的情况下，都是指最坏时间复杂度


### 算法的空间复杂度
算法的空间复杂度通过计算算法的存储空间实现

$S(n) = O(f(n))$


其中，n为问题规模，f(n)为在问题规模为n时所占用存储空间的函数
大O表示法同样适用于算法的空间复杂度
当算法执行时所需要的空间是常数时，空间复杂度为O(1)
#### 空间与时间的策略
多数情况下，算法执行时所用的时间更令人关注
如果有必要，可以通过增加空间复杂度来降低时间复杂度
同理，也可以通过增加时间复杂度来降低空间复杂度

算法的空间复杂度通过计算算法所需的存储空间实现,算法空间复杂度的计算公式记作:S(n)=O(f(n))，其中,n为问题的规模，f(n)为语句关于n所占存储空间的函数













## 算法绪论
### 计算
- 计算=信息处理
借助某种工具，遵照一定规则，以明确而机械的形式进行
- 计算模型=计算机=信息处理工具
- 算法，即特定计算模型下，旨在解决特定问题的指令序列
#### 特点
- 输入
待处理的信息（问题）
- 输出
经处理的信息（答案）
- 正确性
的确可以解决指定的问题
- 确定性
任一算法都可以描述为一个由基本操作组成的序列
- 可行性
每一基本操作都可实现，且在常数时间内完成
- 有穷性
对于任何输入，经有穷次基本操作，都可以得到输出

程序不等于算法
#### 好算法需要具备特点
##### 正确
- 符合语法，能够编译、链接
- 能够正确处理简单的输入
- 能够正确处理大规模的输入
- 能够正确处理一般性的输入
- 能够正确处理退化的输入
- 能够正确处理任意合法的输入
##### 健壮
能辨别不合法的输入并做适当处理，而不致非正常退出
##### 可读
结构化+准确命名+注释+...
##### 效率
- 最重要的特点
- 速度尽可能快，存储空间尽可能少

### 计算模型
#### 性能测度
计算=数据结构+算法（Data Structure+Algorithm,即DSA）
不同DSA有好坏优劣之分，性能测度就是量化度量DSA的性能
#### 算法分析
##### 正确性
##### 成本
问题实例的规模，往往是决定计算成本的主要因素
规模接近，计算成本也接近
规模扩大，计算成本亦上升
#### 最坏情况
在规模同为n的所有实例中，只关注最坏（成本最高）者
#### 理想模型
- 不同的算法，可能更适用于不同规模的输入
- 不同的算法，可能更适应于不同类型的输入
- 同一算法，可能由不同程序员，用不同程序语言，经不同编译器实现
- 同一算法，可能实现并运行于不同的体系结构、操作系统

为给出客观的评判，需要抽象出一个理想的平台或模型

不再依赖于上述种种具体的因素，从而直接而准确地描述、测量并评价算法
#### 图灵机（TM模型）
#### RAM模型
和TM模型一样，RAM模型也是一般计算工具的简化与抽象
使我们可以独立于具体的平台，对算法的效率做出可信的比较与评判
在这些模型中，算法的运行时间可以转化为算法需要执行的基本操作次数

图灵机、RAM等模型为度量算法性能提供了准确的尺度

### 渐进复杂度
#### 大O记号(big-O notation)
大O记号是渐进符号，就是度量算法的尺子上的刻度
是定性估算
从形式上简化时间成本的表示
当n足够大时，常系数和n的低次项可忽略

### 复杂度分析
#### 算法分析
两个主要任务：正确性（不变性×单调性）+复杂度

C++等高级语言的基本指令，均等效于常数条RAM的基本指令，在渐进意义下，二者大体相当
- 分支转向：goto  //算法的灵魂：出于结构化考虑，被隐藏了
- 迭代循环：for()、while()、...  //本质上就是"if+goto"
调用+递归（自我调用）  //本质上也是goto

复杂度分析的主要方法：
迭代：级数求和
递归：递归跟踪+递归方程
猜测+验证

#### 级数
##### 算数级数
与末项平方同阶

$$
T(n) = 1 + 2 + \dots + n = \frac{n(n+1)}{2} = O(n^2)
$$

  
##### 幂方级数：比幂次高出一阶

$$
T_2(n) = 1^2 + 2^2 + 3^2 + \dots + n^2 = \frac{n(n+1)(2n+1)}{6} = O(n^3)
$$


$$
T_3(n) = 1^3 + 2^3 + 3^3 + \dots + n^3 = \frac{n^2(n+1)^2}{4} = O(n^4)
$$


$$
T_4(n) = 1^4 + 2^4 + 3^4 + \dots + n^4 = \frac{n(n+1)(2n+1)(3n^2+3n-1)}{30} = O(n^5)
$$

##### 几何级数(a>1)：与末项同阶

$$
T_a(n) = a^0 + a^1 + \dots + a^n = \frac{a^{n+1} - 1}{a - 1} = O(a^n)
$$


$$
1 + 2 + 4 + \dots + 2^n = 2^{n+1} - 1 = O(2^{n+1}) = O(2^n)
$$


##### 收敛级数

$$
\frac{1}{1 \cdot 2} + \frac{1}{2 \cdot 3} + \frac{1}{3 \cdot 4} + \dots + \frac{1}{(n-1) \cdot n} = 1 - \frac{1}{n} = O(1)
$$


$$
1 + \frac{1}{2^2} + \dots + \frac{1}{n^2} < 1 + \frac{1}{2^2} + \dots = \frac{\pi^2}{6} = O(1)
$$


$$
\frac{1}{3} + \frac{1}{7} + \frac{1}{8} + \frac{1}{15} + \frac{1}{24} + \frac{1}{26} + \frac{1}{31} + \frac{1}{35} + \dots = 1 = O(1)
$$


##### 几何分布

$$
(1 - \lambda)(1 + 2\lambda + 3\lambda^2 + 4\lambda^3 + \dots) = \frac{1}{1 - \lambda} = O(1), \quad 0 < \lambda < 1
$$


##### 调和级数

$$
h(n) = 1 + \frac{1}{2} + \frac{1}{3} + \dots + \frac{1}{n} = \Theta(\log n)
$$


##### 对数级数

$$
\log 1 + \log 2 + \log 3 + \dots + \log n = \log(n!) = \Theta(n \log n)
$$



#### 循环
```
for(int i=0;i<n;i++)
for(int j=0;j<n;j++)
```
算术级数：

$$
n + n + n + \dots + n = n \cdot n = O(n^2)
$$


#### 正确性的证明
通过分析算法具有不变性和单调性，来证明正确性是算法分析中非常重要和常用的方法

#### 封底估算
big-O估算是定性估算，定量估算的方法有封底估算（Back Of The Envelope Calculation）
常用封底估算常量
##### 1天

$$
1 \, \text{天} = 24 \, \text{小时} \times 60 \, \text{分钟} \times 60 \, \text{秒} \approx 25 \times 4000 = 10^5 \, \text{秒}
$$

##### 1生

$$
1 \, \text{生} \approx 1 \, \text{世纪} = 100 \, \text{年} \times 365 \, \text{天} = 3 \times 10^4 \, \text{天} = 3 \times 10^9 \, \text{秒}
$$

##### 三生三世

$$
三生三世 \approx 300 \, \text{年} = 10^{10} \, \text{秒} = \left( 1 \, \text{googol} \right)^{1/10} \, \text{秒}
$$

##### 宇宙大爆炸至今

$$
\text{宇宙大爆炸至今} = 10^{21} = 10 \times (10^{10})^2 \, \text{秒}
$$

### 迭代与递归
#### 减而治之（Decrease and conquer）--算法策略
为求解一个大规模的问题，可以将其划分为两个子问题：其一平凡，另一规模缩减。分别求解子问题，由子问题的解，得到原问题的解
##### 递归跟踪（recursion trace）分析
检查每个递归实例，累计所需时间（调用语句本身，计入对应的子实力），其总和即算法执行时间
递归跟踪直观形象，但是仅适用于简明的递归模式，如线性递归
##### 递推方程
如果递归跟踪是几何，递推方程就是代数
递推方程：

$$
T(n) = T(n-1) + O(1) = O(n)
$$


$$
T(0) = O(1)
$$


#### 分而治之（Divide and conquer）--算法策略
为求解一个大规模的问题，可以将其划分为若干（通常两个）子问题，规模大体相当，分别求解子问题，由子问题的解，得到原问题的解
##### 递推方程

$$
T(n) = 2 \times T\left(\frac{n}{2}\right) + O(1) = O(n)
$$


$$
T(0) = O(1)
$$


#### 大师定理Master Theorem

### 动态规划
DSA设计和优化的一种重要形式和手段
动态规划就是通过递归找到算法的本质后，再通过迭代实现该算法
#### 迭代
##### 解决方法1（记忆：memoization）
将已计算过实例的结果制表备查
##### 解决方法2（动态规划：dynamic programming）
颠倒计算方向，由自顶而下递归，改为自底而上迭代

#### 最长公共子序列
##### 子序列(Subsequence)
由序列中若干字符，按原相对次序构成
##### 最长公共子序列(Longest Common Subsequence)
两个序列公共子序列中的最长者，可能有多个，可能有歧义

#### 动态规划
递归可以设计出可行且正确的解，但是如果要将效率提高，使它成为一个实用算法的话，往往需要进一步调试。在调试中，动态规划扮演着重要角色，用来消除重复计算，提高效率

### 局限
#### 缓存
访问连续数据比访问间隔数据速度快，因为系统会自动把经常调用的数据和它周围的数据放到缓存中，减少IO次数，提高访问速度

## vector
vector是线性结构，笼统地成为线性序列

抽象数据类型（ADT）/Abstract Data Type

### 接口与实现
抽象数据类型=数据模型+定义在该模型上的一组操作
| 抽象定义        | 外部的逻辑特性   | 操作 & 语义               |
|-----------------|------------------|---------------------------|
| 一种定义        | 不考虑时间复杂度 | 不涉及数据的存储方式     |


数据结构=基于某种特定语言，实现ADT的一整套算法

| 具体实现        | 内部的表示与实现     | 完整的算法                   |
|-----------------|----------------------|------------------------------|
| 多种实现        | 与复杂度密切相关     | 要考虑数据的具体存储机制     |


Application->ADT Interface<-Implementation

#### vectorADT
vector是C/C++语言中，对数组的推广

C/C++语言中，数组A[]中的元素与 ```[0,n)``` 内的编号一一对应。反之，每个元素均由（非负）编号唯一指代，并可直接访问

```A[i]```的物理地址=A+i*s,s为单个元素占用的空间量，故亦称作线性数组(linear array)

vector是数组的抽象与泛化，由一组元素按线性次序封装而成，各元素与 ```[0,n)``` 内的秩(rank)一一对应，元素的类型不限于基本类型，操作、管理维护更加简化、统一与安全，可更为便捷地参与复杂数据结构的定制与实现

### 可扩充vector
#### 静态空间管理
开辟内存数组_elem[]并使用一段地址连续的物理空间
```_capacity```:总容量
```_size```:当前的实际规模n

如果采用静态空间管理策略，容量_capacity固定，则有明显的不足
1. 上溢(overflow)
```_elem[]``` 不足以存放所有元素，尽管此时系统仍有足够空间
2. 下溢(underflow)
```_elem[]``` 中的元素寥寥无几，装填因子(load factor)


$$
\lambda = \frac{\texttt{\_size}}{\texttt{\_capacity}} \ll 50\%
$$


更糟糕的是，一般的应用环境中难以准确预测空间的需求量

#### 动态空间管理
在即将发生上溢时，适当地扩大内部数组的容量

得益于vector的封装，尽管扩容后数据区的物理地址有所改变，却不致于出现野指针

#### 分摊复杂度
平均分析和分摊分析
##### 平均/期望复杂度(average/expected complexity)
根据数据结构各种操作出现概率的分布，将对应的成本加权平均
各种可能的操作，作为独立事件分别考察，割裂了操作之间的相关性和连贯性，往往不能准确地评判数据结构和算法的真实性能

##### 分摊复杂度(amortized complexity)
对数据结构连续地实施足够多次操作，所需总体成本分摊至单次操作

从实际可行的角度，对一系列操作做整体的考量，更加忠实地刻画了可能出现的操作序列，可以更为精准地评判数据结构和算法的真实性能

### 无序vector
输入敏感(input-sensitive)的算法：最好和最坏的情况复杂度相差悬殊的算法

### 有序vector唯一化
有序vector需要元素间可以相互比较
有序/无序序列中，任意/总有一对相邻元素顺序/逆序

因此，相邻逆序对的数目，可用以度量向量的逆序程度

无序向量经预处理转换为有序向量之后，相关算法多可优化

### 有序vector二分查找（版本A）
#### 原理
二分（折半）策略：轴点mi总是取作中点，于是每经过至多两次比较，或者能够命中，或者将问题规模缩减一半

二分查找法bin_search(e,lo,hi)
先找最中间的元素V[mi],如果e < V[mi]，在V(lo,mi)中继续二分查找,查找长度是1；如果V[mi] < e，在V(lo,mi)中继续二分查找，查找长度是2；如果e=V[mi]，查找长度是2

二分查找法bin_search的渐进复杂度是O(1.5logn)

**代码中一般都使用<，而不是>号，便于阅读和理解**

如何更为精细地评估查找算法的性能？
考查关键码的比较次数，即查找长度(search length)
通常，需分别针对成功与失败查找，从最好、最坏、平均等角度评估

### 有序vector Fib查找（Fibonacci search）
#### 构思
二分查找版本A的效率仍有改进余地，因为转向左、右分支前的关键码比较次数不等，而递归深度却相同

Fibonacci查找的ASL，（在常系数的意义上）优于二分查找
#### 最优性
通用策略：对于任何的 ```A[0,n)``` ，总是选取A[λn]作为轴点，0<=λ<1
例如：二分查找对应λ=0.5，Fibonacci查找对应λ=φ=0.6180339
经过计算，λ=φ是最优解，性能最好

### 有序vector二分查找（版本B）
#### 构思
二分查找中左、右分支转向代价不平衡的问题，也可直接解决

每次迭代（或每个递归实例）仅做1次关键码比较，这样所有分支只有2个方向，而不再是3个

同样的，轴点mi取作中点，则查找每深入一层，问题规模也缩减一半
1. e < x: 则e若存在，必属于左侧子区间```s[lo,mi)```，故可递归深入
2. x <= e:则e若存在，必属于右侧子区间```s[mi,hi)```，亦可递归深入

只有当元素数目hi-lo=1时，才判断该元素是否命中

相对于版本A，最好（坏）情况下更坏（好），各种情况下的SL更加接近，整体性能更趋稳定

### 有序vector二分查找（版本C）

### 有序vector插值查找（Interpolation Search）
#### 原理
假设：已知有序向量中各元素随机分布的规律
比如均匀且独立的随机分布
于是 ```[lo,hi)``` 内各元素应大致按照线性趋势增长
因此通过猜测轴点mi，可以极大地提高收敛速度

最坏情况：O(hi-lo)=O(n)
最好情况：稍试即中，初试即中
平均情况：每经过一次比较，O(n)缩至O(根号n)

插值查找本质上是对二进制位宽度做二分查找

插值查找 O(loglogn)---log以2为底

$$
m_i = l_o + (h_i - l_o) \cdot \frac{e - A[l_o]}{A[h_i] - A[l_o]}
$$


#### 综合对比
插值查找通常优势不明显，除非查找区间宽度极大，或者比较操作成本极高
易受小扰动的干扰和蒙骗，在局部花费非常多的时间
须引入乘法和除法运算，比加减法成本高

实际可行的办法：
首先通过插值查找，将查找范围缩小到一定的范围，然后再进行二分查找

| 规模       | 查找方法   |
|------------|------------|
| 大规模     | 插值查找   |
| 中规模     | 折半查找   |
| 小规模     | 顺序查找   |





## 线性表
### 基本概念
#### 定义
线性表（List）是0个或多个数据元素的有限序列
线性表中的数据元素之间是有顺序的
线性表中的数据元素个数是有限的
线性表中的数据元素的类型必须相同
#### 数学定义
线性表是具有相同类型的n(n>=0)个数据元素的有限序列(a1,a2,...,an)

ai是表项，n是表长度
#### 性质
a0为线性表的第一个元素，只有一个后继
an为线性表的最后一个元素，只有一个前驱
除了a0和an外的其他元素ai，既有前驱，又有后继
线性表能够逐项访问和顺序存取
#### 线性表的操作
- 创建线性表
- 销毁线性表
- 清空线性表
- 将元素插入线性表
- 将元素从线性表中删除
- 获取线性表中某个位置的元素
- 获取线性表的长度

线性表在程序中表现为一种特殊的数据类型
线性表的操作在程序中的表现为一组函数

### 线性表的顺序存储结构
#### 基本概念
线性表的顺序存储结构，指的是用一段地址连续的存储单元一次存储线性表的数据元素

线性表的顺序结构可以用C语言的一维数组来实现

为了建立一个线性表，要在内存中找一块地，于是这块地的第一个位置就非常关键，它是存储空间的起始位置
#### 数据长度与线性表长度区别
数组的长度是存放线性表的存储空间的长度，存储分配后这个量一般是不变的

线性表的长度是线性表中数据元素的个数，随着线性表插入和删除操作的进行，这个量是变化的

在任意时刻，线性表的长度应该小于等于数组的长度
#### 存取性能
线性表的顺序存储结构存取时间性能为O(1)，通常把具有这一特点的存储结构称为随机存取结构

#### 设计与实现
##### 插入与删除
###### 插入元素算法
- 第1步：判断线性表是否合法
- 第2步：判断插入位置是否合法
- 第3步：把最后一个元素到插入位置的元素后移一个位置
- 第4步：将新元素插入
- 第5步：线性表长度加1

###### 删除元素算法
- 第1步：判断线性表是否合法
- 第2步：判断删除位置是否合法
- 第3步：将元素取出
- 第4步：将删除位置后的元素分别向前移动一个位置
- 第5步：线性表长度减1

##### 查找元素
###### 获取元素操作
- 第1步：判断线性表是否合法
- 第2步：判断位置是否合法
- 第3步：直接通过数组下标方式获取元素

#### 优点
- 无需为线性表中的逻辑关系增加额外的空间
- 可以快速的获取表中合法位置的元素
#### 缺点
- 插入和删除操作需要移动大量元素
- 当线性表长度变化较大时难以确定存储空间的容量
- 造成存储空间的“碎片”


### 线性表的链式存储结构
线性表的链式存储结构的特点是用一组任意的存储单元存储线性表的数据元素
#### 基本概念
为了表示每个数据元素与其直接后继元素之间的逻辑关系，每个元素除了存储本身的信息外，还需要存储指示其直接后继元素的地址
##### 结点
存储数据元素信息的域称为数据域，存储直接后继位置的域称为指针域。指针域中存储的信息称作指针或链。这两部分信息组成数据元素的存储映像，称为结点（Node）

结点由存放数据元素的数据域和存放后继结点的指针域组成

n个结点（a1的存储映像）链结成一个链表，即为线性表（a1，a2，...，）的链式存储结构，因此此链表的每个结点中只包含一个指针域，所以叫单链表

#### 头指针和头结点
##### 头指针
链表中第一个结点的存储位置叫头指针，整个链表的存取必须从头指针开始进行。之后的每一个结点，其实就是上一个的后继指针指向的位置。最后一个结点指针为“空”（通常用NULL或“^”符号表示）

##### 头结点
有时，为了操作方便，会在单链表的第一个结点前增加一个结点，称为头结点，头结点的数据域可以不存储任何信息，也可以存储如线性表的长度等附加信息，头结点的指针域存储指向第一个结点的指针

##### 头指针与头结点的异同点
头指针是指链表指向的第一个结点的指针，若链表有头结点，则是指向头结点的指针
- 头指针有标志作用，一般用头指针冠以链表的名字
- 不论链表是否为空，头指针均不为空


头结点是为了操作的方便而设立的，数据域一般无意义
- 有了头结点，对在第一元素结点前插入结点和删除结点等操作就统一了
- 头结点不一定是链表必须要素


#### 设计与实现
##### 单链表的读取
###### 获取元素算法
因为链式存储的第i个元素到底在哪里，没办法一开始就知道，必须从头开始找
- 第1步：声明一个结点p指向链表第一个结点，初始化j从1开始
- 第2步：当 j < i 时，就遍历链表,让p的指针向后移动,不断指向下一结点,j累加1
- 第3步：若到链表末尾p为空,则说明第i个元素不存在
- 第4步：否则查找成功，返回结点p的数据

最坏的情况的时间复杂度是O(n)

**由于单链表的结构中没有定义表长，所以不能事先知道要循环多少次，因此也就不方便使用for来控制循环。其主要核心思想就是“工作指针后移”，这其实也是很多算法的常用技术**

##### 单链表的插入与删除
###### 单链表的插入算法
- 第1步：声明一结点p指向链表第一个结点,初始化j从1开始
- 第2步：当 j < i 时,就遍历链表,让p的指针向后移动,不断指向下一结点，j累加1
- 第3步：若到链表末尾p为空，则说明第i个元素不存在
- 第4步：否则查找成功，在系统中生成一个空结点s
- 第5步：将数据元素e赋值给s->data
- 第6步：单链表的插入标准语句
```
s->next=p->next;
p->next=s;
```
**注意：这是插入算法的核心代码，这两句代码顺序不能反**

- 第7步：返回成功

**对于单链表的表头和表尾的特殊情况，操作时相同的**

###### 单链表的删除算法
将要删除元素的前继结点的指针绕过，指向它的后继结点即可

核心代码
```
p->next=p->next->next
```
遍历单链表找到第i-1个结点p，若存在这样的结点也存在后继结点，则删除该后继结点

- 第1步：声明一结点p指向链表第一个结点,初始化j从1开始
- 第2步：当 j < i 时,就遍历链表，让p的指针向后移动，不断指向下一个结点，j累加1
- 第3步：若到链表末尾p为空,则说明第i个元素不存在
- 第4步：否则查找成功，将欲删除的结点 ```p->next``` 赋值给q
- 第5步：单链表的删除标准语句 ```p->next=q->next```
- 第6步：将q结点中的数据赋值给e,作为返回
- 第7步：释放q结点
- 第8步：返回成功

##### 整表创建与整表删除
###### 整表创建算法
头插法
- 第1步：声明一结点p和计数器变量i
- 第2步：初始化一空链表L
- 第3步：让L的头结点的指针指向NULL,即建立一个带头结点的单链表
- 第4步：循环以下操作：
  1. 生成一新结点赋值给p
  2. 随机生成一数字赋值给p的数据域p->data
  3. 将p插入到头结点与前一新结点之间

这里使用的是插队的方法，即始终让新结点在第一位，这种算法简称为头插法



尾插法
创建一个LinkList类型的指针作为尾指针，始终指向最后一个结点

**尾插法需要将新结点插入到当前链表的表尾上，为此需要增加一个尾指针r,使其始终指向当前链表的尾结点，最后需要将尾结点的next域置为NULL**


###### 整表删除算法
- 第1步：声明一结点p和q
- 第2步：将第一个结点赋值给p
- 第3步：循环以下操作：
  1. 将下一结点赋值给q
  2. 释放p
  3. 将q赋值给p



##### 链表基本操作
###### 初始化链表
###### 销毁链表
该运算会释放L所占的内存空间，其过程是让pre，p指向两个相邻的结点，当p不为空时循环，释放pre，然后p，pre同步后移一个结点
###### 判断链表是否为空
###### 求链表的长度
由于链表没有存放结点个数的信息，所以需要遍历链表，用n来记录结点个数
###### 输出链表
###### 查找链表第i个元素值
- 第1步：声明一个结点p，指向链表的第一个结点，初始化j从1开始
- 第2步：当 j < i 时，就遍历链表，让p的指针向后移动，不断指向下一结点，j累加1
- 第3步：若到链表末尾，p为空，说明第i个元素不存在
- 第4步：否则查找成功，返回结点p的数据

**核心思想是工作指针后移**

###### 插入数据结点
遍历单链表找到第i-1个结点p，将元素值为e的结点*s插入到其后
###### 删除数据结点

#### 优点
- 无需一次性定制链表的容量
- 插入和删除操作无序移动数据元素
#### 缺点
- 数据元素必须保存后继元素的位置信息
- 获取指定数据的元素操作，需要顺序访问之前的元素

#### 单链表存储结构与顺序存储结构对比
##### 存储分配方式
- 顺序存储结构用一段连续的存储单元依次存储线性表的数据元素
- 单链表采用链式存储结构，用一组任意的存储单元存放线性表的元素
##### 时间性能
###### 查找
- 顺序存储结构O(1)
- 单链表O(n)
###### 插入和删除
- 顺序存储结构需要平均移动表长一半的元素，时间为O(n)
- 单链表在删除某位置的指针后，插入和删除时间仅为O(1)
##### 空间性能
- 顺序存储结构需要预分配存储空间，分大了浪费，分小了易发生上溢
- 单链表不需要分配存储空间，只要有就可以分配，元素个数也不受限制

#### 总结
- 如果线性表需要频繁查找，很少进行插入和删除操作时，宜采用顺序存储结构；若需要频繁插入和删除时，宜采用单链表结构。例如游戏中，玩家账号信息除注册是插入数据，绝大多数情况都是读取，应考虑使用顺序存储结构；而玩家的装备和道具列表，随时会增加或删除，应该考虑使用单链表结构
- 当线性表中的元素个数变化较大或根本不知道有多大时，最好用单链表结构，这样可以不需要考虑存储空间的大小问题；而如果事先知道线性表的大致长度，用顺序存储结构效率会高很多



### 静态链表
用数组描述的链表称为静态链表，或游标实现法
通常把未被使用的数组元素称为备用链表
#### 优点
在插入和删除操作时，只需要修改游标，不需要移动元素，进而改进了在顺序存储结构中插入和删除操作需要移动大量元素的缺点
#### 缺点
- 没有解决连续存储分配带来的表长难以确定的问题
- 失去了顺序存储结构随即存取的特性

*静态链表其实是为了给没有指针的高级语言设计的一种实现单链表能力的方法*

### 循环链表
#### 定义
将单链表中最后一个数据元素的next指针由空指针改为指向第一个元素，使整个单链表形成一个环，这种头尾相接的单链表称为单循环链表，简称循环链表（circular linked list）

循环链表解决了如何从链表当中一个结点出发，访问到链表的全部结点这个很麻烦的问题
#### 游标
与线性链表相比，循环链表增加了游标功能

在循环链表中可以定义一个“当前”指针，这个指针通常称为游标（slider），可以通过这个游标来遍历链表中的所有元素
#### 单向循环链表和单向链表的区别
- 单向链表为头指针，循环链表为尾指针，头指针指向头结点，尾指针指向终端结点
- 为统一方便操作，单向链表设置头结点，单向循环链表设置头结点和尾结点
- 设置尾结点后，尾指针指向尾结点，插入，删除等操作不用移动尾指针


在单向链表中，链表最后一个结点的next指针指向NULL，而所谓的循环链表就是将最后一个结点本来指向空的改变其指针指向从而指向首结点（注意不是头结点，头结点是方便链表操作而存在的结构体，循环链表是一种特殊的单向链表）循环链表可以继承自单向链表

#### 循环链表实现难点
- 插入数据的位置为0时，头结点和尾结点的next指针均指向新结点
- 删除位置为0的结点时，头结点和尾结点的next指针都指向为位置为1的结点，然后安全删除位置为0的结点

#### 循环链表新增操作
- 将游标重置指向链表中的第一个数据元素
- 获取当前游标指向的数据元素
- 将游标移动指向到链表中的下一个数据元素
- 直接指定删除链表中的某个数据元素

#### 优点
循环链表在单链表的基础上做了功能加强
#### 缺点
代码复杂度提高了
#### 约瑟夫问题（循环链表的典型应用）
n个人围成一个圆圈，首先第1个人从1开始，每个人依次按顺时针报数，报到第m个人，令其出列，然后再从下一个人开始从1顺时针报数，报到第m个人，再令其出列，...，如此下去，求出列顺序


### 双向链表（double linked list）
单链表的结点都只有一个指向下一个结点的指针
单链表的数据元素无法直接访问其前驱元素
逆序访问单链表中的元素是及其耗时的操作，算法复杂度是O(n^2)
#### 定义
在单链表的结点中增加一个指向其前驱的pre指针
双向链表中的结点都有两个指针域，一个指向直接后继，另一个指向直接前驱
```
p->next->pre=p=p->pre->next
```
双向链表也可以是循环表

#### 双向链表拥有单链表的所有操作
- 创建链表
- 销毁链表
- 获取链表长度
- 清空链表
- 获取第pos个元素操作
- 插入元素到位置pos
双向链表在插入元素时，需要更改两个指针变量
核心代码：
```
s->pre=p;
s->next=p->next;
p->next->pre=s;
p->next=s;
```
要特别注意它们的顺序

##### 删除位置pos处的元素
双向链表在删除元素时，需要更改两个指针变量
核心代码：
```
p->pre->next=p->next;
p->next->pre=p->pre;
free(p);
```

#### 双向链表的新操作
- 获取当前游标指向的数据元素
- 将游标重置指向链表中的第一个数据元素
- 将游标移动指向到链表中的下一个数据元素
- 将游标移动指向到链表中的上一个数据元素

#### 优点
- 双向链表在单链表的基础上增加了指向前驱的指针
- 功能上双向链表可以完全取代单链表的使用
- 循环链表的Next、Pre和Current操作可以高效地遍历链表中的所有元素

#### 缺点
代码复杂
#### 总结
双向链表比单向链表复杂，且占用空间比单向链表大，但是，由于它良好的对称性，使得对某个结点的前后结点的操作，带来了方便，可以有效提高算法的时间性能，即用空间换时间


## 栈与队列
### 栈的定义
栈（stack）限定仅在表尾进行插入和删除操作的线性表
#### LIFO
允许插入和删除的一端称为栈顶（top），另一端称为栈底（bottom），不含任何数据元素的栈称为空栈。栈又称为后进先出（Last In First Out,LIFO）线性表，简称LIFO结构
#### 理解栈的定义
- 首先它是一个线性表，也就是说，栈元素具有线性关系，即前驱后继关系。只不过它是一种特殊的线性表。定义中说是在线性表的表尾进行插入和删除操作,这里表尾是指栈顶而不是栈底
- 它的特殊之处就在于限制了这个线性表的插入和删除位置，它始终只在栈顶进行。这也就使得：栈底是固定的,最先进栈的只能在栈底
#### 进栈出栈变化形式
栈对线性表的插入和删除的位置进行了限制，并没有对元素进出的时间进行限制。也就是说，在不是所有元素都进栈的情况下，已进栈的元素也可以出栈，只要保证是栈顶元素出栈就可以

### 栈的顺序存储结构
#### 定义
栈的顺序存储是线性表顺序存储的简化，简称为顺序栈
线性表的顺序存储结构是用数组实现的，在顺序栈中，下标为0的一端作为栈底

- 通常定义一个top变量来指示栈顶元素在数组中的位置，类似游标
- 存储栈的长度为StackSize
- top必须小于StackSize，当栈存在一个元素时，top==0
- 因此通常把空栈的判定条件定为top==-1
#### 栈的操作
##### 栈的插入操作（push）
栈的插入操作叫做进栈，也称压栈、入栈

- 第1步：判断栈是否已满，已满直接返回
- 第2步：栈顶变量Top增加1
- 第3步：将新插入元素赋值给栈顶空间

##### 栈的删除操作（pop）
栈的删除操作叫做出栈，也称弹栈

- 第1步：判断栈是否为空，如果为空直接返回
- 第2步：把将要删除的栈顶元素赋值给e，在后序用户逻辑中使用
- 第3步：栈顶变量top减少1

##### 时间复杂度
入栈和出栈操作均没有涉及任何循环语句，时间复杂度均为O(1)

### 两栈共享空间
栈的顺序存储结构使用方便，它只准栈顶进出元素，所以不存在线性表插入和删除时需要移动元素的问题。但是它有一个很大的缺陷，就是必须事先确定数组存储空间，对于一个栈，只能尽量考虑周全，设计出合适大小的数组来处理

但是对于两个相同类型的栈，我们可以做到最大限度地利用其实现开辟的存储空间来进行操作

如果有两个相同类型的栈，分别各自开辟了数组空间，极有可能第一个栈已经满了，而另一个栈还有很多存储空间。我们完全可以用一个数组来存储两个栈，这需要一些技巧

#### 实现原理
让一个栈的栈底为数组的起始端，即下标为0处，另一个栈的栈底为数组的末端，即下标为数组长度n-1处。两个栈如果增加元素，就是两端点向中间延伸
#### 关键思路
- 他们在数组两端，向中间靠拢，top1和top2是栈1和栈2的栈顶指针，只要top1和top2不见面，两个栈就可以一直使用
- 栈1为空时，top1==-1，栈2为空时，top2==n
- 两个栈顶指针相差1时栈满，即top1+1==top2
#### 进栈实现
两栈共享空间的push方法，除了要插入元素值参数外，还需要有一个判断是栈1还是栈2的栈号参数stackNumber
- 第1步：判断栈是否已满，已满直接返回
- 第2步：通过传入的stackNumber判断是栈1还是栈2进栈
- 第3步：如果是栈1进栈，先top1+1，后给数组元素赋值；如果是栈2进栈，先top2-1，后给数组元素赋值
#### 出栈实现
两栈共享空间的pop方法，只需要判断是栈1还是栈2
- 第1步：如果是栈1出栈，判断栈1是否为空，如果不为空，栈顶元素出栈，top1-1
- 第2步：如果是栈2出栈，判断栈2是否为空，如果不为空，栈顶元素出栈，top2+1
#### 总结
事实上，使用两栈共享空间的数据结构，通常都是当两个栈的空间需求有相反关系时，即一个栈增长时另一个栈在缩减的情况，这样使用两栈共享空间存储方法才有较大的意义。否则两个栈都在不停增长，很快就会溢出

而且要注意，两栈共享空间仅适用于两个栈具有相同的数据类型


### 栈的链式存储结构
#### 定义
栈的链式存储结构，简称链栈

通常把栈顶放在单链表的头部，这样头指针和栈顶指针可以合二为一

对于链栈来说，头部已经有栈顶了，头结点失去意义，不需要头结点

链栈基本不存在栈满的情况，除非内存空间不足。所以不存在溢出问题

对于空栈来说，链表原定义是头指针指向空，那么链栈的空栈就是 ```top==NULL```

链栈的操作绝大部分和单链表类似，只是在插入和删除上特殊一些
#### 链栈的结构代码（C语言实现）
```
typedef struct StackNode    //链栈的栈结点
{
    SElemType data;
    struct StackNode* next;
}StackNode,*LinkStackPtr;

typedef struct LinkStack    //链栈
{
    LinkStackPtr top;       //指向栈顶的top指针
    int count;
}LinkStack;
```
#### 链栈进栈操作
插入元素e为新的栈顶元素
- 第1步：创建一个新结点（开辟内存，创建一个LinkStackPtr指针s，把e赋值给s的data变量）
- 第2步：把当前的栈顶元素赋值给新结点的直接后继
- 第3步：将新的结点s赋值给栈顶指针
- 第4步：链栈的计数count加1

#### 链栈的出栈操作
- 第1步：创建一个LinkStackPtr指针p并置空
- 第2步：判断栈是否为空，为空则直接返回
- 第3步：把栈顶结点的数据，即top->data赋值给e用以用户后续逻辑
- 第4步：把栈顶结点赋值给p
- 第5步：把栈顶指针下移一位，指向后一结点，即 ```S->top=S->top->next```
- 第6步：释放结点p
- 第7步：链栈的计数count减1

#### 时间复杂度
链栈的进栈push和出栈pop都没有循环操作，时间复杂度均为O(1)


#### 顺序栈和链栈对比
- 它们在时间复杂度上是一样的，都是O(1)，对于空间性能，顺序栈需要事先确定一个固定的长度，可能会存在内存空间浪费的问题，但它的优势是存取时定位很方便，而链栈则要求每个元素都有指针域，这同时也增加了一些内存开销，但对于栈的长度无限制
- 如果栈的使用过程中元素变化不可预料,有时很小,有时非常大，那么最好是用链栈
- 如果元素变化在可控范围内,使用顺序栈会更好一些

### 栈的作用
栈的引入简化了程序设计的问题，划分了不同关注层次，使得思考范围缩小，更加聚焦于我们要解决的问题核心。反之，像数组等，因为要分散精力去考虑数组的下标增减等细节问题,反而掩盖了问题的本质

### 栈的应用----递归
#### 斐波那契数列
后一项等于前面相邻两项之和
#### 递归定义
在高级语言中，调用自己和其他函数开没有本质的个问。我们把一个直接调用自己或通过一系列的调用语句间接地调用自己的函数，称做递归函数

递归定义必须至少有一个条件，满足时递归不再进行，即不再引用自身而是返回值退出
#### 递归和迭代的比较
##### 递归的优点
迭代使用的是循环结构，递归使用的是选择结构，递归能使程序的结构更清晰、更简洁、更容易让人理解
##### 递归的缺点
大量的递归调用会建立函数的副本，会耗费大量的时间和内存。迭代则不需要反复调用函数和占用额外的内存，因此应该视不同情况选择不同的代码实现方式
#### 递归与栈的关系
- 递归过程分为前行和退回两个阶段，很符合栈的数据结构
- 在前行阶段，对于每一层递归，函数的局部变量、参数值以及返回地址都被压入栈中。在退回阶段，位于栈顶的局部变量、参数值和返回地址被弹出，用于返回调用层次中执行代码的其余部分，也就是恢复了调用的状态
- 当然，对于现在的高级语言，这样的递归问题是不需要用户来管理这个栈的，一切都由系统代劳了

### 栈的应用----四则运算表达式求值
#### 后缀（逆波兰）表示法定义（Reverse Polish Notation,RPN）
##### 原理
- 括号都是成对出现的，有左括号就一定会有右括号，对于多重括号，最终也是完全嵌套匹配的
- 这用栈结构正好合适，只有碰到左括号，就将此左括号进栈、不管表达式有多少重括号，反正遇到左括号就进栈，而后面出现右括号时，就让栈顶的左括号出栈，期间让数字运算，这样,最终有括号的表达式从左到右巡查一遍，栈应该是由空到有元素，最终再因全部匹配成功后成为空栈的结果
##### 后缀表达式计算方法
从左到右遍历表达式的每个数字和符号，遇到是数字就进栈，遇到是符号，就将处于栈顶两个数字出恚，进行运算，运算结果进栈，一直到最终获得结果
##### 中缀表达式转后缀表达式
标准四则运算表达式也叫中缀表达式
###### 转换规则
从左到右遍历中缀表达式的每个数字和符号，若是数字就输出，即成为后缀表达式的一部分;若是符号，则判断其与栈顶符号的优先级，是右括号或优先级低于栈顶符号（乘除优先加减)则栈顶元素依次出栈并输出，并将当前符号进栈，一直到最终输出后缀表达式为止


要想让计算机具有处理我们通常的使用的中缀表达式的能力，最重要的就是两步：
1. 将中缀表达式转化为后缀表达式（栈用来进出运算的符号）
2. 将后缀表达式进行运算得出结果（栈用来进出运算的数字）

### 队列的定义（Queue）
队列是只允许在一端进行插入操作，在另一端进行删除操作的线性表

队列是一种先进先出（First In First Out，FIFO）的线性表
允许插入的一端称为队尾，允许删除的一端称为队头

队列在程序设计中用的非常频繁

### 循环队列
#### 队列顺序存储的不足
##### 原始版队列顺序存储
如果把0号位置作为队头，在出队时，所有的元素都要移动，时间复杂度O(n)
##### 改进版队列顺序存储
- 如果不把0号位置作为队头，引入两个指针，front指针指向队头元素，rear指针指向队尾元素的下一个位置，这样当front等于rear时，队列为空队列
- 解决了时间复杂度的问题，但是会出现假溢出
- 如果rear已到达开辟的内存的末尾处，而此刻前面有已出队列的空闲位置，它也无法填补空闲位置，只能发生假溢出

**假溢出：数组已满，但是前面有的地方如下标为0、1是空闲的**
#### 循环队列的定义
解决假溢出的办法是后面满了，就再从头开始，也就是头尾相接的循环

队列的这种头尾相接的顺序存储结构称为循环队列
#### 循环队列的front和rear指针
当空队列时，```front==rear```，但是在循环队列中，当队列满时，front也等于rear

判断此时队列是空的还是满的有两种方法
##### 方法一
设置一个标志变量flag，当```front==rear``` ，且flag=0时队列为空，当 ```front==rear```，且flag=1时队列为满
##### 方法二
当队列为空时，条件就是 ```front==rear``` 。当队列满时，我们修改其条件，保留一个元素空间。当数组中还有一个空闲单元，就标定为此队列已满

方法二中队列已满的条件为 
```(rear+1)%QueueSize==front```
通用的计算队列长度公式为

$\left(\text{rear} - \text{front} + \text{QueueSize}\right) \% \text{QueueSize}$


### 队列的链式存储结构
队列的链式存储结构，其实就是线性表的单链表，只不过它只能尾进头出而已，简称为链队列

为了操作方便，将队头指针指向链队列的头结点，队尾指针指向终端结点

空队列时，front和rear都指向头结点

#### 队列的链式存储结构--入队操作
入队操作就是在链表尾部插入结点
#### 队列的链式存储结构--出队操作
出队操作就是头结点的后继结点出队，将头结点的后继改为它后面的结点，如果链表除头结点外只剩一个元素，则需要将rear指向头结点

#### 链队列与循环队列的比较
##### 时间上
时间复杂度都是O(1)，但是循环队列是事先申请好空间，使用期间不释放，而对于链队列，每次申请和释放结点也会存在一些时间开销，如果入队出队频繁，则两者还是有细微差异
##### 空间上
对于空间上来说，循环队列必须有一个固定的长度，所以就有了存储元素个数和空间浪费的问题。而链队列不存在这个问题。尽管它需要一个指针域，会产生一些空间上的开销，但也可以接受。所以在空间上链队列更加灵活
##### 适用情况
在可以确定队列长度最大值的情况下，用循环队列
如果无法预估队列的长度时，则用链队列


## 串
### 串的定义
串（string）是由零个或多个字符组成的有限序列，又叫字符串
一般记为s="a1a2...an"(n>=0)，其中，s是串的名称，用双引号括起来的字符序列是串的值，注意双引号不属于串的内容，ai（1<=i<=n）可以是字母、数字或其他字符，i就是该字符在串中的位置
#### 基本概念
- 串中的字符数目n称为串的长度，定义中的有限就是指长度n是一个有限的数值
- 零个字符的串称为空串（null string），它的长度为零，可以直接用两个双引号表示""""，也可以用希腊字母"Φ"表示
- 所谓的序列，说明串的相邻字符之间具有前驱和后继的关系
#### 其他概念
##### 空格串
是只包含空格的串。注意它与空串的区别，空格串是有内容有长度的，而且可以不止一个空格
##### 子串与主串
串中任意个数的连续字符组成的子序列称为该串的子串，相应地，包含子串的串称为主串

子串在主串中的位置就是子串的第一个字符在主串中的序号

例如“over”、“end”、“lie”其实可以认为是“lbver”、"friend”,“believe”这些单词字符串的子串

### 串的比较
串的比较是通过组成串的字符之间的编码来进行的，而字符的编码指的是字符在对应字符集中的序号
#### 常用编码
##### ASCII编码
计算机中常用字符是使用标准的ASCII编码，由8位二进制数表示一个字符，总共可以表示256个字符

可以满足以英语为主的语言和特殊符号进行输入、存储、输出等操作的字符需要
##### Unicode编码
是16位二进制数表示的一个字符，总共可以表示2的16次方个字符，足够表示世界上所有语言的所有字符。为了与ASCII码兼容，Unicode的前256个字符和ASCII码完全相同
#### 相等
在C语言中比较两个串相等，必须是串的长度以及各个对应位置的字符都要相等，才算是相等
#### 不相等
给定两个串:```s=“a1a2……an"，t=“b1b2……bm”``` ，当满足以下条件之一时，s < t。
##### n<m,且a=bi(i=1,2,……,n)
例如当s=“hap”, t=“happy”，就有s < t。因为t比s多出了两个字母
##### 存在某个k≤min(m,n)，使得 ```ai=bi (i=1，2，……, k-1)，ak<bk```
例如当s=“happen”, t=“happy",因为两串的前4个字母均相同，而两串第5个字母(k值)，字母e的 ASCII码是101，而字母y的ASCII 码是121，显然e < y,所以s < t

### 串的抽象数据类型
串的逻辑结构和线性表很相似，不同之处在于串针对的是字符集
因此，对于串的基本操作与线性表是有很大差别的。线性表更关注的是单个元素的操作，比如查找一个元素，插入或删除一个元素，但串中更多的是查找子串位置、得到指定位置子串、替换子串等操作

### 串的存储结构
串的存储结构与线性表相同，也分为两种
#### 串的顺序存储结构
串的顺序存储结构是用一组地址连续的存储单元来存储串中的字符序列的。按照预定义的大小，为每个定义的串变量分配一个固定长度的存储区。一般是用定长数组来定义

既然是定长数组，就存在一个预定义的最大串长度，一般可以将实际的串长度值保存在数组的0下标位置，有的书中也会定义存储在数组的最后一个下标位置。但也有些编程语言不想这么干，觉得存个数字占个空间麻烦。它规定在串值后面加一个个计入串长度的结束标记字符,比如“\O”来表示串值的终结

这样的串的顺序存储方式其实是有问题的,因为字符串的操作，比如两串的连接Concat、新串的插入Strlnsert，以及字符串的替换Replace，都有可能使得串序列的长度超过了数组的长度MaxSize

对于串的顺序存储，串值的存储空间可在程序执行过程中动态分配而得。比如放置在堆区。这个堆

可由C语言的动态分配函数 ```malloc()``` 和 ```free()``` 来管理


#### 串的链式存储结构
对于串的链式存储结构，与线性表是相似的，但由于串结构的特殊性，结构中的每个元素数据是一个字符，如果也简单的应用链表存储串值，一个结点对应一个字符，就会存在很大的空间浪费。因此，一个结点可以存放一个字符，也可以考虑存放多个字符，最后一个结点若是未被占满时，可以用“#”或其他非串值字符补全

一个结点存多少个字符才合适就变得很重要，这会直接影响着串处理的效率，需要根据实际情况做出选择

但串的链式存储结构除了在连接串与串操作时有一定方便之外，总的来说不如顺序存储灵活,性能也不如顺序存储结构好

### 朴素的模式匹配算法
在一个字符串汇总查询子串的定位操作通常称作串的模式匹配，是串中最重要的操作之一

就是对主串的每一个字符作为子串的开头，与要匹配的字符串进行匹配。对主串做大循环，每个字符开头做子串的长度的小循环，直到匹配成功或全部遍历完成为止

最坏情况时间复杂度为O((n-m+1)*m)，效率过于低下

### KMP模式匹配算法
#### 原理
#### 时间复杂度
O(n+m)

KMP算法仅当子串与主串之间存在许多“部分匹配”的情况下才体现出它的优势，否则两者差异并不明显



## 树
### 定义
树(Tree)是n(n≥0）个结点的有限集。n=0时称为空树
在任意一棵非空树中:
- 有且仅有一个特定的称为根(Root)的结点
- 当n>1时,其余结点可分为m(m>0）个互不相交的有限集T1、T2、……、Tm,其中每一个集合本身又是一棵树,并且称为根的子树（SubTree)

#### 对于树的定义还需要特别注意两点
- n>0时，根结点是唯一的，不可能存在多个根结点，数据结构中的树只能有一个根结点
- m>0时，子树的个数没有限制，但它们一定是互不相交的

#### 结点分类
树的结点包含一个数据元素及若干指向其子树的分支
结点拥有的子树数称为结点的度(Degree)

- 度为0的结点称为叶结点(Leaf)或终端结点
- 度不为0的结点称为非终端结点或分支结点
- 除根结点之外，分支结点也称为内部结点

树的度是树内各结点的度的最大值

#### 结点间关系
- 结点的子树的根称为该结点的孩子（Child)，相应地，该结点称为孩子的双亲(Parent)
- 同一个双亲的孩子之间互称兄弟（Sibling)
- 结点的祖先是从根到该结点所经分支上的所有结点
- 以某结点为根的子树中的任一结点都称为该结点的子孙

#### 树的其他相关概念
- 结点的层次(Level)从根开始定义起，根为第一层，根的孩子为第二层
若某结点在第Ⅰ层，则其子树的根就在第1+1层
- 其双亲在同一层的结点互为堂兄弟
- 树中结点的最大层次称为树的深度(Depth)或高度
- 如果将树中结点的各子树看成从左至右是有次序的，不能互换的，则称该树为有序树，否则称为无序树
- 森林(Forest)是m(m≥0)棵互不相交的树的集合。对树中每个结点而言，其子树的集合即为森林

#### 树结构与线性结构的对比
##### 线性结构
- 第一个数据元素：无前驱
- 最后一个数据元素：无后继
- 中间元素：一个前驱一个后继
##### 树结构
- 根结点：无双亲，唯一
- 叶结点：无孩子，可以存在多个叶结点
- 中间结点：一个双亲，多个孩子

### 树的存储结构
#### 双亲表示法
- 以一组连续空间存储树的结点，同时在每个结点中，附设一个指示器指示其双亲结点到链表中的位置
- 即每个结点知道自己是谁，也知道双亲在哪里
- 结点包含一个数据域data和一个指针域parent，存储该结点的双亲在数组中的下标

根结点没有双亲，根结点的位置域设置为-1

#### 孩子表示法
使用多重链表来存储树的结点，即每个结点有多个指针域，其中每个指针指向一棵子树的根结点，这种方法叫做多重链表表示法

因为树的每个结点的度，也就是孩子数是不同的，可以设计两个方案来解决
##### 方案一
- 指针域的个数等于树的度
- 数据域data
- 指针域child1到childd，指向该结点的孩子结点

这种方法对于树中各结点的度相差很大时，显然是很浪费空间的，因为有很多的结点的指针域都是空的。不过如果树的各结点度相差很小时，那就意味着开辟的空间被充分利用了,这时存储结构的缺点反而变成了优点
##### 方案二
- 每个结点指针域的个数等于该结点的度，并专门取一个位置来存储结点指针域的个数
- 数据域data
- 度域degree
- 指针域child1到childd，指向该结点的孩子结点

这种方法克服了浪费空间的缺点，对空间利用率是很高了，但是由于各个结点的链表是不相同的结构，加上要维护结点的度的数值，在运算上就会带来时间上的损耗

既可以减少空指针的浪费，又能使结点结构相同的改进方法就是孩子表示法

把每个结点的孩子结点排列起来，以单链表作为存储结构，则n个结点有n个孩子链表，如果是叶子结点，则此单链表为空，然后n个头指针又组成一个线性表，采用顺序存储结构，存放进一个一维数组中

##### 两种结点结构
###### 一个是孩子链表的孩子结点
- 数据域child，用来存储某个结点在表头数组中的下标
- 指针域next，用来存储指向某结点的下一个孩子结点的指针
###### 另一个是表头数组的表头结点
- 数据域data，存储某结点的数据信息
- 头指针域firstchild，存储该结点的孩子链表的头指针

孩子表示法对于我们要查找某个结点的某个孩子，或者找某个结点的兄弟，只需要查找这个结点的孩子单链表即可。对于遍历整棵树也是很方便的，对头结点的数组循环即可

但是，这也存在着问题，我如何知道某个结点的双亲是谁呢？比较麻烦，需要整棵树遍历才行，可以把孩子表示法和双亲表示法结合，称为双亲孩子表示法，是孩子表示法的改进

#### 孩子兄弟表示法
任意一棵树，它的结点的第一个孩子如果存在就是唯一的，它的右兄弟如果存在也是唯一的。因此，可以设置两个指针，分别指向该结点的第一个孩子和此结点的右兄弟

##### 结点结构
- 数据域data
- 指针域firstchild，存储该结点的第一个孩子结点的存储地址
- 指针域rightsib，存储该结点的右兄弟的存储地址

这个表示法的最大好处就是把一棵复杂的树变成了一棵二叉树

### 二叉树
#### 定义
二叉树(Binary Tree)是n(n≥0）个结点的有限集合，该集合或者为空集(称为空二叉树)。或者由一个根结点和两棵互不相交的、分别称为根结点的左子树和右子树的二叉树组成
二叉树的定义是用递归的方式
#### 特点
- 每个结点最多有两棵子树，所以二叉树中不存在度大于2的结点。注意不是只有两棵子树，而是最多有。没有子树或者有一棵子树都是可以的
- 左子树和右子树是有顺序的，次序不能任意颠倒。就像人是双手、双脚，但显然左手、左脚和右手、右脚是不一样的
- 即使树中某结点只有一棵子树，也要区分它是左子树还是右子树。就像摔伤了手，伤的是左手还是右手，对你的生活影响度是完全不同的

#### 基本形态
二叉树有五种基本形态
##### 空二叉树
##### 只有一个根结点
##### 根结点只有左子树
##### 根结点只有右子树
##### 根结点既有左子树又有右子树

#### 特殊二叉树
##### 斜树
所有的结点都只有左子树的二叉树叫左斜树。所有结点都只有右子树的二叉树叫右斜树。这两者统称为斜树

斜树的特点是每一层都只有一个结点，结点个数与二叉树的深度相同

线性表结构可以理解为树的一种极其特殊的表现形式

##### 满二叉树
在一棵二叉树中，如果所有分支结点都存在左子树和右子树，并且所有叶子都在同一层上，这样的二叉树称为满二叉树

满二叉树的所有叶子都在同一层上，这就做到了整棵树的平衡

满二叉树的特点：
1. 叶子只能出现在最下一层。出现在其他层就不可能达成平衡
2. 非叶子结点的度一定是2。否则就是“缺胳膊少腿”了
3. 在同样深度的二叉树中，满二叉树的结点个数最多，叶子数最多

##### 完全二叉树
对一棵具有n个结点的二叉树按层序编号，如果编号为i (1 < i < n) 的结点与同样深度的满二叉树中编号为i的结点在二叉树中位置完全相同，则这棵二叉树称为完全二叉树

满二叉树一定是完全二叉树，但是完全二叉树不一定是满二叉树

完全二叉树的特点：
1. 叶子结点只能出现在最下两层
2. 最下层的叶子一定集中在左部连续位置
3. 倒数第二层，若有叶子结点，一定都在右部连续位置
4. 如果结点度为1，则该结点只有左孩子，即不存在只有右子树的情况
5. 同样结点数的二叉树,完全二叉树的深度最小


判断二叉树是否是完全二叉树的方法：
按满二叉树的结构逐层顺序编号，如果编号出现空档，说明不是完全二叉树，否则就是

### 二叉树的性质
#### 性质1
在二叉树的第i层上，最多有2的i-1次方（2^(i-1)）个结点（i≥1）

#### 性质2
深度为k的二叉树最多有2的k次方减1（2^k-1）个结点（k≥1）

#### 性质3
对任何一棵二叉树T，如果其叶子结点数为n0，度为2的结点数为n2，则n0=n2+1

#### 性质4
具有n个结点的完全二叉树的深度为

$\left\lfloor \log_2 n \right\rfloor + 1$

其中， $\left\lfloor x \right\rfloor$表示不大于 $x$ 的最大整数

#### 性质5
如果对一棵有n个结点的完全二叉树（其深度为$\left\lfloor \log_2 n \right\rfloor + 1$）的结点按层序编号（从第层到第$\left\lfloor \log_2 n \right\rfloor + 1$层，每层从左到右），对任一结点i（1≤i≤n）有：
- 如果i=1，则结点i是二叉树的根，无双亲;如果i>1，则其双亲是结点|_i/2_|
- 如果2i>n，则结点i无左孩子（结点i为叶子结点)；否则其左孩子是结点2i
- 如果2i+1>n，则结点i无右孩子；否则其右孩子是结点2i+1

### 二叉树的存储结构
#### 二叉树的顺序存储结构
顺序存储结构一般只用于完全二叉树

#### 二叉链表
二叉树每个结点最多有两个孩子，所以为它设计一个数据域和两个指针域，这样的链表叫二叉链表

数据域data
- 指针域lchild，存放指向左孩子的指针
- 指针域rchild，存放指向右孩子的指针

如果有需要，还可以增加一个指向其双亲的指针域，称为三叉链表

### 遍历二叉树
#### 二叉树遍历原理
二叉树的遍历次序非常重要

二叉树的遍历(traversing binary tree)是指从根结点出发，按照某种次序依次访问二叉树中所有结点，使得每个结点被访问一次且仅被访问一次

两个关键字：访问和次序
访问就是输出结点的数据信息

#### 二叉树的遍历方法
##### 前序/先序遍历
规则：如果二叉树为空，则空操作返回，否则先访问根结点，然后前序遍历左子树，再前序遍历右子树
遍历顺序为根-左-右

##### 中序遍历
规则：如果二叉树为空，则空操作返回，否则从根结点开始（注意不是先访问根结点），中序遍历根结点的左子树，然后是访问根结点，最后中序遍历右子树
遍历顺序为左-根-右

##### 后序遍历
规则：如果二叉树为空，则空操作返回，否则从左到右先叶子后结点的方法遍历访问左右子树，最后访问根结点
遍历顺序为左-右-根

##### 层序遍历
规则：如果二叉树为空，则空操作返回，否则从树的第一层，也就是根结点开始访问，从上而下逐层遍历，在同一层中，按从左到右的顺序对结点逐个访问

用图形的方式来表现树的结构，应该说是非常直观和容易理解，但是对于计算机来说,它只有循环、判断等方式来处理，也就是说,它只会处理线性序列，而四种遍历方法,其实都是在把树中的结点变成某种意义的线性序列，这就给程序的实现带来了好处
另外不同的遍历提供了对结点依次处理的不同方式，可以在遍历过程中对结点进行各种处理

**已知前序遍历序列和中序遍历序列，可以唯一确定一棵二叉树**
**已知后序遍历序列和中序遍历序列，可以唯一确定一棵二叉树**
**但是已知前序和后序遍历，是不能确定一棵二叉树的**

### 二叉树的建立
如果我们要在内存中建立一个树，为了能让每个结点确认是否有左右孩子，我们对它进行了扩展，也就是将二叉树中每个结点的空指针引出一个虚结点，其值为一特定值，比如“#”。我们称这种处理后的二叉树为原二叉树的扩展二叉树。扩展二叉树就可以做到一个遍历序列确定一棵二叉树了

### 线索二叉树
#### 线索二叉树原理
##### 二叉链表存在的问题
1. 对于一个有n个结点的二叉链表，每个结点有指向左右孩子的两个指针域，所以一共是2n个指针域。而n个结点的二叉树一共有n-1条分支线数，也就是说，其实是存在2n- (n-1)=n+1个空指针域。这些空间不存储任何事物，白白浪费着内存的资源
2. 在二叉链表上，我们只能知道每个结点指向其左右孩子结点的地址，而不知道某个结点的前驱和后继是谁。每次需要知道的时候，都必须先遍历一次

因此可以考虑利用空地址，存放指向结点在某种遍历次序下的前驱和后继结点的地址

#### 线索二叉树定义
指向前驱和后继的指针称为线索，加上线索的二叉链表称为线索链表，相应的二叉树称为线索二叉树（Threaded Binary Tree）

线索二叉树，等于是把一棵二叉树转变成了一个双向链表，这样对我们的插入删除结点、查找某个结点都带来了方便。所以我们对二叉树以某种次序遍历使其变为线索二叉树的过程称作是线索化

#### 线索二叉树的改进
在当前的线索二叉树中，无法知道某一结点的lchild是指向它的左孩子还是指向前驱，rchild是指向右孩子还是指向后继

因此，在每个结点再增设两个标志域ltag和rtag，ltag和rtag只是存放0或1数字的布尔型变量，其占用的内存空间要小于像lchild和rchild的指针变量

#### 线索二叉树的结点结构
结点结构为
```lchild--ltag--data--rtag--rchild```

其中：
- ltag为0时，lchild指向该结点的左孩子；ltag为1时，lchild指向该结点的前驱
- rtag为0时，rchild指向该结点的右孩子，rtag为1时，rchild指向该结点的后继

#### 线索化的实质
线索化的实质就是将二叉链表中的空指针改为指向前驱或后继的线索。由于前驱和后继的信息只有在遍历该二叉树时才能得到，所以线索化的过程就是在遍历的过程中修改空指针的过程

遍历线索二叉树，其实就等于是操作一个双向链表结构

- 和双向链表结构一样，在二叉树线索链表上添加一个头结点，并令其lchild域的指针指向二叉树的根结点，其rchild域的指针指向中序遍历时访问的最后一个结点
- 反之，令二叉树的中序序列中的第一个结点中，lchild域指针和最后一个结点的rchild域指针均指向头结点
- 这样定义的好处就是我们既可以从第一个结点起顺后继进行遍历，也可以从最后一个结点起顺前驱进行遍历

由于它充分利用了空指针域的空间(这等于节省了空间)，又保证了创建时的一次遍历就可以终生受用前驱后继的信息(这意味着节省了时间)。所以在实际问题中，如果所用的二叉树需经常遍历或查找结点时需要某种遍历序列中的前驱和后继，那么采用线索二叉链表的存储结构就是非常不错的选择

### 树、森林与二叉树的转换
#### 树转换为二叉树
转换步骤：
- 第1步：加线。在所有兄弟结点之间加一条连线
- 第2步：去线。对树中每个结点，只保留它与第一个孩子结点的连线，删除它与其他孩子结点之间的连线
- 第3步：层次调整。以树的根结点为轴心，将整棵树顺时针旋转一定的角度使之结构层次分明。注意第一个孩子是二叉树结点的左孩子，兄弟转换过来的孩子是结点的右孩子


#### 森林转换为二叉树
森林是由若干棵树组成的，所以完全可以理解为，森林中的每一棵树都是兄弟，可以按照兄弟的处理办法来操作

步骤如下:
- 第1步：把每个树转换为二叉树
- 第2步：第一棵二叉树不动，从第二棵二叉树开始，依次把后一棵二叉树的根结点作为前一棵二叉树的根结点的右孩子、用线连接起来。当所有的二叉树连接起来后就得到了由森林转换来的二叉树

#### 二叉树转换为树
二叉树转换为树，是树转换为二叉树的逆过程

步骤如下：
- 第1步：加线。若某结点的左孩子结点存在，则将这个左孩子的右孩子结点、右孩子的右孩子结点、右孩子的右孩子的右孩子结点……即左孩子的n个右孩子结点都作为此结点的孩子。将该结点与这些右孩子结点用线连接起来
- 第2步：去线。删除原二叉树中所有结点与其右孩子结点的连线
- 第3步：层次调整。使之结构层次分明

#### 二叉树转换为森林
判断一棵二叉树能够转换成一棵树还是森林，标准很简单，那就是只要看这棵二叉树的根结点有没有右孩子，有就是森林，没有就是一棵树

那么如果是转换成森林，步骤如下:
- 第1步：从根结点开始，若右孩子存在，则把与右孩子结点的连线删除，再查看分离后的二叉树，若右孩子存在，则连线删除，……，直到所有右孩子连线都删除为止，得到分离的二叉树
- 第2步：再将每棵分离后的二叉树转换为树即可

#### 树与森林的遍历
树的遍历分为两种方式：
- 一种是先根遍历树，即先访问树的根结点，然后依次先根遍历根的每棵子树
- 另一种是后根遍历树，即先依次后根遍历每棵子树，然后再访问根结点

森林的遍历也分为两种方式：
##### 前序遍历
先访问森林中第一棵树的根结点，然后再依次先根遍历根的每棵子树，再依次用同样方式遍历除去第一棵树的剩余树构成的森林
##### 后序遍历
先访问森林中第一棵树，后根遍历的方式遍历每棵子树，然后再访问根结点，再依次同样方式遍历除去第一棵树的剩余树构成的森林

森林的前序遍历和二叉树的前序遍历结果相同，森林的后序遍历和二叉树的中序遍历结果相同

当以二叉链表作树的存储结构时，树的先根遍历和后根遍历完全可以借用二叉树的前序遍历和中序遍历的算法来实现。这其实也就证实，我们找到了对树和森林这种复杂问题的简单解决办法

### 赫夫曼树及其应用
#### 赫夫曼树
赫夫曼编码是最基本的压缩编码方式
赫夫曼编码用到了特殊的二叉树，称为赫夫曼树
#### 赫夫曼树的定义与原理
从树中一个结点到另一个结点之间的分支构成两个结点之间的路径，路径上的分支数目称做路径长度树的路径长度就是从树根到每一结点的路径长度之和

如果考虑到带权的结点，结点的带权的路径长度为从该结点到树根之间的路径长度与结点上权的乘积。树的带权路径长度为树中所有叶子结点的带权路径长度之和。假设有n个权值{Wi,W2…,Wn}，构造一棵有n个叶子结点的二叉树，每个叶子结点带权Wk，每个叶子的路径长度为lk，我们通常记作，则其中带权路径长度WPL最小的二叉树称做赫夫曼树。也称为最优二叉树

##### 赫夫曼树的赫夫曼算法描述
- 第1步：根据给定的n个权值{W1,W2…,Wn}构成n棵二叉树的集合F={T1,T2…,Tn }，其中每棵二叉树Ti中只有一个带权为Wi根结点，其左右子树均为空
- 第2步：在F中选取两棵根结点的权值最小的树作为左右子树构造一棵新的二叉树，且置新的二叉树的根结点的权值为其左右子树上根结点的权值之和
- 第3步：在F中删除这两棵树，同时将新得到的二叉树加入F中
- 第4步：重复第2步和第3步，直到F只含一棵树为止。这棵树便是赫夫曼树

#### 赫夫曼编码
如果要设计长短不等的编码，则必须是任一字符的编码都不是另一个字符的编码的前缀，这种编码称作前缀编码

一般地，设需要编码的字符集为{d1,d2…dn},各个字符在电文中出现的次数或频率集合为{W1,W2,…,Wn}，以d1,dz…,dn作为叶子结点，以W1,W2…,Wn作为相应叶子结点的权值来构造一棵赫夫曼树。规定赫夫曼树的左分支代表0,右分支代表1,则从根结点到叶子结点所经过的路径分支组成的0和1的序列便为该结点对应字符的编码，这就是赫夫曼编码

## 图
### 图的定义
图(Graph)是由顶点的有穷非空集合和顶点之间边的集合组成,通常表示为：G(V,E),其中,G表示一个图，V是图G中顶点的集合，E是图G中边的集合
#### 图数据结构的注意事项
- 线性表中我们把数据元素叫元素，树中将数据元素叫结点，在图中数据元素，我们则称之为顶点（Vertex）
- 线性表中可以没有数据元素，称为空表。树中可以没有结点，叫做空树。但是在图结构中,不允许没有顶点。在定义中，若V是顶点的集合，则强调了顶点集合V有穷非空
- 线性表中，相邻的数据元素之间具有线性关系；树结构中，相邻两层的结点具有层次关系；而图中，任意两个顶点之间都可能有关系，顶点之间的逻辑关系用边来表示，边集可以是空的

#### 各种图的定义
##### 无向边
若顶点v到v之间的边没有方向，则称这条边为无向边（Edge），用无序偶对（v）来表示
##### 无向图
如果图中任意两个顶点之间的边都是无向边，则称该图为无向图(Undirected graphs)
##### 有向边
若从顶点Vi到Vj的边有方向，则称这条边为有向边，也称为弧（Arc)，用有序偶<Vi,Vj>来表示，Vi称为弧尾（Tail），Vj称为弧头（Head）
##### 有向图
如果图中任意两个顶点之间的边都是有向边，则称该图为有向图（Directed Graphs）

**特别注意：无向边用小括号 ```()``` 表示，有向边用尖括号 ```<>``` 表示**

##### 简单图
在图中，若不存在顶点到其自身的边，且同一条边不重复出现，则称这样的图为简单图

##### 无向完全图
在无向图中，如果任意两个顶点之间都存在边，则称该图为无向完全图。含有n个顶点的无向完全图有n×(n-1)/2条边

##### 有向完全图
在有向图中，如果任意两个顶点之间都存在方向互为相反的两条弧，则称该图为有向完全图，含有n个顶点的有向完全图有n×(n-1)条边

即对于具有n个顶点和e条边数的图，无向图0≤e≤n (n-1)/2，有向图0≤e≤n (n-1)

- 有很少条边或弧的图称为稀疏图，反之称为稠密图。这里的稀疏和稠密是模糊的概念，都是相对而言的

- 有些图的边或弧具有与它相关的数字，这种与图的边或弧相关的数叫做权(Weight)。这些权可以表示从一个顶点到另一个顶点的距离或耗费。这种带权的图通常称为网(Network)

- 假设有两个图G=(V,{E}）和G'=(V',{E'})，如果V'∈V且E'∈E，则称G'为G的子图（Subgraph）

#### 图的顶点与边间关系
- 对于无向图 G=(V,{E})，如果边（v,v')∈E，则称顶点v和v'互为邻接点(Adjacent)，即v和v'相邻接。边(v,v')依附(incident)于顶点v和v',或者说(v,v')与顶点v和v'相关联。顶点v的度（Degree）是和v相关联的边的数目，记为TD(v)

边数其实就是各顶点度数和的一半

-  对于有向图G=(V,{E}),如果弧 ```<v,v'>∈E``` ,则称顶点v邻接到顶点v'，顶点v'邻接自顶点v。弧 ```<v,v'>``` 和顶点 ```v,v'``` 相关联
- 以顶点v为头的弧的数目称为v的入度(InDegree)，记为ID(v);以v为尾的弧的数目称为v的出度(OutDegree)，记为OD(v);顶点v的度为 TD(v)=ID(v)+OD(v)


无向图G=(V,{E})中从顶点v到顶点v'的路径（Path）是一个顶点序列(v=Vi,0,Vi,1;…Vi,m=V')，其中(Vi,j-1,Vi,j)∈E,1≤j≤m

*树中根结点到任意结点的路径是唯一的，但图中顶点与顶点之间的路径却是不唯一的*

路径长度是路径上的边或弧的数目

第一个顶点到最后一个顶点相同的路径称为回路或环（Cycle）

序列中顶点不重复出现的路径称为简单路径。除了第一个顶点和最后一个顶点之外，其余顶点不重复出现的回路，称为简单回路或简单环


#### 连通图
- 在无向图G中，如果从顶点v到顶点v有路径，则称v和v是连通的。如果对于图中任意两个顶点vi、vEE,v和v都是连通的，则称G是连通图（ConnectedGraph)
- 无向图中的极大连通子图称为连通分量。注意连通分量的概念，它强调：
1. 要是子图
2. 子图要是连通的
3. 连通子图含有极大顶点数
4. 具有极大顶点数的连通子图包含依附于这些顶点的所有边

在有向图G中，如果对于每一对v、v∈V、V≠vj，从Vi到Vj和从Vj到Vi都存在路径，则称G是强连通图。有向图中的极大强连通子图称做有向图的强连通分量

所谓的一个连通图的生成树是一个极小的连通子图，它含有图中全部的 n个顶点，但只有足以构成一棵树的n-1条边

如果一个有向图恰有一个顶点的入度为0，其余顶点的入度均为1，则是一棵有向树

一个有向图的生成森林由若干棵有向树组成，含有图中全部顶点，但只有足以构成若干棵不相交的有向树的弧

### 图的存储结构
图不可能用简单的顺序存储结构来表示，而多重链表尽管可以实现图结构，但是是有问题的

图有5种不同的存储结构
#### 邻接矩阵
##### 思路
图是由顶点和边或弧两部分组成，可以考虑分两个结构来分别存储

顶点不分大小、主次，所以用一维数组来存储

边或弧由于是顶点与顶点之间的关系，一维搞不定，就用一个二维数组来存储
##### 定义
图的邻接矩阵（Adjacency Matrix）存储方式是用两个数组来表示图。即一个一维数组存储图中顶点信息，一个二维数组（称为邻接矩阵）存储图中的边或弧的信息

无向图的边数组是一个对称矩阵

有了这个矩阵，我们就可以很容易地知道图中的信息：
1．我们要判定任意两顶点是否有边无边就非常容易了
2．我们要知道某个顶点的度，其实就是这个顶点Vi在邻接矩阵中第i行(或第i列)的元素之和。比如顶点V1的度就是1+0+1+0=2
3．求顶点Vi的所有邻接点就是将矩阵中第i行元素扫描一遍，```arc[i][j]``` 为1就是邻接点


n个顶点和e条边的无向网图创建，时间复杂度为O(n+n^2+e)，其中对邻接矩阵的初始化耗费了O(n^2)的时间

#### 邻接表
邻接矩阵对于边数相对较少的图，存在对存储空间的极大浪费。因此考虑对边或弧使用链式存储的方式来避免空间浪费的问题

数组和链表相结合的存储方式称为邻接表（Adjacency List）

##### 邻接表的处理办法
1. 图中顶点用一个一维数组存储，因为数组可以较容易地读取顶点信息，更加方便。另外，对于顶点数组中，每个数据元素还需要存储指向第一个邻接点的指针，以便于查找该顶点的边信息
2. 图中每个顶点v的所有邻接点构成一个线性表，由于邻接点的个数不定，所以用单链表存储，无向图称为顶点v的边表，有向图则称为顶点v作为弧尾的出边表

#### 十字链表
对于有向图来说，邻接表是有缺陷的。关心了出度问题，但是想了解入度必须遍历整个图才能知道。反之，逆邻接表解决了入度却不了解出度的情况

十字链表（Orthogonal List）可以把邻接表和逆邻接表结合起来

##### 十字链表结点结构
```data--firstin--firstout```
firstin表示入边表头指针，指向该顶点的入边表中第一个结点
firstout表示出边表头指针，指向该顶点的出边表中的第一个结点

重新定义的边表结点结构如下：
```tailvex--headvex--headlink--taillink```
tailvex是指弧起点在顶点表的下标
headvex是指弧终点在顶点表的下标

#### 邻接多重表
对于无向图的邻接表，对已访问过的边做标记、删除某一条边等这种需要找到这条边表结点进行的操作，是比较麻烦的

重新定义的边表结点结构如下：
```ivex--ilink--jvex--jlink```
ivex和jvex是与某条边依附的的两个顶点在顶点表中下标
ilink指向依附项点ivex的下一条边
jlink指向依附顶点jvex的下一条边
这就是邻接多重表结构

#### 边集数组
边集数组是由两个一维数组构成。一个是存储顶点的信息，另一个是存储边的信息，这个边数组每个数据元素由一条边的起点下标（begin）、终点下标（end）和权（weight）组成
定义的边数组结构如下：
```begin--end--weight```
begin是存储起点下标
end是存储终点下标
weight是存储权值


### 图的遍历
图的遍历与树的遍历类似。我们希望从图中某一顶点出发访遍图中其余顶点，且使每一个顶点仅被访问一次，这个过程就叫做图的遍历（Traversing Graph）
图的遍历有两种方案：深度优先遍历和广度优先遍历
#### 深度优先遍历（Depth First Search）
也叫深度优先搜索，简称DFS
深度优先遍历其实就是一个递归的过程，类似树的前序遍历
##### 对于连通图
从图中某个顶点v出发，访问此顶点，然后从v的未被访问的邻接点出发，深度优先遍历图，直至图中所有和v有路径相通的顶点都被访问到
##### 对于非连通图
对于非连通图，只需要对它的连通分量分别进行深度优先遍历，即在先前一个顶点进行一次深度优先遍历后,若图中尚有顶点未被访问，则另选图中一个未曾被访问的顶点作起始点，重复上述过程，直至图中所有顶点都被访问到为止

#### 广度优先遍历（Breadth First Search）
又称为广度优先搜索，简称BFS
广度优先遍历类似于树的层序遍历

#### 两种遍历方法对比
它们在时间复杂度上是一样的，不同之处仅在于对顶点的访问顺序不同。因此两者在全图遍历上没有优劣之分，只是视不同的情况选择不同的算法

深度优先更适合目标比较明确，以找到目标为主要目的的情况，而广度优先更适合在不断扩大便利范围时找到相对最优解的情况


### 最小生成树
最小成本，就是n个顶点，用n-1条边把一个连通图连接起来，并使得权值之和最小

一个连通图的生成树是一个极小的连通子图，它含有图中全部的顶点，但只有足以构成一棵树的n-1条边。我们把构造连通网的最小代价生成树称为最小生成树(Minimum Cost Spanning Tree)

找连通网的最小生成树，经典的有两种算法，普里姆算法和克鲁斯卡尔算法

#### 普里姆（Prim）算法
假设N=(PB)是连通网,TE是N上最小生成树中边的集合。算法从```U={u0}(u0∈V),TE={}``` 开始。重复执行下述操作:在所有```u∈U，v∈V-U的边(u,v)∈E``` 中找一条代价最小的边（u0,v0）并入集合TE,同时v0并入U，直至U=V为止。此时TE中必有n-1条边，则 ```T=(V{TE})``` 为N的最小生成树
由算法代码中的循环嵌套可得知此算法的时间复杂度为0(n^2)

#### 克鲁斯卡尔（Kruskal）算法
假设 ```N=(V,{E})``` 是连通网，则令最小生成树的初始状态为只有n个顶点而无边的非连通图 ```T={V,{}}``` ，图中每个顶点自成一个连通分量。在E中选择代价最小的边，若该边依附的顶点落在T中不同的连通分量上，则将此边加入到T中，否则舍去此边而选择下一条代价最小的边。依次类推，直至T中所有顶点都在同一连通分量上为止

此算法的Find函数由边数e决定，时间复杂度为O(loge)，而外面有一个for循环e次。所以克鲁斯卡尔算法的时间复杂度为O(eloge)

#### 两种算法对比
克鲁斯卡尔算法主要是针对边来展开，变数少时效率会非常高，所以对于稀疏图有很大优势
普里姆算法对于稠密图，即边数非常多的情况会更好一些

### 最短路径
- 在网图和非网图中，最短路径的含义是不同的
- 由于非网图它没有边上的权值，所谓的最短路径，其实就是指两顶点之间经过的边数最少的路径
- 而对于网图来说，最短路径，是指两顶点之间经过的边上权值之和最少的路径，并且我们称路径上的第一个顶点是源点，最后一个顶点是终点
- 显然，我们研究网图更有实际意义，就地图来说，距离就是两顶点间的权值之和。而非网图完全可以理解为所有的边的权值都为1的网

#### 迪杰斯特拉（Dijkstra）算法
这是一个按路径长度递增的次序产生最短路径的算法

通过迪杰斯特拉（Dijkstra）算法解决了从某个源点到其余各顶点的最短路径问题。从循环嵌套可以很容易得到此算法的时间复杂度为O(n^2)

#### 弗洛伊德（Floyd）算法
弗洛伊德（Flbyd）算法的代码简洁到就是一个二重循环初始化加一个三重循环权值修正，就完成了所有顶点到所有顶点的最短路径计算。几乎就如同是我们在学习C语言循环嵌套的样例代码而已。这是非常漂亮的算法。很可惜由于它的三重循环，因此也是O(n^3)时间复杂度。如果面临需要求所有顶点至所有顶点的最短路径问题时，弗洛伊德(Flbyd)算法应该是不错的选择

另外，求最短路径的两个算法对有向图依然有效，因为二者的差异仅仅是邻接矩阵是否对称而已

### 拓扑排序
在一个表示工程的有向图中，用顶点表示活动，用弧表示活动之间的优先关系，这样的有向图为顶点表示活动的网，我们称为AOV网(Activity On VertexNetwork）。AOV 网中的弧表示活动之间存在的某种制约关系

设G=(V,E)是一个具有n个顶点的有向图，V中的顶点序列 V1，V2，…,Vn,满足若从顶点Vi到Vj有一条路径，则在顶点序列中顶点Vi必在顶点Vj之前。则我们称这样的顶点序列为一个拓扑序列

所谓拓扑排序，其实就是对一个有向图构造拓扑序列的过程。构造时会有两个结果，如果此网的全部顶点都被输出，则说明它是不存在环(回路)的AOV网；如果输出顶点数少了，哪怕是少了一个，也说明这个网存在环(回路)，不是AOV网
#### 拓扑排序算法
对AOV网进行拓扑排序的基本思路是：从AOV网中选择一个入度为0的顶点输出，然后删去此顶点，并删除以此顶点为尾的弧，继续重复此步骤，直到输出全部顶点或者AOV网中不存在入度为0的顶点为止

顶点表结点结构
```in--data--firstedge```
入度域in

### 关键路径
拓扑排序主要是为了解决一个工程能否顺序进行的问题。而关键路径解决工程完成需要的最短时间问题

对一个流程图获得最短时间，必须要分析它们的拓扑关系，并找到当中最关键的流程，这个流程的时间就是最短时间
在一个表示工程的带权有向图中，用顶点表示事件，用有向边表示活动，用边上的权值表示活动的持续时间，这种有向图的边表示活动的网，我们称之为 AOE 网（Activity On Edge Network)。AOE网中没有入边的顶点称为始点或源点，没有出边的顶点称为终点或汇点。由于一个工程，总有一个开始，一个结束，所以正常情况下，AOE网只有一个源点一个汇点

路径上各个活动所持续的时间之和称为路径长度，从源点到汇点具有最大长度的路径叫关键路径，在关键路径上的活动叫关键活动

#### 关键路径算法原理
我们只需要找到所有活动的最早开始时间和最晚开始时间，并且比较它们,如果相等就意味着此活动是关键活动，活动间的路径为关键路径。如果不等，则不是

为此,我们需要定义如下几个参数：
##### 事件的最早发生时间etv
(earliest time of vertex)
即顶点V的最早发生时
##### 事件的最晚发生时间ltv
(latest time of vertex)
即顶点Vk的最晚发生时间，也就是每个顶点对应的事件最晚需要开始的时间，超出此时间将会延误整个工期
##### 活动的最早开工时间ete
(earliest time of edge)
即弧ak的最早发生时间
##### 活动的最晚开工时间lte
(atest time of edge)
即弧ak的最晚发生时间，也就是不推迟工期的最晚开工时间
我们是由1和2可以求得3和4,然后再根据ete[k]是否与lte[k]相等来判断ax是否是关键活动

#### 关键路径算法



## 查找
查找（Searching）就是根据给定的某个值，在查找表中确定一个其关键字等于给定值的数据元素（或记录）
### 查找概念
#### 查找相关术语
##### 查找表（Search Table）
是由同一类型的数据元素（或记录）构成的集合
##### 关键字（Key）
是数据元素中某个数据项的值，又称为键值，用它可以标识一个数据元素。也可以标识一个记录的某个数据项（字段），称为关键码
##### 主关键字（Primary Key）
如果关键字可以唯一地标识一个记录，则称此关键字为主关键字
##### 次关键字（Secondary Key）
对于可以识别多个数据元素（或记录）的关键字，称为次关键字。次关键字也可以理解为是不以唯一标识一个数据元素（或记录）的关键字，它对应的数据项就是次关键码
#### 查找结果
##### 查找成功
若表中存在等于给定值的记录，则称查找是成功的，此时查找的结果给出整个记录的信息，或指示该记录在查找表中的位置
##### 查找不成功
若表中不存在关键字等于给定值的记录，则称查找不成功，此时查找的结果可给出一个“空”记录或“空”指针
#### 查找操作方式
##### 静态查找表（Static Search Table）
只做查找操作的查找表
它的主要操作有：
1. 查询某个“特定的”数据元素是否在查找表中
2. 检索某个“特定的”数据元素和各种属性
##### 动态查找表（Dynamic Search Table）
在查找过程中同时插入查找表中不存在的数据元素，或从查找表中删除已经存在的某个数据元素
它的操作时：
1. 查找时插入数据元素
2. 查找时删除数据元素

#### 为查找设置数据结构
为了提高查找的效率，我们需要专门为查找操作设置数据结构，这种面向查找操作的数据结构称为查找结构
从逻辑上来说，查找所基于的数据结构是集合，集合中的记录之间没有本质关系。可是要想获得较高的查找性能，我们就不能不改变数据元素之间的关系，在存储时可以将查找集合组织成表、树等结构
- 对于静态查找，可以应用线性表结构组织数据
- 对于动态查找，可以考虑二叉排序树的查找技术
- 还可以用散列表结构来解决一些查找问题

### 顺序表查找
顺序查找(Sequential Search)又叫线性查找，是最基本的查找技术

它的查找过程是：从表中第一个(或最后一个）记录开始，逐个进行记录的关键字和给定值比较，若某个记录的关键字和给定值相等，则查找成功，找到所查的记录；如果直到最后一个(或第一个)记录，其关键字和给定值比较都不等时，则表中没有所查的记录，查找不成功
#### 顺序表查找优化
基本的顺序查找不够完美，每次循环都需要判断i是否越界，可以设置一个哨兵，可以不需要每次让i与n作比较

顺序表查找法时间复杂度是O(n),算法简单，对静态查找表的记录没有任何要求，但是n很大时，查找效率极为低下

### 有序表查找
#### 折半查找
折半查找(Binary Search)技术，又称为二分查找。它的前提是线性表中的记录必须是关键码有序(通常从小到大有序)，线性表必须采用顺序存储

折半查找的基本思想是：在有序表中，取中间记录作为比较对象，若给定值与中间记录的关键字相等，则查找成功；若给定值小于中间记录的关键字，则在中间记录的左半区继续查找:若给定值大于中间记录的关键字，则在中间记录的右半区继续查找。不断重复上述过程，直到查找成功，或所有查找区域无记录，查找失败为止

折半查找算法的时间复杂度为O(logn)，显然远远好于顺序查找的O(n)时间复杂度

不过由于折半查找的前提条件是需要有序表顺序存储,对于静态查找表，一次排序后不再变化，这样的算法已经比较好了。但对于需要频繁执行插入或删除操作的数据集来说，维护有序的排序会带来不小的工作量,那就不建议使用

#### 插值查找
插值查找(Interpolation Search)是根据要查找的关键字key与查找表中最大最小记录的关键字比较后的查找方法，其核心就在于插值的计算公式:

$$
\frac{a[\text{low}]}{a[\text{high}] - a[\text{low}]}
$$



从时间复杂度来看,它也是O(logn)，但对于表长较大，而关键字分布又比较均匀的查找表来说，插值查找算法的平均性能比折半查找要好得多。反之，数组中如果分布类似 ```{0,1,2,2000,2001,………,999998, 999999}``` 这种极端不均匀的数据，用插值查找未必是很合适的选择

#### 斐波那契查找
斐波那契查找（Fibonacci Search）是一种有序查找，利用了黄金分割原理来实现

斐波那契查找算法的核心在于：
1. 当 ```key=a[mid]``` 时，查找就成功
2. 当 ```key<a[mid]``` 时，新范围是第low个到第mid-1个，此时范围个数为 ```F[k-1]-1``` 个
3. 当 ```key>a[mid]``` 时，新范围是第m+1个到第high个，此时范围个数为 ```F[k-2]-1``` 个

斐波那契查找的时间复杂度也为O(logn)，就平均性能来说，斐波那契查找要优于折半查找。但是最坏情况，比如key=1，那么始终都处于左侧长半区在查找，则查找效率低于折半查找

折半查找是进行加法与除法运算

$$
\text{mid} = \frac{a[\text{low}] + a[\text{high}]}{2}
$$


插值查找进行复杂的四则运算

$$
\text{mid} = \text{low} + \left( \text{high} - \text{low} \right) \times \frac{\text{key} - a[\text{low}]}{a[\text{high}] - a[\text{low}]}
$$


斐波那契查找只是最简单加减法运算

$$
\text{mid} = \text{low} + F[k-1] - 1
$$


在海量数据的查找过程中,这种细微的差别可能会影响最终的查找效率

应该说，三种有序表的查找本质上是分隔点的选择不同，各有优劣，实际开发时可根据数据的特点综合考虑再做出选择

### 线性索引查找
前面几种比较高效的查找方法都是基于有序的基础之上的。但事实上，很多数据集可能增长非常快，例如，某些微博网站或大型论坛的帖子和回复总数每天都是成百万上千万条，或者一些服务器的日志信息记录也可能是海量数据，要保证记录全部是按照当中的某个关键字有序，其时间代价是非常高昂的，所以这种数据通常都是按先后顺序存储

对于这样的查找表，能够快速查找到需要的数据的办法就是索引

数据结构的最终目的是提高数据的处理速度，索引是为了加快查找速度而设计的一种数据结构。索引就是把一个关键字与它对应的记录相关联的过程，一个索引由若干个索引项构成，每个索引项至少应包含关键字和其对应的记录在存储器中的位置等信息。索引技术是组织大型数据库以及磁盘文件的一种重要技术

索引按照结构可以分为线性索引、树形索引和多级索引。所谓线性索引就是将索引项集合组织为线性结构，也称为索引表。三种线性索引：稠密索引、分块索引和倒排索引
#### 稠密索引
稠密索引是指在线性索引中，将数据集中的每个记录对应一个索引项

稠密索引应对的可能是成千上万的数据，因此，对于稠密索引这个索引表来说，索引项一定是按照关键码有序的排列

索引项有序也就意味着，查找关键字时，可以用折半、插值、斐波那契等有序查找算法，大大提高了效率

如果数据集非常大，比如上亿，那也就意味着索引也得同样的数据集长度规模，对于内存有限的计算机来说，可能就需要反复去访问磁盘，查找性能反而大大下降了

#### 分块索引
稠密索引因为索引项与数据集的记录个数相同，所以空间代价很大。为了减少索引项的个数，我们可以对数据集进行分块，使其分块有序，然后再对每一块建立一个索引项，从而减少索引项的个数

分块有序，是把数据集的记录分成了若干块，并且这些块需要满足两个条件：
##### 块内无序
即每一块内的记录不要求有序。当然，如果能够让块内有序对查找来说更理想，不过这就要付出大量时间和空间的代价，因此通常不要求块内有序
##### 块间有序
例如，要求第二块所有记录的关键字均要大于第一块中所有记录的关键字，第三块的所有记录的关键字均要大于第二块的所有记录关键字……因为只有块间有序,才有可能在查找时带来效率

对于分块有序的数据集，将每块对应一个索引项，这种索引方法叫做分块索引

分块索引的索引项结构分三个数据项：
##### 最大关键码
它存储每一块中的最大关键字，这样的好处就是可以使得在它之后的下一块中的最小关键字也能比这一块最大的关键字要大

存储了块中的记录个数，以便于循环时使用

用于指向块首数据元素的指针，便于开始对这一块中记录进行遍历

在分块索引表中查找，就是分两步进行：
- 第1步：在分块索引表中查找要查关键字所在的块。由于分块索引表是块间有序的，因此很容易利用折半、插值等算法得到结果
- 第2步：根据块首指针找到相应的块，并在块中顺序查找关键码。因为块中可以是无序的，因此只能顺序查找

设n个记录的数据集被平均分成m块，每个块中有t条记录，Lb为查找索引表的平均查找长度。分块索引查找的平均查找长度为

$$
\text{ASLw} = Lb + Lw = \frac{1}{2} \times \left( \frac{n}{t} + t \right) + 1
$$


分块索引的效率比顺序查找的O(n)高了不少，但是与折半查找的O(logn)相比还有不少差距。因此在确定所在块的过程中，由于块间有序，所以可以应用折半、插值等手段来提高效率


#### 倒排索引
索引项的通用结构是:
##### 次关键码
##### 记录号表
其中记录号表存储具有相同次关键字的所有记录的记录号(可以是指向记录的指针或者是该记录的主关键字)。这样的索引方法就是倒排索引(inverted index)

倒排索引源于实际应用中需要根据属性(或字段、次关键码)的值来查找记录。这种索引表中的每一项都包括一个属性值和具有该属性值的各记录的地址。由于不是由记录来确定属性值,而是由属性值来确定记录的位置，因而称为倒排索引

倒排索引的优点显然就是查找记录非常快，基本等于生成索引表后,查找时都不用去读取记录，就可以得到结果。但它的缺点是这个记录号不定长，维护比较困难，插入和删除操作都需要做相应的处理


### 二叉排序树
二叉排序树（Binary Sort Tree)，又称为二叉查找树。它或者是一棵空树，或者是具有下列性质的二叉树
- 若它的左子树不空,则左子树上所有结点的值均小于它的根结构的值;
- 若它的右子树不空,则右子树上所有结点的值均大于它的根结点的值;
- 它的左、右子树也分别为二叉排序树

二叉排序树的前提是二叉树，它采用递归的定义方法，再者，它的结点间满足一定的次序关系，左子树结点一定比其双亲结点小，右子树结点一定比其双亲结点大

构造一棵二叉排序树的目的，其实并不是为了排序，而是为了提高查找和插入删除关键字的速度。不管怎么说，在一个有序数据集上的查找，速度总是要快于无序的数据集的，而二叉排序树这种非线性的结构，也有利于插入和删除的实现

#### 二叉排序树查找操作

#### 二叉排序树插入操作

#### 二叉排序树删除操作
二叉排序树的删除操作不是那么容易，因为删除结点后，这棵树依然要满足二叉排序树的特性。因此，在删除结点时要考虑多种情况
##### 查找并删除叶结点
很容易，因为删除叶结点对其他结点的结构没有影响
##### 要删除的结点只有左子树或右子树
相对较好解决。在结点删除后，要将它的左子树或右子树整个移动到删除结点的位置
##### 要删除的结点既有左子树又有右子树
比较好的方法是，找到需要删除的结点p的直接前驱（或直接后继）s，用s来替换结点p，然后再删除此结点s

#### 二叉排序树总结
- 二叉排序树是以链式方式存储，保持了链式存储结构在执行插入或删除操作时不用移动元素的优点，只要找到合适的插入和删除位置后，仅需修改链接指针即可。插入删除的时间性能比较好
而对于二叉排序树的查找，走的就是从根结点到要查找的结点的路径，其比较次数等于给定值的结点在二叉排序树的层数
- 极端情况，最少为1次，即根结点就是要找的结点，最多也不会超过树的深度。也就是说,二叉排序树的查找性能取决于二叉排序树的形状
- 但是问题就在于，二叉排序树的形状是不确定的

如果二叉排序树是比较平衡的，即其深度与完全二叉树相同，那么查找时间复杂度就是O(logn)，近似于折半查找。不平衡的最坏情况是斜树，查找时间复杂度为O(n)，等同于顺序查找

因此，如果希望对一个集合按二叉排序树查找，最好是把它构建成一棵平衡的二叉排序树


### 平衡二叉树（AVL树）
#### 平衡二叉树概念
平衡二叉树（Self-Balancing Binary Search Tree或Height-Balanced Binary Search Tree），是一种二叉排序树，其中每一个结点的左子树和右子树的高度差至多等于1

它是一种高度平衡的二叉排序树。要么它是一棵空树，要么它的左子树和右子树都是平衡二叉树，且左子树和右子树的深度之差的绝对值不超过1

二叉树上结点的左子树深度减去右子树深度的值称为平衡因子BF(Balance Factor)，平衡二叉树上所有结点的平衡因子只可能是-1、0和1。只要二叉树上有一个结点的平衡因子的绝对值大于1，则该二叉树就是不平衡的

**特别注意，平衡二叉树的前提是二叉排序树，它是有序的**

距离插入结点最近的，且平衡因子的绝对值大于1的结点为根的子树，我们称为最小不平衡子树

#### 平衡二叉树实现原理
平衡二叉树构建的基本思想就是在构建二叉排序树的过程中，每当插人一个结点时，先检查是否因插入而破坏了树的平衡性，若是，则找出最小不平衡子树。在保持二叉排序树特性的前提下，调整最小不平衡子树中各结点之间的链接关系，进行相应的旋转，使之成为新的平衡子树

所谓的平衡二叉树，其实就是在二叉排序树创建过程中保证它的平衡性，一旦发现有不平衡的情况，马上处理，这样就不会造成不可收拾的情况出现

当最小不平衡子树根结点的平衡因子BF是大于1时就右旋，小于-1时就左旋。插入结点后，最小不平衡子树的BF与它的子树的BF符号相反时，就需要对结点先进行一次旋转以使得符号相同后，再反向旋转一次才能够完成平衡操作


#### 平衡二叉树实现算法
平衡二叉树核心思想就是把不平衡消灭在最早时刻

如果我们需要查找的集合本身没有顺序，在频繁查找的同时也需要经常的插入和删除操作，显然我们需要构建一棵二叉排序树，但是不平衡的二叉排序树，查找效率是非常低的，因此我们需要在构建时，就让这棵二叉排序树是平衡二叉树，此时我们的查找时间复杂度就为O(logn)，而插入和删除也为O(logn)。这显然是比较理想的一种动态查找表算法

### 红黑树
红黑树与AVL树都是高效的二叉搜索树
#### 红黑树的概念与性质
##### 红黑树的定义
- 红黑树，是一种二叉搜索树，但在每个结点上增加一个存储位表示结点的颜色，可以是Red或Black
- 通过对任何一条从根到叶子的路径上各个结点着色方式的限制，红黑树确保没有一条路径会比其他路径长出俩倍，因而是接近平衡的

**AVL树是严格平衡的二叉搜索树，左右子树高度不超过1；红黑树是近似平衡，最长路径不超过最短路径的2倍**

##### 红黑树的性质：
- 每个结点不是红色就是黑色
- 根结点是黑色的
- 如果一个结点是红色的，则它的两个孩子结点是黑色的
- 对于每个结点，从该结点到其所有后代叶结点的简单路径上，均包含相同数目的黑色结点
- 每个NIL结点都是黑色的(此处的结点指的是空结点)（该条规则确定了路径条数）

红黑树能保证其最长路径中结点个数不超过最短路径结点个数的2倍的原因：
- 红黑树第三条性质说明红黑树不能存在连续（父子相连）的红结点，可以存在连续的黑结点，又由于第四条性质每个路径上的黑结点个数都相同，所以对于最短路径来说一定是都是黑色结点，对于最长路径来说一定是黑色红色相间的路径，所以最长路径不超过最短路径长度的2倍

#### 红黑树结点的定义
对于红黑树来说以颜色来代替AVL树的平衡因子的作用，除此之外在定义上没有什么区别

在结点的定义中要将结点的默认颜色给成红色的原因：
- 如果默认颜色为黑，那么在插入中插入一个黑结点一定会让该路径上的黑结点数量加1，从而与其他路径上黑结点数量造成不一致，而一定会影响该棵红黑树
- 如果默认颜色为红，那么在插入中插入一个红结点，可能新插入结点的父结点为黑色结点则没有影响，也可能新插入结点的父结点为红结点，由于不能存在连续的（父子相连的）红色结点，而对该棵树造成影响

所以相比较来说，默认红色比默认黑色要好

#### 红黑树的插入操作
红黑树是在二叉搜索树的基础上加上其平衡限制条件，当违反限制条件时就需要做出相应的调整

红黑树的插入可分为两步：
- 第1步：按照二叉搜索的树规则插入新结点
- 第2步：新结点插入后检查红黑树的性质是否造到破坏

**注意：**
- 因为新结点的默认颜色是红色，如果其父亲结点的颜色是黑色，则没有违反红黑树任何性质，则不需要调整
- 当新插入结点的父亲结点颜色为红色时，就违反了性质三不能有连在一起的红色结点，此时需要对红黑树分情况来讨论
- 因为插入结点的父结点是红色的，说明父结点不是根结点（根结点是黑色的），因此插入结点的祖父结点（父结点的父结点）就一定存在
- 红黑树调整时具体应该如何调整，主要是看插入结点的叔叔（插入结点的父结点的兄弟结点），根据插入结点叔叔的不同，可将红黑树的调整分为三种情况

**约定cur为当前结点，p为父结点，g为祖父结点，u为叔叔结点**

##### 变色处理
情况1：cur为红，p为红，g为黑，u存在且为红

分析：
1. 为了让该以g为根的子树不存在连续的红色结点，并且不增加路径上的黑色结点，我们让g的颜色变红，让u和p的颜色变黑，各路径上黑色结点数量没变，以此达到红黑树的性质
2. 因为p和u的颜色变黑，对其子树没有影响，但是g的颜色变红，可能存在影响，需要继续向上判断

判断逻辑：
1. 如果g就是该棵树的根那么最后需要将g的颜色变黑
2. 如果g是整棵树的子树，如果g的父节点是是黑色结点则不用调整，如果是红色结点则需要根据具体情况来做出调整

解决方式：
将p,u改为黑，g改为红，然后把g当成cur，继续向上调整

##### 单旋+变色处理
情况2：cur为红，p为红，g为黑，u不存在或u存在且为黑

分析：
1. 当u节点不存在时，说明cur一定是插入节点，如果cur不是新插入节点，那么在插入前时cur一定是黑色结点（由黑色变红），则插入前就不符合路径上黑结点数量相同的性质
2. 当u节点存在且为黑色时，说明cur的颜色插入前一定是黑色，进过插入后变色为红色，否则在插入前就存在连续的红色结点，不符合红黑树性质

解决方法：
1. 如果p为g的左孩子，cur为p的左孩子，则进行右单旋转，p变黑，g变红
2. 如果p为g的右孩子，cur为p的右孩子，则进行左单旋转，p变黑，g变红

##### 双旋+变色
情况三: cur为红，p为红，g为黑，u不存在或u存在且为黑

分析：
这里显然一次旋转无法达到红黑树平衡，由此就需要进行两次旋转

解决方式：
1. 如果p为g的左孩子，cur为p的右孩子，则针对p做左单旋转，p旋转后再对g进行右单旋，旋转后将cur变黑，g变红
2. 如果p为g的右孩子，cur为p的左孩子，则针对p做右单旋转，p旋转后再对g进行左单旋，旋转后将cur变黑，g变红

#### 红黑树的验证
红黑树的检测分为两步：
- 第1步：检测其是否满足二叉搜索树(中序遍历是否为有序序列)
- 第2步：检测其是否满足红黑树的性质

#### 红黑树与AVL树的比较
- 红黑树和AVL树都是高效的平衡二叉树，增删改查的时间复杂度都是O(logn)
- 红黑树不追求绝对平衡，其只需保证最长路径不超过最短路径的2倍，相对而言，降低了插入和旋转的次数
- 所以在经常进行增删的结构中性能比AVL树更优，而且红黑树实现比较简单，所以实际运用中红黑树更多


### 多路查找树（B树）
一个结点只能存储一个元素，在元素非常多的时候，就使得要么树的度非常大(结点拥有子树的个数的最大值)，要么树的高度非常大，甚至两者都必须足够大才行。这就使得内存存取外存次数非常多，这显然成了时间效率上的瓶颈，这迫使我们要打破每一个结点只存储一个元素的限制,为此引入了多路查找树的概念

多路查找树(Muitl-way Search Tree)，其每一个结点的孩子数可以多于两个，且每一个结点处可以存储多个元素。由于它是查找树，所有元素之间存在某种特定的排序关系

在这里，每一个结点可以存储多少个元素，以及它的孩子数的多少是非常关键的。它有4种特殊形式:2-3树、2-3-4树、B树和B+树
#### 2-3树
##### 2-3树相关概念
- 2-3树是这样的一棵多路查找树：其中的每一个结点都具有两个孩子(称为2结点)或三个孩子（称为3结点）
- 一个2结点包含一个元素和两个孩子(或没有孩子)，且与二叉排序树类似，左子树包含的元素小于该元素，右子树包含的元素大于该元素。不过，与二叉排序树不同的是，这个2结点要么没有孩子,要有就有两个，不能只有一个孩子
- 一个3结点包含一小一大两个元素和三个孩子（或没有孩子），一个3结点要么没有孩子，要么具有3个孩子。如果某个3结点有孩子的话，左子树包含小于较小元素的元素，右子树包含大于较大元素的元素，中间子树包含介于两元素间的元素
- 2-3树中所有的叶子都在同一层次上
- 2-3树的新结点插入与已有结点删除比较复杂。因为每个结点可能是2结点，也可能是3结点，要保证所有的叶子都在同一层次，需要进行一番复杂操作

##### 2-3树插入实现
对于2-3树的插入来说，与二叉排序树相同，插入操作一定是发生在叶子结点上。可与二叉排序树不同的是，2-3树插入一个元素的过程有可能会对该树的其余结构产生连锁反应


2-3树插入可分为三种情况：
- 对于空树，插入一个2结点即可
- 插入结点到一个2结点的叶子上。应该说，由于其本身就只有一个元素，所以只需要将其升级为3结点即可。要视插入的元素与当前叶子结小后，决定谁在左谁在右
- 要往3结点中插入一个新元素。因为3结点本身已经是2-3树的结点最大容量(已经有两个元素)，因此就需要将其拆分，且将树中两元素或插入元素的三者中选择其一向上移动一层。复杂的情况也正在于此

##### 2-3树删除实现
2-3树的删除也分为三种情况
- 所删除元素位于一个3结点的叶子结点上，这非常简单，只需要在该结点处删除该元素即可，不会影响到整棵树的其他结点结构
- 所删除的元素位于一个2结点上，即要删除的是一个只有一个元素的结点。如果直接删除，删除后的树就不满足2-3树的定义了
- 对于删除叶子是2结点的情况，需要分四种情况来处理：
  - 情况1：此结点的双亲也是2结点，且拥有一个3结点的右孩子
  - 情况2：此结点的双亲也是2结点，且拥有一个2结点的右孩子
  - 情况3：此结点的双亲是一个3结点
  - 情况4：当前树是一个满二叉树

#### 2-3-4树
是对2-3树的概念扩展，包括了4结点的使用

一个4结点包含小中大三个元素和四个孩子（或没有孩子）。一个4结点要么没有孩子，要么具有4个孩子

如果某个4结点有孩子的话左子树包含小于最小元素的元素;第二子树包含大于最小元素，小于第二元素的元素;第三子树包含大于第二元素，小于最大元素的元素;右子树包含大于最大元素的元素

#### B树
B树(B-Tree)是一种平衡的多路查找树，2-3树和2-3-4树都是B树的特例。结点最大的孩子数目称为B树的阶（order），因此，2-3树是3阶B树，2-3-4树是4阶B树

一个m阶的B树具有如下属性:
- 如果根结点不是叶结点，则其至少有两棵子树
- 每一个非根的分支结点都有k-1个元素和k个孩子，其中


$$
\left\lceil \frac{m}{2} \right\rceil ≤ k ≤ m
$$


$\left\lceil \frac{m}{2} \right\rceil$表示不小于m/2的最小整数


- 每一个叶子结点n都有k-1个元素,其中


$$
\left\lceil \frac{m}{2} \right\rceil ≤ k ≤ m
$$


- 所有叶子结点都位于同一层次
- 所有分支结点包含下列信息数据（n,A0,K1,A1,K2,A2…,Kn,An)，其中:Ki(i=1,2,…n)为关键字，且Ki < Ki+1 (i=1,2,…,n-1); Ai (i=0,2,…,n)为指向子树根结点的指针，且指针Ai-1所指子树中所有结点的关键字均小于Ki(i=1,2,…,n)，An所指子树中所有结点的关键子均大于Kn, 


$$
\left\lceil \frac{m}{2} \right\rceil - 1 \leq n \leq m - 1
$$


为关键字的个数(或n+1为子树的个数)



在B树上查找的过程是一个顺时针查找结点和在结点中查找关键字的交叉过程

B树的数据结构就是为内外存的数据交互准备的

#### B+树
在B树结构中，往返于每个结点之间意味着必须在硬盘的页面之间进行多次访问

B+树是为了解决所有元素遍历等基本问题，在B树结构基础上，加上的新的元素组织方式

一棵m阶的B+树和m阶的B树的差异在于
- 有n棵子树的结点中包含有n个关键字
- 所有的叶子结点包含全部关键字的信息，及指向含这些关键字记录的指针,叶子结点本身依关键字的大小自小而大顺序链接
- 所有分支结点可以看成是索引,结点中仅含有其子树中的最大(或最小)关键字


### 散列表查找（哈希表）
只通过某个函数f，使得存储位置=f(关键字)。这样可以通过查找关键字而不需要比较就可以获得需要的记录的存储位置，这就是一种新的存储技术——散列技术
#### 散列表查找定义
散列技术是在记录的存储位置和它的关键字之间建立一个确定的对应关系f，使得每个关键字key对应一个存储位置f (key)。查找时,根据这个确定的对应关系找到给定值key的映射f(key)，若查找集合中存在这个记录，则必定在f (key)的位置上
这里我们把这种对应关系f称为散列函数，又称为哈希(Hash) 函数。按这个思想，采用散列技术将记录存储在一块连续的存储空间中，这块连续存储空间称为散列表或哈希表(Hash table)。关键字对应的记录存储位置称为散列地址
#### 散列表查找步骤
散列过程分两步：
- 第1步：在存储时，通过散列函数计算记录的散列地址，并按此散列地址存储该记录
- 第2步：当查找记录时，通过同样的散列函数计算记录的散列地址，按此散列地址访问该记录。由于存取用的是同一个散列函数，因此结果也是相同的
#### 散列表适用范围
散列技术既是一种存储方法，也是一种查找方法。它与线性表、树、图等结构不同的是，前面几种结构，数据元素之间都存在某种逻辑关系，可以用连线图示表示出来，而散列技术的记录之间不存在逻辑关系，只与关键字有关联。因此，散列主要是面向查找的存储结构

散列技术最适合的求解问题是查找与给定值相等的记录。对于查找来说，简化了比较过程，效率就会大大提高。但万事有利就有弊，散列技术不具备很多常规数据结构的能力

两个关键字key1≠key2，但是却有f(key1)=f(key2)，这种现象我们称为冲突(collision)，并把 key1和key2称为这个散列函数的同义词 (synonym)

出现了冲突非常糟糕，那将造成数据查找错误。而如何处理冲突就成了一个很重要的课题

### 散列函数的构造方法
#### 设计散列函数的原则
- 计算简单
- 散列地址分布均匀
#### 直接定址法
取关键字的某个线性函数值为散列地址，即

$$
f(\text{key}) = a \times \text{key} + b
$$


a、b为常数

优点是简单、均匀，也不会产生冲突，但问题是这需要事先知道关键字的分布情况，适合查找表较小且连续的情况。由于这样的限制，在现实应用中，此方法虽然简单，但却并不常用

#### 数字分析法
如果关键字是位数较多的数字，可以使用抽取。抽取方法是使用关键字的一部分来计算散列存储位置的方法，这在散列函数中是常用的手段

总的目的就是为了提供一个散列函数，能够合理地将关键字分配到散列表的各个位置

数字分析法通常适合处理关键字位数比较大的情况，如果事先知道关键字的分布且关键字的若干位分布较均匀，可以考虑这个方法

#### 平方取中法
这个方法计算很简单，假设关键字是1234，那么它的平方就是1522756，再抽取中间的3位就是227，用做散列地址。再比如关键字是 4321，那么它的平方就是18671041，抽取中间的3位就可以是671，也可以是710，用做散列地址。平方取中法比较适合于不知道关键字的分布,而位数又不是很大的情况

#### 折叠法
折叠法是将关键字从左到右分割成位数相等的几部分（注意最后一部分位数不够时可以短些），然后将这几部分叠加求和，并按散列表表长，取后几位作为散列地址

比如我们的关键字是9876543210,散列表表长为三位，我们将它分为四组，987|654|321|0，然后将它们叠加求和987+654+321+0=1962，再求后3位得到散列地址为962

折叠法事先不需要知道关键字的分布，适合关键字位数较多的情况

#### 除留余数法
此方法为最常用的构造散列函数方法。对于散列表长为m的散列函数公式为：

$$
f(\text{key}) = \text{key} \bmod p \quad (p \leq m)
$$

mod是取模（求余数）。事实上，这个方法不仅可以对关键字直接取模，也可以在折叠、平方取中后再取模

此方法的关键在于选择合适的p，如果p选的不好，可能会容易产生同义词

因此根据前辈们的经验，若散列表表长为m，通常p为小于或等于表长（最好接近m）的最小质数或不包含小于20质因子的合数

#### 随机数法
选择一个随机数，取关键字的随机函数值为它的散列地址。也就是f(key)=random(key)。这里random是随机函数。当关键字的长度不等时，采用这个方法构造散列函数是比较合适的

如果关键字是字符串如何处理？无论是英文字符，还是中文字符，也包括各种各样的符号，它们都可以转化为某种数字来对待，比如ASCII码或者Unicode码等，因此也就可以使用上面的这些方法

#### 构造方法总结
现实中，应该视不同的情况采用不同的散列函数。重点考虑以下因素：
- 计算散列地址所需的时间
- 关键字的长度
- 散列表的大小
- 关键字的分布情况
- 记录查找的频率
综合这些因素，才能决策选择哪种散列函数更合适

### 处理散列冲突的方法
设计的再好的散列函数，也不可能完全避免冲突，因此，要考虑如何处理冲突
#### 开放定址法
##### 线性探测法
开放定址法就是一旦发生了冲突，就去寻找下一个空的散列地址，只要散列表足够大，空的散列地址总能找到，并将记录存入
它的公式是：

$$
f_i(\text{key}) = \left( f(\text{key}) + d_i \right) \bmod m
\qquad \left( d_i = 1, 2, 3, \dots, m-1 \right)
$$

我们把这种解决冲突的开放定址法称为线性探测法

##### 二次探测法
本来都不是同义词，却需要争夺一个地址的情况，称为堆积。很显然，堆积的出现，使得我们需要不断处理冲突，无论是存入还是查找效率都会大大降低

可以双向寻找可能的空位置，另外增加平方运算，目的是为了不让关键字都聚集在某一块区域，这种方法称为二次探测法

$$
f_i(\text{key}) = \left( f(\text{key}) + d_i \right) \bmod m
$$

其中：

$$
d_i = \begin{cases}+1^2, -1^2, +2^2, -2^2, \dots, +q^2, -q^2 \\
\text{其中 } q \leq \frac{m}{2}\end{cases}
$$



##### 随机探测法
还有一种方法是，在冲突时，对于位移量di采用随机函数计算得到，称为随机探测法
随机探测法使用的随机是伪随机数。如果设置随机种子相同，则不断调用随机函数生成不会重复的数列。在查找时，使用同样的随机种子，每次得到的数列是相同的，可以得到相同的散列地址

$$
f_i(\text{key}) = \left( f(\text{key}) + d_i \right) \bmod m
$$

其中：

$$
d_i \text{ 是一个在 } 1 \text{ 到 } m-1 \text{ 范围内的随机数列}
$$


总之，开放定址法只要散列表未填满，总能找到不发生冲突的地址，是我们常用的解决冲突的办法

#### 再散列函数法
对于散列表来说，事先准备多个散列函数

$$
f_i(\text{key}) = RH_i(\text{key}) \quad \text{for} \quad i = 1, 2, \dots, k
$$

这里的RHi就是不同的散列函数。可以把前面说的除留余数、折叠、平方取中全部用上。每当发生散列地址冲突时，就换一个散列函数计算，总会有一个可以把冲突解决掉。这种方法能够使得关键字不产生聚集，当然，相应地也增加了计算的时间

#### 链地址法
将所有关键字为同义词的记录存储在一个单链表中，我们称这种表为同义词子表，在散列表中只存储所有同义词子表的头指针
此时，已经不存在什么冲突换址的问题，无论有多少个冲突,都只是在当前位置给单链表增加结点的问题

链地址法对于可能会造成很多冲突的散列函数来说，提供了绝不会出现找不到地址的保障。当然，这也就带来了查找时需要遍历单链表的性能损耗

#### 公共溢出区法
为所有冲突的关键字建立一个公共的溢出区来存放

在查找时，对给定值通过散列函数计算出散列地址后，先与基本表的相应位置进行比对，如果相等，则查找成功；如果不相等，则到溢出表去进行顺序查找。如果相对于基本表而言，有冲突的数据很少的情况下，公共溢出区的结构对查找性能来说还是非常高的


### 散列表查找实现
#### 散列表查找算法实现
首先是需要定义一个散列表的结构以及一些相关的常数。其中HashTable就是散列表结构。结构当中的elem为一个动态数组

#### 散列表查找性能分析
如果没有冲突，散列查找是查找效率最高的，时间复杂度为O(1)。但是实际应用中，冲突不可避免
散列查找的平均查找长度取决于以下因素：
##### 散列函数是否均匀
散列函数的好坏直接影响着出现冲突的频繁程度，不过，由于不同的散列函数对同一组随机的关键字，产生冲突的可能性是相同的，因此可以不考虑它对平均查找长度的影响
##### 处理冲突的方法
相同的关键字、相同的散列函数,但处理冲突的方法不同，会使得平均查找长度不同。比如线性探测处理冲突可能会产生堆积，显然就没有二次探测法好，而链地址法处理冲突不会产生任何堆积，因而具有更佳的平均查找性能
##### 散列表的装填因子
- 装填因子α=填入表中的记录个数/散列表长度
- α标志着散列表的装满的程度。当填入表中的记录越多，α就越大，产生冲突的可能性就越大
- 散列表的平均查找长度取决于装填因子，而不是取决于查找集合中的记录个数
- 不管记录个数n有多大，我们总可以选择一个合适的装填因子以便将平均查找长度限定在一个范围之内，此时我们散列查找的时间复杂度就真的是0(1)了。为了做到这一点，通常我们都是将散列表的空间设置得比查找集合大，此时虽然是浪费了一定的空间，但换来的是查找效率的大大提升，总的来说，还是非常值得的


## 排序
### 排序的基本概念与分类
#### 排序的严格定义
假设含有n个记录的序列为{r1,r2,……,rn},其相应的关键字分别为{k1,k2,……,kn}，需确定1,2,……,n的一种排列p1,p2,……,pn，使其相应的关键字满足```kp1 ≤ kp2 ≤ …… < kpn```（非递减或非递增）关系，即使得序列成为一个按关键字有序的序列{rp1,rp2…,rpn}，这样的操作就称为排序

**在排序问题中，通常将数据元素称为记录。输入的是一个记录集合，输出的也是一个记录集合。所以，可以将排序看成是线性表的一种操作**

排序的依据是关键字之间的大小关系。对同一个记录集合，针对不同的关键字进行排序，可以得到不同的序列

这里关键字ki可以是记录r的主关键字，也可以是次关键字，甚至是若干数据项的组合

#### 排序的稳定性
假设ki=kj (1≤i≤n,1≤j≤n,i≠j)，且在排序前的序列中ri领先于rj(即i < j)。如果排序后ri仍领先于rj，则称所用的排序方法是稳定的；反之，若可能使得排序后的序列中rj领先ri，则称所用的排序方法是不稳定的

只要有一组关键字实例发生颠倒的情况,就可认为此排序方法是不稳定的。排序算法是否稳定的，要通过分析后才能得出

#### 内排序与外排序
根据在排序过程中待排序的记录是否全部被放置在内存中，排序分为：
- 内排序
- 外排序

内排序是在排序整个过程中，待排序的所有记录全部被放置在内存中

外排序是由于排序的记录个数太多，不能同时放置在内存，整个排序过程需要在内外存之间多次交换数据才能进行

对于内排序来说,排序算法的性能主要是受3个方面影响：
##### 时间性能
排序是数据处理中经常执行的一种操作，往往属于系统的核心部分，因此排序算法的时间开销是衡量其好坏的最重要的标志

在内排序中，主要进行两种操作：比较和移动

比较指关键字之间的比较，这是要做排序最起码的操作

移动指记录从一个位置移动到另一个位置。事实上，移动可以通过改变记录的存储方式来予以避免

总之，高效率的内排序算法应该是具有尽可能少的关键字比较次数和尽可能少的记录移动次数
##### 辅助空间
评价排序算法的另一个主要标准是执行算法所需要的辅助存储空间。辅助存储空间是除了存放待排序所占用的存储空间之外，执行算法所需要的其他存储空间
##### 算法的复杂性
注意这里指的是算法本身的复杂度，而不是指算法的时间复杂度。显然算法过于复杂也会影响排序的性能

根据排序过程中借助的主要操作，我们把内排序分为：插入排序、交换排序、选择排序和归并排序

这些都是比较成熟的排序技术，已经被广泛地应用于许许多多的程序语言或数据库当中，甚至它们都已经封装了关于排序算法的实现代码。因此，我们学习这些排序算法的目的更多并不是为了去在现实中编程排序算法，而是通过学习来提高我们编写算法的能力，以便于去解决更多复杂和灵活的应用性问题
七种排序的算法，按照算法的复杂度分为两大类：

| 简单算法       | 改进算法   |
| -------------- | ---------- |
| 冒泡排序      | 希尔排序   |
| 简单选择排序   | 堆排序     |
| 直接插入排序   | 归并排序   |
|                | 快速排序   |


### 冒泡排序
冒泡排序算法最简单，最容易理解，最简单排序实现
#### 冒泡排序概念
冒泡排序（Bubble Sort）是一种交换排序，它的基本思想是：两两比较相邻记录的关键字，如果反序则交换，直到没有反序的记录为止。冒泡的实现在细节上可以有很多种变化

- 初级版冒泡排序是交换下标为i和j的元素
- 初级版冒泡排序算法的效率是非常低的

#### 冒泡排序算法
标准版冒泡排序是交换下标是j的j+1的元素

这一算法比初级版有进步，在数据非常多的排序过程中，这种差异会体现出来。较小的数字会如同气泡般慢慢浮到上面，因此此算法命名为冒泡算法

#### 冒泡算法优化
增加一个flag标识，如果有数据交换，则更改标识值，从而结束循环

#### 冒泡排序复杂度
冒泡排序时间复杂度为O(n^2)

### 简单选择排序
简单选择排序法(Simple Selection Sort)就是通过n-i次关键字间的比较，从n-i+1个记录中选出关键字最小的记录，并和第i(1≤i≤n)个记录交换之
#### 简单选择排序复杂度分析
简单选择排序时间复杂度为O(n^2),但是简单选择排序性能上要略优于冒泡排序

### 直接插入排序
#### 直接插入排序算法
直接插入排序(Straight Insertion Sort)的基本操作是将一个记录插入到已经排好序的有序表中，从而得到一个新的、记录数增1的有序表
#### 直接插入排序复杂度分析
空间上，它只需要一个记录的辅助空间
时间上，直接插入排序法的时间复杂度为O(n^2)。但是直接插入排序法比冒泡排序和简单选择排序的性能要好一些

### 希尔排序
简单排序算法还有很多，但时间复杂度都是O(n^2)，而希尔排序的出现，把内排序算法的时间复杂度提升到了O(nlogn)
#### 希尔排序原理
希尔排序(Shell Sort)是D.L.Shell于1959年提出来的一种排序算法，在这之前排序算法的时间复杂度基本都是O(n^2)的，希尔排序算法是突破这个时间复杂度的第一批算法之一
希尔排序是对直接插入排序进行了改进

如何让待排序的记录个数较少呢？可以将原本有大量记录数的记录进行分组。分割成若干个子序列，此时每个子序列待排序的记录个数就比较少了，然后在这些子序列内分别进行直接插入排序，当整个序列都基本有序时，注意只是基本有序时，再对全体记录进行一次直接插入排序

**所谓的基本有序，就是小的关键字基本在前面，大的基本在后面，不大不小的基本在中间**

分割待排序记录的目的是减少待排序记录的个数，并使整个序列向基本有序发展。因此，需要采取跳跃分割的策略：将相距某个“增量”的记录组成一个子序列，这样才能保证在子序列内分别进行直接插入排序后得到的结果是基本有序而不是局部有序

#### 希尔排序算法
#### 希尔排序复杂度分析
希尔排序的关键并不是随便分组后各自排序，而是将相隔某个“增量”的记录组成一个子序列，实现跳跃式移动，使得排序效率提高
希尔排序的“增量”选取非常关键
大量研究表明，当增量序列为

$$
\delta[k] = 2^{(t - k + 1)} - 1 \quad \text{for} \quad 0 \leq k \leq t \leq \left\lfloor \log(n+1) \right\rfloor
$$


时，可以获得不错的效率，其时间复杂度是 $O(n^{3/2})$ ，好于直接排序的 $O(n^2)$

**需要注意的是，增量序列的最后一个增量值必须等于1才行**

由于记录是跳跃式的移动，希尔排序并不是一种稳定的排序算法

### 堆排序
简单选择排序在待排序的n个记录中选择一个最小的记录需要比较n-1次。但是这样的操作没有把每一趟的比较结果保存下来，所以后一趟排序时又重复执行了这些比较操作，因而记录的比较次数较多

堆排序（Heap Sort）是对简单选择排序进行的一种改进
#### 堆数据结构
堆是具有下列性质的完全二叉树：
- 每个结点的值都大于或等于其左右孩子结点的值，称为大顶堆
或者每个结点的值都小于或等于其左右孩子结点的值,称为小顶堆

根节点一定是堆中所有结点的最大（小）者

#### 堆排序算法
堆排序（Heap Sort）就是利用堆（假设利用大顶堆）进行排序的方法
它的基本思想是：将待排序的序列构造成一个大顶堆。此时,整个序列的最大值就是堆顶的根结点。将它移走（其实就是将其与堆数组的末尾元素交换，此时末尾元素就是最大值），然后将剩余的n-1个序列重新构造成一个堆,这样就会得到n个元素中的次小值。如此反复执行,便能得到一个有序序列了

#### 堆排序算法复杂度
- 堆排序最好、最坏和平均时间复杂度均为O(nlogn)
- 堆排序的空间复杂度上，只有一个用来交换的暂存单元，非常的不错
- 但是由于记录的比较与交换是跳跃式进行，因此堆排序也是一种不稳定的排序方法
- 另外，由于初始构建堆所需比较次数较多，因此，它并不适合待排序序列个数较少的情况

### 归并排序
#### 归并排序算法
“归并”一词的中文含义就是合并、并入的意思，而在数据结构中的定义是将两个或两个以上的有序表组合成一个新的有序表
归并排序（Merging Sort）就是利用归并的思想实现的排序方法
它的原理是假设初始序列含有n个记录，则可以看成是n个有序的子序列，每个子序列的长度为1，然后两两归并，得到

$$
\left\lceil \frac{n}{2} \right\rceil
$$


（ $\left\lceil x \right\rceil$ 表示不小于x的最小整数）个长度为2或1的有序子序列；再两两归并，……，如此重复，直至得到一个长度为n的有序序列为止，这种排序方法称为2路归并排序
#### 归并排序算法复杂度分析
总时间复杂度为

$$
O(n \log n)
$$

空间复杂度为

$$
O(n + \log n)
$$

归并排序需要两两比较，不存在跳跃，因此归并排序是一种稳定的排序算法

归并排序是一种比较占用内存，但效率高且稳定的算法

#### 非递归实现归并排序
非递归的迭代做法更加直截了当，从最小的序列开始归并直至完成

非递归的迭代方法，避免了递归时深度为logn的栈空间，空间只是用到申请归并临时用的TR数组，因此空间复杂度为0(n)，并且避免递归也在时间性能上有一定的提升。应该说，使用归并排序时，尽量考虑用非递归方法

### 快速排序
快速排序是很重要、很常用的排序算法。在C++ STL、Java SDK或.NET FrameWork SDK等开发工具包中的源代码中都能找到它的某种实现版本。快速排序算法是20世纪十大算法之一

希尔排序相当于直接插入排序的升级，它们同属于插入排序类；堆排序相当于简单选择排序的升级，它们同属于选择排序类
而快速排序其实就是冒泡排序的升级，它们都属于交换排序类。即它也是通过不断比较和移动交换来实现排序的，只不过它的实现，增大了记录的比较和移动的距离,将关键字较大的记录从前面直接移动到后面，关键字较小的记录从后面直接移动到前面，从而减少了总的比较次数和移动交换次数

#### 快速排序算法
快速排序（Quick Sort）的基本思想是：通过一趟排序将待排记录分割成独立的两部分，其中一部分记录的关键字均比另一部分记录的关键字小，则可分别对这两部分记录继续进行排序，以达到整个序列有序的目的
先选取当中的一个关键字，然后想尽办法将它放到一个位置，使得它左边的值都比它小，右边的值比它大，我们将这样的关键字称为枢轴(pivot)

Partition 函数，其实就是将选取的pivotkey不断交换，将比它小的换到它的左边，比它大的换到它的右边，它也在交换中不断更改自己的位置，直到完全满足这个要求为止

#### 快速排序复杂度分析
##### 时间复杂度
快速排序的时间性能取决于快速排序递归的深度
- 最优情况下，快速排序算法的时间复杂度为O(nlogn)
- 最坏情况下，快速排序算法的时间复杂度为O(n^2)
- 快速排序算法的平均时间复杂度为O(nlogn)
##### 空间复杂度
- 最优情况下，快速排序算法的空间复杂度为O(logn)
- 最坏情况下，快速排序算法的空间复杂度为O(n)
- 快速排序算法的平均空间复杂度为O(logn)

可惜的是，由于关键字的比较和交换是跳跃进行的，因此，快速排序是一种不稳定的排序方法

#### 快速排序优化
##### 优化选取枢轴
因为在现实中,待排序的系列极有可能是基本有序的，此时，总是固定选取第一个关键字（其实无论是固定选取哪一个位置的关键字）作为首个枢轴就变成了极为不合理的作法
##### 随机选取枢轴法
随机获得一个low与high之间的数rnd，让它的关键字 ```Lr[rnd]``` 与 ```Lr[low]``` 交换

这在某种程度上解决了对基本有序的序列快速排序时的性能瓶颈。但是随机有可能随机到的依然是很小或很大的关键字
##### 三数取中法
再改进,于是就有了三数取中_(median-of-three)法

即取三个关键字先进行排序，将中间数作为枢轴，一般是取左端、右端和中间三个数，也可以随机选取。这样至少这个中间数一定不会是最小或者最大的数，从概率来说，取三个数均为最小或最大数的可能性是微乎其微的，因此中间数位于较为中间的值的可能性就大大提高了

由于整个序列是无序状态，随机选取三个数和从左中右端取三个数其实是一回事，而且随机数生成器本身还会带来时间上的开销，因此随机生成不予考虑
##### 九数取中法
三数取中对小数组来说有很大的概率选择到一个比较好的pivotkey，但是对于非常大的待排序的序列来说，还是不足以保证能够选择出一个好的pivotkey，因此还有个办法是所谓九数取中(median-of-nine)

它先从数组中分三次取样每次取三个数，三个样品各取出中数，然后从这三个中数当中再取出一个中数作为枢轴。显然这就更加保证了取到的pivotkey是比较接近中间值的关键字

###### 优化不必要的交换
将代码中的交换操作改为替换操作

###### 优化小数组时的排序方案
如果数组非常小，快速排序反而不如直接插入排序效果好（直接插入是简单排序中性能最好的）

原因在于快速排序用到了递归操作，在大量数据排序时，这点性能影响相对于它的整体算法优势而言是可以忽略的，但如果数组只有几个记录需要排序时，它的性能影响就凸现出来了

具体操作是在QSort函数中增加一个判断，当High-low不大于某个常数时，就用直接插入排序，这样就能保证最大化地利用两种排序的优势来完成排序工作

###### 优化递归操作

### 不同排序算法的对比

| 排序方法       | 平均情况         | 最好情况         | 最坏情况         | 辅助空间          | 稳定性  |
| -------------- | ---------------- | ---------------- | ---------------- | ----------------- | ------- |
| 冒泡排序       | \( O(n^2) \)     | \( O(n) \)       | \( O(n^2) \)     | \( O(1) \)        | 稳定    |
| 简单选择排序   | \( O(n^2) \)     | \( O(n^2) \)     | \( O(n^2) \)     | \( O(1) \)        | 稳定    |
| 直接插入排序   | \( O(n^2) \)     | \( O(n) \)       | \( O(n^2) \)     | \( O(1) \)        | 稳定    |
| 希尔排序       | \( O(n \log n) \sim O(n^2) \) | \( O(n^{1.3}) \)  | \( O(n^2) \)     | \( O(1) \)        | 不稳定  |
| 堆排序         | \( O(n \log n) \) | \( O(n \log n) \) | \( O(n \log n) \) | \( O(1) \)        | 不稳定  |
| 归并排序       | \( O(n \log n) \) | \( O(n \log n) \) | \( O(n \log n) \) | \( O(n) \)        | 稳定    |
| 快速排序       | \( O(n \log n) \) | \( O(n \log n) \) | \( O(n^2) \)     | \( O(\log n) \sim O(n) \) | 不稳定  |


- 从平均情况来看，最后3种改进算法要胜过希尔排序，并远远胜过前3种简单算法
- 从最好情况看，反而冒泡和直接插入排序要更胜一筹。如果待排序序列总是基本有序，反而不应该考虑4种复杂的改进算法
- 从最坏情况看，堆排序与归并排序又强过快速排序以及其他简单排序

从这三组时间复杂度的数据对比中,我们可以得出这样一个认识：
堆排序和归并排序就像两个参加奥数考试的优等生，心理素质强，发挥稳定。而快速排序像是很情绪化的天才，心情好时表现极佳，碰到较糟糕环境会变得差强人意。但是他们如果都来比赛计算个位数的加减法，它们反而算不过成绩极普通的冒泡和直接插入

- 从空间复杂度来说，归并排序强调要马跑得快，就得给马吃个饱。快速排序也有相应的空间要求，反而堆排序等却都是少量索取，大量付出，对空间要求是0(1)。如果执行算法的软件所处的环境非常在乎内存使用量的多少时，选择归并排序和快速排序就不是一个较好的决策

- 从稳定性来看，归并排序最好，对于非常在乎排序稳定性的应用中，归并排序是个好算法

- 从待排序记录的个数上来说，待排序的个数n越小，采用简单排序方法越合适。反之，n越大，采用改进排序方法越合适。这也就是我们为什么对快速排序优化时,增加了一个阀值，低于阀值时换作直接插入排序的原因


- 简单选择排序在关键字本身信息量很大时就变得非常有优势，因为它是通过大量比较后选择明确记录进行移动。因此对于数据量不是很大而记录的关键字信息量较大的排序要求，简单排序算法是占优的。另外，记录的关键字信息量大小对那四个改进算法影响不大

总之，从综合各项指标来说，经过优化的快速排序是性能最好的排序算法，但是不同的场合我们也应该考虑使用不同的算法来应对它










